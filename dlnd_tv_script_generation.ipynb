{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TV Script Generation\n",
    "In this project, you'll generate your own [Simpsons](https://en.wikipedia.org/wiki/The_Simpsons) TV scripts using RNNs.  You'll be using part of the [Simpsons dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) of scripts from 27 seasons.  The Neural Network you'll build will generate a new TV script for a scene at [Moe's Tavern](https://simpsonswiki.com/wiki/Moe's_Tavern).\n",
    "## Get the Data\n",
    "The data is already provided for you.  You'll be using a subset of the original dataset.  It consists of only the scenes in Moe's Tavern.  This doesn't include other versions of the tavern, like \"Moe's Cavern\", \"Flaming Moe's\", \"Uncle Moe's Family Feed-Bag\", etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "Play around with `view_sentence_range` to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocab_to_int = { word: k for k, word in enumerate(set(text))}\n",
    "    int_to_vocab = { vocab_to_int[word]: word for word in vocab_to_int}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    return {\n",
    "        '.': '||Period||',\n",
    "        ',': '||Comma||',\n",
    "        '\"': '||Quotation_Mark||',\n",
    "        ';': '||Semicolon||',\n",
    "        '!': '||Exclamation_Mark||',\n",
    "        '?': '||Question_Mark||',\n",
    "        '(': '||Left_Parentheses||',\n",
    "        ')': '||Right_Parenttheses||',\n",
    "        '--': '||Dash||',\n",
    "        '\\n': '||Return'\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/cpu/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Input\n",
    "Implement the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_placeholder = tf.placeholder(tf.int32, [None, None], name=\"input\")\n",
    "    target_placeholder = tf.placeholder(tf.int32, [None, None])\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    \n",
    "    return input_placeholder, target_placeholder, lr\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    num_layers = 2\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size)] * num_layers)\n",
    "    state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    initial_state = tf.identity(state, \"initial_state\")\n",
    "    return cell, initial_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_rnn cell= <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x7fa18a8fb278>\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    print('build_rnn cell=', cell)\n",
    "    _, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    finalstate = tf.identity(state, \"final_state\")\n",
    "    return (_, finalstate)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_rnn cell= <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x7fa18a7cedd8>\n",
      "input_data Tensor(\"Placeholder:0\", shape=(128, 5), dtype=int32)\n",
      "rnn_size 256\n",
      "vocab_size 27\n",
      "rnn Tensor(\"rnn/transpose:0\", shape=(128, 5, 256), dtype=float32)\n",
      "output Tensor(\"Reshape:0\", shape=(640, 256), dtype=float32)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    enc_embed_input = get_embed(input_data, vocab_size, embed_dim)\n",
    "    rnn, final_state = build_rnn(cell, enc_embed_input)\n",
    "    # rnn = tf.concat(rnn, axis=1)\n",
    "    print('input_data', input_data)\n",
    "    print('rnn_size', rnn_size)\n",
    "    print('vocab_size', vocab_size)\n",
    "    print('rnn', rnn)\n",
    "    # print('rnn shape', rnn.get_shape())  # batch_size * sequence_length * num_features (embeding_size)\n",
    "    # seq_output = tf.concat(rnn, axis=1)\n",
    "    # print('seq_output', seq_output)\n",
    "    output = tf.reshape(rnn, [-1, rnn_size])\n",
    "    print('output', output)\n",
    "    # print([128,5] + [vocab_size])\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((rnn_size, vocab_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(vocab_size))\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    input_shape = input_data.get_shape().as_list()\n",
    "    if input_shape[0] is None:  # Handle place-holder dimension\n",
    "        rnn_shape = tf.shape(rnn)\n",
    "        print('batch_size', rnn_shape[0])\n",
    "        print('sequence_length', rnn_shape[1])\n",
    "        logits = tf.reshape(logits, [rnn_shape[0], rnn_shape[1], vocab_size])\n",
    "    else:\n",
    "        logits = tf.reshape(logits, [input_shape[0], input_shape[1], vocab_size])\n",
    "\n",
    "    return (logits, final_state)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For exmple, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 2, 3)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2  3], [ 7  8  9]],\n",
    "    # Batch of targets\n",
    "    [[ 2  3  4], [ 8  9 10]]\n",
    "  ],\n",
    " \n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 4  5  6], [10 11 12]],\n",
    "    # Batch of targets\n",
    "    [[ 5  6  7], [11 12 13]]\n",
    "  ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 128\n",
      "seq_length 5\n",
      "[[[[   0    1    2    3    4]\n",
      "   [   5    6    7    8    9]\n",
      "   [  10   11   12   13   14]\n",
      "   ..., \n",
      "   [ 625  626  627  628  629]\n",
      "   [ 630  631  632  633  634]\n",
      "   [ 635  636  637  638  639]]\n",
      "\n",
      "  [[   1    2    3    4    5]\n",
      "   [   6    7    8    9   10]\n",
      "   [  11   12   13   14   15]\n",
      "   ..., \n",
      "   [ 626  627  628  629  630]\n",
      "   [ 631  632  633  634  635]\n",
      "   [ 636  637  638  639  640]]]\n",
      "\n",
      "\n",
      " [[[ 640  641  642  643  644]\n",
      "   [ 645  646  647  648  649]\n",
      "   [ 650  651  652  653  654]\n",
      "   ..., \n",
      "   [1265 1266 1267 1268 1269]\n",
      "   [1270 1271 1272 1273 1274]\n",
      "   [1275 1276 1277 1278 1279]]\n",
      "\n",
      "  [[ 641  642  643  644  645]\n",
      "   [ 646  647  648  649  650]\n",
      "   [ 651  652  653  654  655]\n",
      "   ..., \n",
      "   [1266 1267 1268 1269 1270]\n",
      "   [1271 1272 1273 1274 1275]\n",
      "   [1276 1277 1278 1279 1280]]]\n",
      "\n",
      "\n",
      " [[[1280 1281 1282 1283 1284]\n",
      "   [1285 1286 1287 1288 1289]\n",
      "   [1290 1291 1292 1293 1294]\n",
      "   ..., \n",
      "   [1905 1906 1907 1908 1909]\n",
      "   [1910 1911 1912 1913 1914]\n",
      "   [1915 1916 1917 1918 1919]]\n",
      "\n",
      "  [[1281 1282 1283 1284 1285]\n",
      "   [1286 1287 1288 1289 1290]\n",
      "   [1291 1292 1293 1294 1295]\n",
      "   ..., \n",
      "   [1906 1907 1908 1909 1910]\n",
      "   [1911 1912 1913 1914 1915]\n",
      "   [1916 1917 1918 1919 1920]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[2560 2561 2562 2563 2564]\n",
      "   [2565 2566 2567 2568 2569]\n",
      "   [2570 2571 2572 2573 2574]\n",
      "   ..., \n",
      "   [3185 3186 3187 3188 3189]\n",
      "   [3190 3191 3192 3193 3194]\n",
      "   [3195 3196 3197 3198 3199]]\n",
      "\n",
      "  [[2561 2562 2563 2564 2565]\n",
      "   [2566 2567 2568 2569 2570]\n",
      "   [2571 2572 2573 2574 2575]\n",
      "   ..., \n",
      "   [3186 3187 3188 3189 3190]\n",
      "   [3191 3192 3193 3194 3195]\n",
      "   [3196 3197 3198 3199 3200]]]\n",
      "\n",
      "\n",
      " [[[3200 3201 3202 3203 3204]\n",
      "   [3205 3206 3207 3208 3209]\n",
      "   [3210 3211 3212 3213 3214]\n",
      "   ..., \n",
      "   [3825 3826 3827 3828 3829]\n",
      "   [3830 3831 3832 3833 3834]\n",
      "   [3835 3836 3837 3838 3839]]\n",
      "\n",
      "  [[3201 3202 3203 3204 3205]\n",
      "   [3206 3207 3208 3209 3210]\n",
      "   [3211 3212 3213 3214 3215]\n",
      "   ..., \n",
      "   [3826 3827 3828 3829 3830]\n",
      "   [3831 3832 3833 3834 3835]\n",
      "   [3836 3837 3838 3839 3840]]]\n",
      "\n",
      "\n",
      " [[[3840 3841 3842 3843 3844]\n",
      "   [3845 3846 3847 3848 3849]\n",
      "   [3850 3851 3852 3853 3854]\n",
      "   ..., \n",
      "   [4465 4466 4467 4468 4469]\n",
      "   [4470 4471 4472 4473 4474]\n",
      "   [4475 4476 4477 4478 4479]]\n",
      "\n",
      "  [[3841 3842 3843 3844 3845]\n",
      "   [3846 3847 3848 3849 3850]\n",
      "   [3851 3852 3853 3854 3855]\n",
      "   ..., \n",
      "   [4466 4467 4468 4469 4470]\n",
      "   [4471 4472 4473 4474 4475]\n",
      "   [4476 4477 4478 4479 4480]]]]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    print('batch_size', batch_size)\n",
    "    print('seq_length', seq_length)\n",
    "    batch_count = int(len(int_text) / (batch_size * seq_length))\n",
    "    int_length = batch_count * batch_size * seq_length\n",
    "    inputs = np.array(int_text[:int_length]).reshape(-1, batch_size, seq_length)\n",
    "    outputs = np.array(int_text[1:int_length+1]).reshape(-1, batch_size, seq_length)\n",
    "    \n",
    "    # print(outputs)\n",
    "    # Merge inputs and outputs?\n",
    "    batches = np.stack([inputs, outputs], axis=1)\n",
    "    print(batches)\n",
    "    return batches\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 12  # average sentence size\n",
    "# RNN Size\n",
    "rnn_size = 100\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 128\n",
    "# Sequence Length\n",
    "seq_length = 10\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 1\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_rnn cell= <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x7fa18a812978>\n",
      "input_data Tensor(\"input:0\", shape=(?, ?), dtype=int32)\n",
      "rnn_size 100\n",
      "vocab_size 6779\n",
      "rnn Tensor(\"rnn/transpose:0\", shape=(?, ?, 100), dtype=float32)\n",
      "output Tensor(\"Reshape:0\", shape=(?, 100), dtype=float32)\n",
      "batch_size Tensor(\"strided_slice_1:0\", shape=(), dtype=int32)\n",
      "sequence_length Tensor(\"strided_slice_2:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forms](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 12\n",
      "seq_length 10\n",
      "[[[[5638 4636  532 ..., 6239 3139  971]\n",
      "   [2475 1089 6149 ..., 4257 2592 4662]\n",
      "   [2592 6577 2592 ..., 3624 4911 2592]\n",
      "   ..., \n",
      "   [2812 3754 5983 ..., 6239 1946 5638]\n",
      "   [2821  971 2252 ..., 3754 2573 2868]\n",
      "   [3714 6239 1946 ..., 2732 2592 5348]]\n",
      "\n",
      "  [[4636  532 5384 ..., 3139  971 2475]\n",
      "   [1089 6149 2293 ..., 2592 4662 2592]\n",
      "   [6577 2592 6107 ..., 4911 2592 6167]\n",
      "   ..., \n",
      "   [3754 5983 3041 ..., 1946 5638 2821]\n",
      "   [ 971 2252 5039 ..., 2573 2868 3714]\n",
      "   [6239 1946 3228 ..., 2592 5348 6239]]]\n",
      "\n",
      "\n",
      " [[[6239 2786 3601 ..., 5638 5039 2592]\n",
      "   [1259 2592 1494 ..., 5169 3754 2732]\n",
      "   [6239 1946 5858 ...,  204 2293 6149]\n",
      "   ..., \n",
      "   [3993 4709 6239 ..., 3161 6022 1075]\n",
      "   [4777 2740  971 ..., 6053 1946 5638]\n",
      "   [4636 5670 3041 ..., 2128  197 5410]]\n",
      "\n",
      "  [[2786 3601 4384 ..., 5039 2592 1259]\n",
      "   [2592 1494 3718 ..., 3754 2732 6239]\n",
      "   [1946 5858 4662 ..., 2293 6149 3878]\n",
      "   ..., \n",
      "   [4709 6239 4807 ..., 6022 1075 4777]\n",
      "   [2740  971 5888 ..., 1946 5638 4636]\n",
      "   [5670 3041 5780 ...,  197 5410 4488]]]\n",
      "\n",
      "\n",
      " [[[4488 1075 4475 ..., 2592 1095 5621]\n",
      "   [2592 5348 6239 ..., 4636 6603 2700]\n",
      "   [6447 6239 1946 ..., 2691 3312 6107]\n",
      "   ..., \n",
      "   [4567 4438 5217 ..., 2700 1087 2713]\n",
      "   [6036 2442 3044 ..., 5217 3196 4709]\n",
      "   [6149 2867 5410 ..., 1946 5858 6460]]\n",
      "\n",
      "  [[1075 4475 2837 ..., 1095 5621 2592]\n",
      "   [5348 6239 6239 ..., 6603 2700 6447]\n",
      "   [6239 1946 1946 ..., 3312 6107  971]\n",
      "   ..., \n",
      "   [4438 5217  971 ..., 1087 2713 6036]\n",
      "   [2442 3044 6149 ..., 3196 4709 6149]\n",
      "   [2867 5410 5973 ..., 5858 6460 2592]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[2700 2166 5420 ..., 1422 5074 5385]\n",
      "   [1779 1524 1348 ..., 1494 4614 4526]\n",
      "   [6239 2848 6236 ..., 6239 6239 6460]\n",
      "   ..., \n",
      "   [4265 6053 1946 ..., 2592 5039 2592]\n",
      "   [2700 4274 1494 ..., 1946 3228 4662]\n",
      "   [2592 1202 2270 ..., 4567 5888 6239]]\n",
      "\n",
      "  [[2166 5420 1494 ..., 5074 5385 1779]\n",
      "   [1524 1348 2592 ..., 4614 4526 6239]\n",
      "   [2848 6236 2700 ..., 6239 6460 2592]\n",
      "   ..., \n",
      "   [6053 1946 1946 ..., 5039 2592 2700]\n",
      "   [4274 1494  147 ..., 3228 4662 2592]\n",
      "   [1202 2270 6239 ..., 5888 6239 1933]]]\n",
      "\n",
      "\n",
      " [[[1933 3505 2592 ..., 5858 1259 2592]\n",
      "   [5348 2592  895 ..., 2646 5217  971]\n",
      "   [4723 2837 1946 ..., 1868 5779 2646]\n",
      "   ..., \n",
      "   [3604 1362 6239 ..., 2682 1232 6239]\n",
      "   [4636 6149 5039 ..., 6239 2700  688]\n",
      "   [2162 5180 1003 ..., 1002   20 2592]]\n",
      "\n",
      "  [[3505 2592 1933 ..., 1259 2592 5348]\n",
      "   [2592  895 1933 ..., 5217  971 4723]\n",
      "   [2837 1946 5638 ..., 5779 2646 5160]\n",
      "   ..., \n",
      "   [1362 6239 2713 ..., 1232 6239 4636]\n",
      "   [6149 5039 6053 ..., 2700  688 2162]\n",
      "   [5180 1003 1027 ...,   20 2592 5039]]]\n",
      "\n",
      "\n",
      " [[[5039 6239 4282 ..., 4754 6239 1946]\n",
      "   [3228 5233 2592 ...,  273 3196 3551]\n",
      "   [5217 4567 2966 ..., 1663 2592 5418]\n",
      "   ..., \n",
      "   [3196 4983 6239 ..., 2536 1362 2837]\n",
      "   [1946 4660 1307 ..., 1429  396 2166]\n",
      "   [6149 2867  622 ..., 4150 2700 5513]]\n",
      "\n",
      "  [[6239 4282 4199 ..., 6239 1946 3228]\n",
      "   [5233 2592 5348 ..., 3196 3551 5217]\n",
      "   [4567 2966 2592 ..., 2592 5418 6239]\n",
      "   ..., \n",
      "   [4983 6239 1946 ..., 1362 2837 1946]\n",
      "   [4660 1307  588 ...,  396 2166 6149]\n",
      "   [2867  622 1429 ..., 2700 5513 2700]]]]\n",
      "Epoch   0 Batch    0/575   train_loss = 8.821\n",
      "Epoch   0 Batch    1/575   train_loss = 8.819\n",
      "Epoch   0 Batch    2/575   train_loss = 8.818\n",
      "Epoch   0 Batch    3/575   train_loss = 8.815\n",
      "Epoch   0 Batch    4/575   train_loss = 8.810\n",
      "Epoch   0 Batch    5/575   train_loss = 8.809\n",
      "Epoch   0 Batch    6/575   train_loss = 8.804\n",
      "Epoch   0 Batch    7/575   train_loss = 8.803\n",
      "Epoch   0 Batch    8/575   train_loss = 8.795\n",
      "Epoch   0 Batch    9/575   train_loss = 8.787\n",
      "Epoch   0 Batch   10/575   train_loss = 8.778\n",
      "Epoch   0 Batch   11/575   train_loss = 8.769\n",
      "Epoch   0 Batch   12/575   train_loss = 8.756\n",
      "Epoch   0 Batch   13/575   train_loss = 8.737\n",
      "Epoch   0 Batch   14/575   train_loss = 8.707\n",
      "Epoch   0 Batch   15/575   train_loss = 8.692\n",
      "Epoch   0 Batch   16/575   train_loss = 8.653\n",
      "Epoch   0 Batch   17/575   train_loss = 8.529\n",
      "Epoch   0 Batch   18/575   train_loss = 8.487\n",
      "Epoch   0 Batch   19/575   train_loss = 8.501\n",
      "Epoch   0 Batch   20/575   train_loss = 8.269\n",
      "Epoch   0 Batch   21/575   train_loss = 8.165\n",
      "Epoch   0 Batch   22/575   train_loss = 8.278\n",
      "Epoch   0 Batch   23/575   train_loss = 7.937\n",
      "Epoch   0 Batch   24/575   train_loss = 7.923\n",
      "Epoch   0 Batch   25/575   train_loss = 7.743\n",
      "Epoch   0 Batch   26/575   train_loss = 7.807\n",
      "Epoch   0 Batch   27/575   train_loss = 7.372\n",
      "Epoch   0 Batch   28/575   train_loss = 7.131\n",
      "Epoch   0 Batch   29/575   train_loss = 7.257\n",
      "Epoch   0 Batch   30/575   train_loss = 7.554\n",
      "Epoch   0 Batch   31/575   train_loss = 7.704\n",
      "Epoch   0 Batch   32/575   train_loss = 7.013\n",
      "Epoch   0 Batch   33/575   train_loss = 7.051\n",
      "Epoch   0 Batch   34/575   train_loss = 6.842\n",
      "Epoch   0 Batch   35/575   train_loss = 6.813\n",
      "Epoch   0 Batch   36/575   train_loss = 7.153\n",
      "Epoch   0 Batch   37/575   train_loss = 6.161\n",
      "Epoch   0 Batch   38/575   train_loss = 6.925\n",
      "Epoch   0 Batch   39/575   train_loss = 6.235\n",
      "Epoch   0 Batch   40/575   train_loss = 6.039\n",
      "Epoch   0 Batch   41/575   train_loss = 7.697\n",
      "Epoch   0 Batch   42/575   train_loss = 6.654\n",
      "Epoch   0 Batch   43/575   train_loss = 6.788\n",
      "Epoch   0 Batch   44/575   train_loss = 6.597\n",
      "Epoch   0 Batch   45/575   train_loss = 6.449\n",
      "Epoch   0 Batch   46/575   train_loss = 6.562\n",
      "Epoch   0 Batch   47/575   train_loss = 6.403\n",
      "Epoch   0 Batch   48/575   train_loss = 6.031\n",
      "Epoch   0 Batch   49/575   train_loss = 6.058\n",
      "Epoch   0 Batch   50/575   train_loss = 6.212\n",
      "Epoch   0 Batch   51/575   train_loss = 6.288\n",
      "Epoch   0 Batch   52/575   train_loss = 6.494\n",
      "Epoch   0 Batch   53/575   train_loss = 6.555\n",
      "Epoch   0 Batch   54/575   train_loss = 6.810\n",
      "Epoch   0 Batch   55/575   train_loss = 6.569\n",
      "Epoch   0 Batch   56/575   train_loss = 6.275\n",
      "Epoch   0 Batch   57/575   train_loss = 6.495\n",
      "Epoch   0 Batch   58/575   train_loss = 6.172\n",
      "Epoch   0 Batch   59/575   train_loss = 5.968\n",
      "Epoch   0 Batch   60/575   train_loss = 6.273\n",
      "Epoch   0 Batch   61/575   train_loss = 6.514\n",
      "Epoch   0 Batch   62/575   train_loss = 6.216\n",
      "Epoch   0 Batch   63/575   train_loss = 6.902\n",
      "Epoch   0 Batch   64/575   train_loss = 6.548\n",
      "Epoch   0 Batch   65/575   train_loss = 6.648\n",
      "Epoch   0 Batch   66/575   train_loss = 6.281\n",
      "Epoch   0 Batch   67/575   train_loss = 6.354\n",
      "Epoch   0 Batch   68/575   train_loss = 6.351\n",
      "Epoch   0 Batch   69/575   train_loss = 6.096\n",
      "Epoch   0 Batch   70/575   train_loss = 6.280\n",
      "Epoch   0 Batch   71/575   train_loss = 6.290\n",
      "Epoch   0 Batch   72/575   train_loss = 6.478\n",
      "Epoch   0 Batch   73/575   train_loss = 6.500\n",
      "Epoch   0 Batch   74/575   train_loss = 6.950\n",
      "Epoch   0 Batch   75/575   train_loss = 6.352\n",
      "Epoch   0 Batch   76/575   train_loss = 6.753\n",
      "Epoch   0 Batch   77/575   train_loss = 6.599\n",
      "Epoch   0 Batch   78/575   train_loss = 6.615\n",
      "Epoch   0 Batch   79/575   train_loss = 6.374\n",
      "Epoch   0 Batch   80/575   train_loss = 6.365\n",
      "Epoch   0 Batch   81/575   train_loss = 6.180\n",
      "Epoch   0 Batch   82/575   train_loss = 6.041\n",
      "Epoch   0 Batch   83/575   train_loss = 5.989\n",
      "Epoch   0 Batch   84/575   train_loss = 6.198\n",
      "Epoch   0 Batch   85/575   train_loss = 6.948\n",
      "Epoch   0 Batch   86/575   train_loss = 6.115\n",
      "Epoch   0 Batch   87/575   train_loss = 6.470\n",
      "Epoch   0 Batch   88/575   train_loss = 5.576\n",
      "Epoch   0 Batch   89/575   train_loss = 6.168\n",
      "Epoch   0 Batch   90/575   train_loss = 6.334\n",
      "Epoch   0 Batch   91/575   train_loss = 6.405\n",
      "Epoch   0 Batch   92/575   train_loss = 6.922\n",
      "Epoch   0 Batch   93/575   train_loss = 6.528\n",
      "Epoch   0 Batch   94/575   train_loss = 6.107\n",
      "Epoch   0 Batch   95/575   train_loss = 6.267\n",
      "Epoch   0 Batch   96/575   train_loss = 6.400\n",
      "Epoch   0 Batch   97/575   train_loss = 6.381\n",
      "Epoch   0 Batch   98/575   train_loss = 6.075\n",
      "Epoch   0 Batch   99/575   train_loss = 7.094\n",
      "Epoch   0 Batch  100/575   train_loss = 6.708\n",
      "Epoch   0 Batch  101/575   train_loss = 6.020\n",
      "Epoch   0 Batch  102/575   train_loss = 5.922\n",
      "Epoch   0 Batch  103/575   train_loss = 6.533\n",
      "Epoch   0 Batch  104/575   train_loss = 6.310\n",
      "Epoch   0 Batch  105/575   train_loss = 6.294\n",
      "Epoch   0 Batch  106/575   train_loss = 6.271\n",
      "Epoch   0 Batch  107/575   train_loss = 5.972\n",
      "Epoch   0 Batch  108/575   train_loss = 6.567\n",
      "Epoch   0 Batch  109/575   train_loss = 6.370\n",
      "Epoch   0 Batch  110/575   train_loss = 5.606\n",
      "Epoch   0 Batch  111/575   train_loss = 6.083\n",
      "Epoch   0 Batch  112/575   train_loss = 6.465\n",
      "Epoch   0 Batch  113/575   train_loss = 6.692\n",
      "Epoch   0 Batch  114/575   train_loss = 6.387\n",
      "Epoch   0 Batch  115/575   train_loss = 6.095\n",
      "Epoch   0 Batch  116/575   train_loss = 6.181\n",
      "Epoch   0 Batch  117/575   train_loss = 5.841\n",
      "Epoch   0 Batch  118/575   train_loss = 6.554\n",
      "Epoch   0 Batch  119/575   train_loss = 6.781\n",
      "Epoch   0 Batch  120/575   train_loss = 6.422\n",
      "Epoch   0 Batch  121/575   train_loss = 6.557\n",
      "Epoch   0 Batch  122/575   train_loss = 6.438\n",
      "Epoch   0 Batch  123/575   train_loss = 6.065\n",
      "Epoch   0 Batch  124/575   train_loss = 6.598\n",
      "Epoch   0 Batch  125/575   train_loss = 6.514\n",
      "Epoch   0 Batch  126/575   train_loss = 6.428\n",
      "Epoch   0 Batch  127/575   train_loss = 5.800\n",
      "Epoch   0 Batch  128/575   train_loss = 6.015\n",
      "Epoch   0 Batch  129/575   train_loss = 6.279\n",
      "Epoch   0 Batch  130/575   train_loss = 6.110\n",
      "Epoch   0 Batch  131/575   train_loss = 5.780\n",
      "Epoch   0 Batch  132/575   train_loss = 5.954\n",
      "Epoch   0 Batch  133/575   train_loss = 6.288\n",
      "Epoch   0 Batch  134/575   train_loss = 6.259\n",
      "Epoch   0 Batch  135/575   train_loss = 5.752\n",
      "Epoch   0 Batch  136/575   train_loss = 7.111\n",
      "Epoch   0 Batch  137/575   train_loss = 6.229\n",
      "Epoch   0 Batch  138/575   train_loss = 6.764\n",
      "Epoch   0 Batch  139/575   train_loss = 6.650\n",
      "Epoch   0 Batch  140/575   train_loss = 6.524\n",
      "Epoch   0 Batch  141/575   train_loss = 6.948\n",
      "Epoch   0 Batch  142/575   train_loss = 7.555\n",
      "Epoch   0 Batch  143/575   train_loss = 6.467\n",
      "Epoch   0 Batch  144/575   train_loss = 6.054\n",
      "Epoch   0 Batch  145/575   train_loss = 5.981\n",
      "Epoch   0 Batch  146/575   train_loss = 5.869\n",
      "Epoch   0 Batch  147/575   train_loss = 6.002\n",
      "Epoch   0 Batch  148/575   train_loss = 5.487\n",
      "Epoch   0 Batch  149/575   train_loss = 6.109\n",
      "Epoch   0 Batch  150/575   train_loss = 6.457\n",
      "Epoch   0 Batch  151/575   train_loss = 6.198\n",
      "Epoch   0 Batch  152/575   train_loss = 5.901\n",
      "Epoch   0 Batch  153/575   train_loss = 5.607\n",
      "Epoch   0 Batch  154/575   train_loss = 5.777\n",
      "Epoch   0 Batch  155/575   train_loss = 6.319\n",
      "Epoch   0 Batch  156/575   train_loss = 5.994\n",
      "Epoch   0 Batch  157/575   train_loss = 6.175\n",
      "Epoch   0 Batch  158/575   train_loss = 6.081\n",
      "Epoch   0 Batch  159/575   train_loss = 6.615\n",
      "Epoch   0 Batch  160/575   train_loss = 6.012\n",
      "Epoch   0 Batch  161/575   train_loss = 6.557\n",
      "Epoch   0 Batch  162/575   train_loss = 6.407\n",
      "Epoch   0 Batch  163/575   train_loss = 6.061\n",
      "Epoch   0 Batch  164/575   train_loss = 6.622\n",
      "Epoch   0 Batch  165/575   train_loss = 6.561\n",
      "Epoch   0 Batch  166/575   train_loss = 5.843\n",
      "Epoch   0 Batch  167/575   train_loss = 5.942\n",
      "Epoch   0 Batch  168/575   train_loss = 6.407\n",
      "Epoch   0 Batch  169/575   train_loss = 5.945\n",
      "Epoch   0 Batch  170/575   train_loss = 6.536\n",
      "Epoch   0 Batch  171/575   train_loss = 6.134\n",
      "Epoch   0 Batch  172/575   train_loss = 6.369\n",
      "Epoch   0 Batch  173/575   train_loss = 6.179\n",
      "Epoch   0 Batch  174/575   train_loss = 6.684\n",
      "Epoch   0 Batch  175/575   train_loss = 6.363\n",
      "Epoch   0 Batch  176/575   train_loss = 6.202\n",
      "Epoch   0 Batch  177/575   train_loss = 6.388\n",
      "Epoch   0 Batch  178/575   train_loss = 6.483\n",
      "Epoch   0 Batch  179/575   train_loss = 6.159\n",
      "Epoch   0 Batch  180/575   train_loss = 6.085\n",
      "Epoch   0 Batch  181/575   train_loss = 6.538\n",
      "Epoch   0 Batch  182/575   train_loss = 6.243\n",
      "Epoch   0 Batch  183/575   train_loss = 6.538\n",
      "Epoch   0 Batch  184/575   train_loss = 5.748\n",
      "Epoch   0 Batch  185/575   train_loss = 6.024\n",
      "Epoch   0 Batch  186/575   train_loss = 6.580\n",
      "Epoch   0 Batch  187/575   train_loss = 5.994\n",
      "Epoch   0 Batch  188/575   train_loss = 6.126\n",
      "Epoch   0 Batch  189/575   train_loss = 5.977\n",
      "Epoch   0 Batch  190/575   train_loss = 6.154\n",
      "Epoch   0 Batch  191/575   train_loss = 6.579\n",
      "Epoch   0 Batch  192/575   train_loss = 6.369\n",
      "Epoch   0 Batch  193/575   train_loss = 6.187\n",
      "Epoch   0 Batch  194/575   train_loss = 6.482\n",
      "Epoch   0 Batch  195/575   train_loss = 5.860\n",
      "Epoch   0 Batch  196/575   train_loss = 5.875\n",
      "Epoch   0 Batch  197/575   train_loss = 5.769\n",
      "Epoch   0 Batch  198/575   train_loss = 6.504\n",
      "Epoch   0 Batch  199/575   train_loss = 5.725\n",
      "Epoch   0 Batch  200/575   train_loss = 5.839\n",
      "Epoch   0 Batch  201/575   train_loss = 6.108\n",
      "Epoch   0 Batch  202/575   train_loss = 5.578\n",
      "Epoch   0 Batch  203/575   train_loss = 5.863\n",
      "Epoch   0 Batch  204/575   train_loss = 5.966\n",
      "Epoch   0 Batch  205/575   train_loss = 5.937\n",
      "Epoch   0 Batch  206/575   train_loss = 6.301\n",
      "Epoch   0 Batch  207/575   train_loss = 5.686\n",
      "Epoch   0 Batch  208/575   train_loss = 5.536\n",
      "Epoch   0 Batch  209/575   train_loss = 5.861\n",
      "Epoch   0 Batch  210/575   train_loss = 5.931\n",
      "Epoch   0 Batch  211/575   train_loss = 6.927\n",
      "Epoch   0 Batch  212/575   train_loss = 6.325\n",
      "Epoch   0 Batch  213/575   train_loss = 6.205\n",
      "Epoch   0 Batch  214/575   train_loss = 5.890\n",
      "Epoch   0 Batch  215/575   train_loss = 5.454\n",
      "Epoch   0 Batch  216/575   train_loss = 5.999\n",
      "Epoch   0 Batch  217/575   train_loss = 5.779\n",
      "Epoch   0 Batch  218/575   train_loss = 6.140\n",
      "Epoch   0 Batch  219/575   train_loss = 6.175\n",
      "Epoch   0 Batch  220/575   train_loss = 5.496\n",
      "Epoch   0 Batch  221/575   train_loss = 6.310\n",
      "Epoch   0 Batch  222/575   train_loss = 6.459\n",
      "Epoch   0 Batch  223/575   train_loss = 6.286\n",
      "Epoch   0 Batch  224/575   train_loss = 5.815\n",
      "Epoch   0 Batch  225/575   train_loss = 6.234\n",
      "Epoch   0 Batch  226/575   train_loss = 6.426\n",
      "Epoch   0 Batch  227/575   train_loss = 6.371\n",
      "Epoch   0 Batch  228/575   train_loss = 6.428\n",
      "Epoch   0 Batch  229/575   train_loss = 5.379\n",
      "Epoch   0 Batch  230/575   train_loss = 5.572\n",
      "Epoch   0 Batch  231/575   train_loss = 6.031\n",
      "Epoch   0 Batch  232/575   train_loss = 5.860\n",
      "Epoch   0 Batch  233/575   train_loss = 6.320\n",
      "Epoch   0 Batch  234/575   train_loss = 6.089\n",
      "Epoch   0 Batch  235/575   train_loss = 5.656\n",
      "Epoch   0 Batch  236/575   train_loss = 5.348\n",
      "Epoch   0 Batch  237/575   train_loss = 5.876\n",
      "Epoch   0 Batch  238/575   train_loss = 5.844\n",
      "Epoch   0 Batch  239/575   train_loss = 6.080\n",
      "Epoch   0 Batch  240/575   train_loss = 6.228\n",
      "Epoch   0 Batch  241/575   train_loss = 6.561\n",
      "Epoch   0 Batch  242/575   train_loss = 5.719\n",
      "Epoch   0 Batch  243/575   train_loss = 6.291\n",
      "Epoch   0 Batch  244/575   train_loss = 5.978\n",
      "Epoch   0 Batch  245/575   train_loss = 6.155\n",
      "Epoch   0 Batch  246/575   train_loss = 6.333\n",
      "Epoch   0 Batch  247/575   train_loss = 5.723\n",
      "Epoch   0 Batch  248/575   train_loss = 6.154\n",
      "Epoch   0 Batch  249/575   train_loss = 6.126\n",
      "Epoch   0 Batch  250/575   train_loss = 5.778\n",
      "Epoch   0 Batch  251/575   train_loss = 6.339\n",
      "Epoch   0 Batch  252/575   train_loss = 6.228\n",
      "Epoch   0 Batch  253/575   train_loss = 5.913\n",
      "Epoch   0 Batch  254/575   train_loss = 5.623\n",
      "Epoch   0 Batch  255/575   train_loss = 6.178\n",
      "Epoch   0 Batch  256/575   train_loss = 5.966\n",
      "Epoch   0 Batch  257/575   train_loss = 6.196\n",
      "Epoch   0 Batch  258/575   train_loss = 6.546\n",
      "Epoch   0 Batch  259/575   train_loss = 5.848\n",
      "Epoch   0 Batch  260/575   train_loss = 5.727\n",
      "Epoch   0 Batch  261/575   train_loss = 6.665\n",
      "Epoch   0 Batch  262/575   train_loss = 6.320\n",
      "Epoch   0 Batch  263/575   train_loss = 5.950\n",
      "Epoch   0 Batch  264/575   train_loss = 5.430\n",
      "Epoch   0 Batch  265/575   train_loss = 5.292\n",
      "Epoch   0 Batch  266/575   train_loss = 5.561\n",
      "Epoch   0 Batch  267/575   train_loss = 5.919\n",
      "Epoch   0 Batch  268/575   train_loss = 5.924\n",
      "Epoch   0 Batch  269/575   train_loss = 6.012\n",
      "Epoch   0 Batch  270/575   train_loss = 6.442\n",
      "Epoch   0 Batch  271/575   train_loss = 5.767\n",
      "Epoch   0 Batch  272/575   train_loss = 6.283\n",
      "Epoch   0 Batch  273/575   train_loss = 6.138\n",
      "Epoch   0 Batch  274/575   train_loss = 6.061\n",
      "Epoch   0 Batch  275/575   train_loss = 6.188\n",
      "Epoch   0 Batch  276/575   train_loss = 6.552\n",
      "Epoch   0 Batch  277/575   train_loss = 5.590\n",
      "Epoch   0 Batch  278/575   train_loss = 5.844\n",
      "Epoch   0 Batch  279/575   train_loss = 5.877\n",
      "Epoch   0 Batch  280/575   train_loss = 6.473\n",
      "Epoch   0 Batch  281/575   train_loss = 5.548\n",
      "Epoch   0 Batch  282/575   train_loss = 5.884\n",
      "Epoch   0 Batch  283/575   train_loss = 6.427\n",
      "Epoch   0 Batch  284/575   train_loss = 6.473\n",
      "Epoch   0 Batch  285/575   train_loss = 5.886\n",
      "Epoch   0 Batch  286/575   train_loss = 6.132\n",
      "Epoch   0 Batch  287/575   train_loss = 6.238\n",
      "Epoch   0 Batch  288/575   train_loss = 5.930\n",
      "Epoch   0 Batch  289/575   train_loss = 6.077\n",
      "Epoch   0 Batch  290/575   train_loss = 6.384\n",
      "Epoch   0 Batch  291/575   train_loss = 7.028\n",
      "Epoch   0 Batch  292/575   train_loss = 6.248\n",
      "Epoch   0 Batch  293/575   train_loss = 6.617\n",
      "Epoch   0 Batch  294/575   train_loss = 6.419\n",
      "Epoch   0 Batch  295/575   train_loss = 5.898\n",
      "Epoch   0 Batch  296/575   train_loss = 5.829\n",
      "Epoch   0 Batch  297/575   train_loss = 5.924\n",
      "Epoch   0 Batch  298/575   train_loss = 6.150\n",
      "Epoch   0 Batch  299/575   train_loss = 6.173\n",
      "Epoch   0 Batch  300/575   train_loss = 5.904\n",
      "Epoch   0 Batch  301/575   train_loss = 5.949\n",
      "Epoch   0 Batch  302/575   train_loss = 6.233\n",
      "Epoch   0 Batch  303/575   train_loss = 5.555\n",
      "Epoch   0 Batch  304/575   train_loss = 5.693\n",
      "Epoch   0 Batch  305/575   train_loss = 5.773\n",
      "Epoch   0 Batch  306/575   train_loss = 6.282\n",
      "Epoch   0 Batch  307/575   train_loss = 6.331\n",
      "Epoch   0 Batch  308/575   train_loss = 6.249\n",
      "Epoch   0 Batch  309/575   train_loss = 6.398\n",
      "Epoch   0 Batch  310/575   train_loss = 6.136\n",
      "Epoch   0 Batch  311/575   train_loss = 5.510\n",
      "Epoch   0 Batch  312/575   train_loss = 6.240\n",
      "Epoch   0 Batch  313/575   train_loss = 6.486\n",
      "Epoch   0 Batch  314/575   train_loss = 6.209\n",
      "Epoch   0 Batch  315/575   train_loss = 6.611\n",
      "Epoch   0 Batch  316/575   train_loss = 6.102\n",
      "Epoch   0 Batch  317/575   train_loss = 5.572\n",
      "Epoch   0 Batch  318/575   train_loss = 5.480\n",
      "Epoch   0 Batch  319/575   train_loss = 5.981\n",
      "Epoch   0 Batch  320/575   train_loss = 5.671\n",
      "Epoch   0 Batch  321/575   train_loss = 6.668\n",
      "Epoch   0 Batch  322/575   train_loss = 5.883\n",
      "Epoch   0 Batch  323/575   train_loss = 6.154\n",
      "Epoch   0 Batch  324/575   train_loss = 5.863\n",
      "Epoch   0 Batch  325/575   train_loss = 6.190\n",
      "Epoch   0 Batch  326/575   train_loss = 5.972\n",
      "Epoch   0 Batch  327/575   train_loss = 6.214\n",
      "Epoch   0 Batch  328/575   train_loss = 6.293\n",
      "Epoch   0 Batch  329/575   train_loss = 6.299\n",
      "Epoch   0 Batch  330/575   train_loss = 5.977\n",
      "Epoch   0 Batch  331/575   train_loss = 6.605\n",
      "Epoch   0 Batch  332/575   train_loss = 6.348\n",
      "Epoch   0 Batch  333/575   train_loss = 6.313\n",
      "Epoch   0 Batch  334/575   train_loss = 5.809\n",
      "Epoch   0 Batch  335/575   train_loss = 5.933\n",
      "Epoch   0 Batch  336/575   train_loss = 6.098\n",
      "Epoch   0 Batch  337/575   train_loss = 6.190\n",
      "Epoch   0 Batch  338/575   train_loss = 5.644\n",
      "Epoch   0 Batch  339/575   train_loss = 6.100\n",
      "Epoch   0 Batch  340/575   train_loss = 5.978\n",
      "Epoch   0 Batch  341/575   train_loss = 5.884\n",
      "Epoch   0 Batch  342/575   train_loss = 5.557\n",
      "Epoch   0 Batch  343/575   train_loss = 6.177\n",
      "Epoch   0 Batch  344/575   train_loss = 6.136\n",
      "Epoch   0 Batch  345/575   train_loss = 5.849\n",
      "Epoch   0 Batch  346/575   train_loss = 6.499\n",
      "Epoch   0 Batch  347/575   train_loss = 5.719\n",
      "Epoch   0 Batch  348/575   train_loss = 5.550\n",
      "Epoch   0 Batch  349/575   train_loss = 5.584\n",
      "Epoch   0 Batch  350/575   train_loss = 6.079\n",
      "Epoch   0 Batch  351/575   train_loss = 6.460\n",
      "Epoch   0 Batch  352/575   train_loss = 6.066\n",
      "Epoch   0 Batch  353/575   train_loss = 6.202\n",
      "Epoch   0 Batch  354/575   train_loss = 5.881\n",
      "Epoch   0 Batch  355/575   train_loss = 6.540\n",
      "Epoch   0 Batch  356/575   train_loss = 5.923\n",
      "Epoch   0 Batch  357/575   train_loss = 5.770\n",
      "Epoch   0 Batch  358/575   train_loss = 6.102\n",
      "Epoch   0 Batch  359/575   train_loss = 6.582\n",
      "Epoch   0 Batch  360/575   train_loss = 5.508\n",
      "Epoch   0 Batch  361/575   train_loss = 6.306\n",
      "Epoch   0 Batch  362/575   train_loss = 5.856\n",
      "Epoch   0 Batch  363/575   train_loss = 5.977\n",
      "Epoch   0 Batch  364/575   train_loss = 5.526\n",
      "Epoch   0 Batch  365/575   train_loss = 6.141\n",
      "Epoch   0 Batch  366/575   train_loss = 5.590\n",
      "Epoch   0 Batch  367/575   train_loss = 5.967\n",
      "Epoch   0 Batch  368/575   train_loss = 6.470\n",
      "Epoch   0 Batch  369/575   train_loss = 6.174\n",
      "Epoch   0 Batch  370/575   train_loss = 6.278\n",
      "Epoch   0 Batch  371/575   train_loss = 6.352\n",
      "Epoch   0 Batch  372/575   train_loss = 5.843\n",
      "Epoch   0 Batch  373/575   train_loss = 6.258\n",
      "Epoch   0 Batch  374/575   train_loss = 5.421\n",
      "Epoch   0 Batch  375/575   train_loss = 5.682\n",
      "Epoch   0 Batch  376/575   train_loss = 5.969\n",
      "Epoch   0 Batch  377/575   train_loss = 6.285\n",
      "Epoch   0 Batch  378/575   train_loss = 6.443\n",
      "Epoch   0 Batch  379/575   train_loss = 5.905\n",
      "Epoch   0 Batch  380/575   train_loss = 5.559\n",
      "Epoch   0 Batch  381/575   train_loss = 5.940\n",
      "Epoch   0 Batch  382/575   train_loss = 6.085\n",
      "Epoch   0 Batch  383/575   train_loss = 5.846\n",
      "Epoch   0 Batch  384/575   train_loss = 5.950\n",
      "Epoch   0 Batch  385/575   train_loss = 5.642\n",
      "Epoch   0 Batch  386/575   train_loss = 5.608\n",
      "Epoch   0 Batch  387/575   train_loss = 6.213\n",
      "Epoch   0 Batch  388/575   train_loss = 5.841\n",
      "Epoch   0 Batch  389/575   train_loss = 5.724\n",
      "Epoch   0 Batch  390/575   train_loss = 6.059\n",
      "Epoch   0 Batch  391/575   train_loss = 6.425\n",
      "Epoch   0 Batch  392/575   train_loss = 5.834\n",
      "Epoch   0 Batch  393/575   train_loss = 5.837\n",
      "Epoch   0 Batch  394/575   train_loss = 6.169\n",
      "Epoch   0 Batch  395/575   train_loss = 5.351\n",
      "Epoch   0 Batch  396/575   train_loss = 5.966\n",
      "Epoch   0 Batch  397/575   train_loss = 6.320\n",
      "Epoch   0 Batch  398/575   train_loss = 6.263\n",
      "Epoch   0 Batch  399/575   train_loss = 6.263\n",
      "Epoch   0 Batch  400/575   train_loss = 6.149\n",
      "Epoch   0 Batch  401/575   train_loss = 5.512\n",
      "Epoch   0 Batch  402/575   train_loss = 5.784\n",
      "Epoch   0 Batch  403/575   train_loss = 6.125\n",
      "Epoch   0 Batch  404/575   train_loss = 6.353\n",
      "Epoch   0 Batch  405/575   train_loss = 6.195\n",
      "Epoch   0 Batch  406/575   train_loss = 5.576\n",
      "Epoch   0 Batch  407/575   train_loss = 5.776\n",
      "Epoch   0 Batch  408/575   train_loss = 6.139\n",
      "Epoch   0 Batch  409/575   train_loss = 5.767\n",
      "Epoch   0 Batch  410/575   train_loss = 5.865\n",
      "Epoch   0 Batch  411/575   train_loss = 5.834\n",
      "Epoch   0 Batch  412/575   train_loss = 6.378\n",
      "Epoch   0 Batch  413/575   train_loss = 6.776\n",
      "Epoch   0 Batch  414/575   train_loss = 6.300\n",
      "Epoch   0 Batch  415/575   train_loss = 6.664\n",
      "Epoch   0 Batch  416/575   train_loss = 5.951\n",
      "Epoch   0 Batch  417/575   train_loss = 5.787\n",
      "Epoch   0 Batch  418/575   train_loss = 5.842\n",
      "Epoch   0 Batch  419/575   train_loss = 5.801\n",
      "Epoch   0 Batch  420/575   train_loss = 5.907\n",
      "Epoch   0 Batch  421/575   train_loss = 6.474\n",
      "Epoch   0 Batch  422/575   train_loss = 5.981\n",
      "Epoch   0 Batch  423/575   train_loss = 6.865\n",
      "Epoch   0 Batch  424/575   train_loss = 6.741\n",
      "Epoch   0 Batch  425/575   train_loss = 6.341\n",
      "Epoch   0 Batch  426/575   train_loss = 6.114\n",
      "Epoch   0 Batch  427/575   train_loss = 5.898\n",
      "Epoch   0 Batch  428/575   train_loss = 5.911\n",
      "Epoch   0 Batch  429/575   train_loss = 5.600\n",
      "Epoch   0 Batch  430/575   train_loss = 5.902\n",
      "Epoch   0 Batch  431/575   train_loss = 5.962\n",
      "Epoch   0 Batch  432/575   train_loss = 6.339\n",
      "Epoch   0 Batch  433/575   train_loss = 6.339\n",
      "Epoch   0 Batch  434/575   train_loss = 6.122\n",
      "Epoch   0 Batch  435/575   train_loss = 6.098\n",
      "Epoch   0 Batch  436/575   train_loss = 5.643\n",
      "Epoch   0 Batch  437/575   train_loss = 6.153\n",
      "Epoch   0 Batch  438/575   train_loss = 6.233\n",
      "Epoch   0 Batch  439/575   train_loss = 6.021\n",
      "Epoch   0 Batch  440/575   train_loss = 6.047\n",
      "Epoch   0 Batch  441/575   train_loss = 6.164\n",
      "Epoch   0 Batch  442/575   train_loss = 6.042\n",
      "Epoch   0 Batch  443/575   train_loss = 6.058\n",
      "Epoch   0 Batch  444/575   train_loss = 6.075\n",
      "Epoch   0 Batch  445/575   train_loss = 5.852\n",
      "Epoch   0 Batch  446/575   train_loss = 5.888\n",
      "Epoch   0 Batch  447/575   train_loss = 6.175\n",
      "Epoch   0 Batch  448/575   train_loss = 5.795\n",
      "Epoch   0 Batch  449/575   train_loss = 6.174\n",
      "Epoch   0 Batch  450/575   train_loss = 6.715\n",
      "Epoch   0 Batch  451/575   train_loss = 6.595\n",
      "Epoch   0 Batch  452/575   train_loss = 6.776\n",
      "Epoch   0 Batch  453/575   train_loss = 6.215\n",
      "Epoch   0 Batch  454/575   train_loss = 6.363\n",
      "Epoch   0 Batch  455/575   train_loss = 5.779\n",
      "Epoch   0 Batch  456/575   train_loss = 6.494\n",
      "Epoch   0 Batch  457/575   train_loss = 5.704\n",
      "Epoch   0 Batch  458/575   train_loss = 6.227\n",
      "Epoch   0 Batch  459/575   train_loss = 6.408\n",
      "Epoch   0 Batch  460/575   train_loss = 6.269\n",
      "Epoch   0 Batch  461/575   train_loss = 6.424\n",
      "Epoch   0 Batch  462/575   train_loss = 5.824\n",
      "Epoch   0 Batch  463/575   train_loss = 6.030\n",
      "Epoch   0 Batch  464/575   train_loss = 5.572\n",
      "Epoch   0 Batch  465/575   train_loss = 5.949\n",
      "Epoch   0 Batch  466/575   train_loss = 5.963\n",
      "Epoch   0 Batch  467/575   train_loss = 5.499\n",
      "Epoch   0 Batch  468/575   train_loss = 5.656\n",
      "Epoch   0 Batch  469/575   train_loss = 6.315\n",
      "Epoch   0 Batch  470/575   train_loss = 6.560\n",
      "Epoch   0 Batch  471/575   train_loss = 5.686\n",
      "Epoch   0 Batch  472/575   train_loss = 6.269\n",
      "Epoch   0 Batch  473/575   train_loss = 5.939\n",
      "Epoch   0 Batch  474/575   train_loss = 6.028\n",
      "Epoch   0 Batch  475/575   train_loss = 5.745\n",
      "Epoch   0 Batch  476/575   train_loss = 5.898\n",
      "Epoch   0 Batch  477/575   train_loss = 6.293\n",
      "Epoch   0 Batch  478/575   train_loss = 6.570\n",
      "Epoch   0 Batch  479/575   train_loss = 6.169\n",
      "Epoch   0 Batch  480/575   train_loss = 6.381\n",
      "Epoch   0 Batch  481/575   train_loss = 6.373\n",
      "Epoch   0 Batch  482/575   train_loss = 5.293\n",
      "Epoch   0 Batch  483/575   train_loss = 6.837\n",
      "Epoch   0 Batch  484/575   train_loss = 6.072\n",
      "Epoch   0 Batch  485/575   train_loss = 6.367\n",
      "Epoch   0 Batch  486/575   train_loss = 6.143\n",
      "Epoch   0 Batch  487/575   train_loss = 6.086\n",
      "Epoch   0 Batch  488/575   train_loss = 5.895\n",
      "Epoch   0 Batch  489/575   train_loss = 6.270\n",
      "Epoch   0 Batch  490/575   train_loss = 6.295\n",
      "Epoch   0 Batch  491/575   train_loss = 5.584\n",
      "Epoch   0 Batch  492/575   train_loss = 5.026\n",
      "Epoch   0 Batch  493/575   train_loss = 5.494\n",
      "Epoch   0 Batch  494/575   train_loss = 5.606\n",
      "Epoch   0 Batch  495/575   train_loss = 5.865\n",
      "Epoch   0 Batch  496/575   train_loss = 6.067\n",
      "Epoch   0 Batch  497/575   train_loss = 6.256\n",
      "Epoch   0 Batch  498/575   train_loss = 5.619\n",
      "Epoch   0 Batch  499/575   train_loss = 5.754\n",
      "Epoch   0 Batch  500/575   train_loss = 5.829\n",
      "Epoch   0 Batch  501/575   train_loss = 5.957\n",
      "Epoch   0 Batch  502/575   train_loss = 5.306\n",
      "Epoch   0 Batch  503/575   train_loss = 6.052\n",
      "Epoch   0 Batch  504/575   train_loss = 5.739\n",
      "Epoch   0 Batch  505/575   train_loss = 5.745\n",
      "Epoch   0 Batch  506/575   train_loss = 6.109\n",
      "Epoch   0 Batch  507/575   train_loss = 6.236\n",
      "Epoch   0 Batch  508/575   train_loss = 6.087\n",
      "Epoch   0 Batch  509/575   train_loss = 6.105\n",
      "Epoch   0 Batch  510/575   train_loss = 5.591\n",
      "Epoch   0 Batch  511/575   train_loss = 5.374\n",
      "Epoch   0 Batch  512/575   train_loss = 6.308\n",
      "Epoch   0 Batch  513/575   train_loss = 6.090\n",
      "Epoch   0 Batch  514/575   train_loss = 5.900\n",
      "Epoch   0 Batch  515/575   train_loss = 6.078\n",
      "Epoch   0 Batch  516/575   train_loss = 6.209\n",
      "Epoch   0 Batch  517/575   train_loss = 5.819\n",
      "Epoch   0 Batch  518/575   train_loss = 5.843\n",
      "Epoch   0 Batch  519/575   train_loss = 5.765\n",
      "Epoch   0 Batch  520/575   train_loss = 5.005\n",
      "Epoch   0 Batch  521/575   train_loss = 5.705\n",
      "Epoch   0 Batch  522/575   train_loss = 5.701\n",
      "Epoch   0 Batch  523/575   train_loss = 6.660\n",
      "Epoch   0 Batch  524/575   train_loss = 7.998\n",
      "Epoch   0 Batch  525/575   train_loss = 5.665\n",
      "Epoch   0 Batch  526/575   train_loss = 5.749\n",
      "Epoch   0 Batch  527/575   train_loss = 5.957\n",
      "Epoch   0 Batch  528/575   train_loss = 5.585\n",
      "Epoch   0 Batch  529/575   train_loss = 5.867\n",
      "Epoch   0 Batch  530/575   train_loss = 6.140\n",
      "Epoch   0 Batch  531/575   train_loss = 5.985\n",
      "Epoch   0 Batch  532/575   train_loss = 5.644\n",
      "Epoch   0 Batch  533/575   train_loss = 5.727\n",
      "Epoch   0 Batch  534/575   train_loss = 5.993\n",
      "Epoch   0 Batch  535/575   train_loss = 6.565\n",
      "Epoch   0 Batch  536/575   train_loss = 6.189\n",
      "Epoch   0 Batch  537/575   train_loss = 5.427\n",
      "Epoch   0 Batch  538/575   train_loss = 5.925\n",
      "Epoch   0 Batch  539/575   train_loss = 5.600\n",
      "Epoch   0 Batch  540/575   train_loss = 6.072\n",
      "Epoch   0 Batch  541/575   train_loss = 6.411\n",
      "Epoch   0 Batch  542/575   train_loss = 6.090\n",
      "Epoch   0 Batch  543/575   train_loss = 6.441\n",
      "Epoch   0 Batch  544/575   train_loss = 5.912\n",
      "Epoch   0 Batch  545/575   train_loss = 6.311\n",
      "Epoch   0 Batch  546/575   train_loss = 6.089\n",
      "Epoch   0 Batch  547/575   train_loss = 6.087\n",
      "Epoch   0 Batch  548/575   train_loss = 6.011\n",
      "Epoch   0 Batch  549/575   train_loss = 5.667\n",
      "Epoch   0 Batch  550/575   train_loss = 5.871\n",
      "Epoch   0 Batch  551/575   train_loss = 5.735\n",
      "Epoch   0 Batch  552/575   train_loss = 6.085\n",
      "Epoch   0 Batch  553/575   train_loss = 5.868\n",
      "Epoch   0 Batch  554/575   train_loss = 6.006\n",
      "Epoch   0 Batch  555/575   train_loss = 6.321\n",
      "Epoch   0 Batch  556/575   train_loss = 5.839\n",
      "Epoch   0 Batch  557/575   train_loss = 5.359\n",
      "Epoch   0 Batch  558/575   train_loss = 5.815\n",
      "Epoch   0 Batch  559/575   train_loss = 5.900\n",
      "Epoch   0 Batch  560/575   train_loss = 5.606\n",
      "Epoch   0 Batch  561/575   train_loss = 5.754\n",
      "Epoch   0 Batch  562/575   train_loss = 5.368\n",
      "Epoch   0 Batch  563/575   train_loss = 5.911\n",
      "Epoch   0 Batch  564/575   train_loss = 5.366\n",
      "Epoch   0 Batch  565/575   train_loss = 5.279\n",
      "Epoch   0 Batch  566/575   train_loss = 5.538\n",
      "Epoch   0 Batch  567/575   train_loss = 5.247\n",
      "Epoch   0 Batch  568/575   train_loss = 5.471\n",
      "Epoch   0 Batch  569/575   train_loss = 5.575\n",
      "Epoch   0 Batch  570/575   train_loss = 5.337\n",
      "Epoch   0 Batch  571/575   train_loss = 5.496\n",
      "Epoch   0 Batch  572/575   train_loss = 4.862\n",
      "Epoch   0 Batch  573/575   train_loss = 5.261\n",
      "Epoch   0 Batch  574/575   train_loss = 6.081\n",
      "Epoch   1 Batch    0/575   train_loss = 5.779\n",
      "Epoch   1 Batch    1/575   train_loss = 5.658\n",
      "Epoch   1 Batch    2/575   train_loss = 5.722\n",
      "Epoch   1 Batch    3/575   train_loss = 5.462\n",
      "Epoch   1 Batch    4/575   train_loss = 5.333\n",
      "Epoch   1 Batch    5/575   train_loss = 5.334\n",
      "Epoch   1 Batch    6/575   train_loss = 5.317\n",
      "Epoch   1 Batch    7/575   train_loss = 5.674\n",
      "Epoch   1 Batch    8/575   train_loss = 5.585\n",
      "Epoch   1 Batch    9/575   train_loss = 5.600\n",
      "Epoch   1 Batch   10/575   train_loss = 5.047\n",
      "Epoch   1 Batch   11/575   train_loss = 5.497\n",
      "Epoch   1 Batch   12/575   train_loss = 5.610\n",
      "Epoch   1 Batch   13/575   train_loss = 5.688\n",
      "Epoch   1 Batch   14/575   train_loss = 6.018\n",
      "Epoch   1 Batch   15/575   train_loss = 5.865\n",
      "Epoch   1 Batch   16/575   train_loss = 5.789\n",
      "Epoch   1 Batch   17/575   train_loss = 4.941\n",
      "Epoch   1 Batch   18/575   train_loss = 5.867\n",
      "Epoch   1 Batch   19/575   train_loss = 5.752\n",
      "Epoch   1 Batch   20/575   train_loss = 5.395\n",
      "Epoch   1 Batch   21/575   train_loss = 5.266\n",
      "Epoch   1 Batch   22/575   train_loss = 6.079\n",
      "Epoch   1 Batch   23/575   train_loss = 5.731\n",
      "Epoch   1 Batch   24/575   train_loss = 5.446\n",
      "Epoch   1 Batch   25/575   train_loss = 5.667\n",
      "Epoch   1 Batch   26/575   train_loss = 5.787\n",
      "Epoch   1 Batch   27/575   train_loss = 5.723\n",
      "Epoch   1 Batch   28/575   train_loss = 5.098\n",
      "Epoch   1 Batch   29/575   train_loss = 5.271\n",
      "Epoch   1 Batch   30/575   train_loss = 6.057\n",
      "Epoch   1 Batch   31/575   train_loss = 6.350\n",
      "Epoch   1 Batch   32/575   train_loss = 5.552\n",
      "Epoch   1 Batch   33/575   train_loss = 5.871\n",
      "Epoch   1 Batch   34/575   train_loss = 5.335\n",
      "Epoch   1 Batch   35/575   train_loss = 5.401\n",
      "Epoch   1 Batch   36/575   train_loss = 5.620\n",
      "Epoch   1 Batch   37/575   train_loss = 4.901\n",
      "Epoch   1 Batch   38/575   train_loss = 5.574\n",
      "Epoch   1 Batch   39/575   train_loss = 4.954\n",
      "Epoch   1 Batch   40/575   train_loss = 4.881\n",
      "Epoch   1 Batch   41/575   train_loss = 6.530\n",
      "Epoch   1 Batch   42/575   train_loss = 5.513\n",
      "Epoch   1 Batch   43/575   train_loss = 5.661\n",
      "Epoch   1 Batch   44/575   train_loss = 5.490\n",
      "Epoch   1 Batch   45/575   train_loss = 5.319\n",
      "Epoch   1 Batch   46/575   train_loss = 5.338\n",
      "Epoch   1 Batch   47/575   train_loss = 5.491\n",
      "Epoch   1 Batch   48/575   train_loss = 4.802\n",
      "Epoch   1 Batch   49/575   train_loss = 4.962\n",
      "Epoch   1 Batch   50/575   train_loss = 5.380\n",
      "Epoch   1 Batch   51/575   train_loss = 5.195\n",
      "Epoch   1 Batch   52/575   train_loss = 5.215\n",
      "Epoch   1 Batch   53/575   train_loss = 5.173\n",
      "Epoch   1 Batch   54/575   train_loss = 5.679\n",
      "Epoch   1 Batch   55/575   train_loss = 5.776\n",
      "Epoch   1 Batch   56/575   train_loss = 5.230\n",
      "Epoch   1 Batch   57/575   train_loss = 5.522\n",
      "Epoch   1 Batch   58/575   train_loss = 5.130\n",
      "Epoch   1 Batch   59/575   train_loss = 4.952\n",
      "Epoch   1 Batch   60/575   train_loss = 5.614\n",
      "Epoch   1 Batch   61/575   train_loss = 5.495\n",
      "Epoch   1 Batch   62/575   train_loss = 5.228\n",
      "Epoch   1 Batch   63/575   train_loss = 5.810\n",
      "Epoch   1 Batch   64/575   train_loss = 5.723\n",
      "Epoch   1 Batch   65/575   train_loss = 5.367\n",
      "Epoch   1 Batch   66/575   train_loss = 5.258\n",
      "Epoch   1 Batch   67/575   train_loss = 5.089\n",
      "Epoch   1 Batch   68/575   train_loss = 5.304\n",
      "Epoch   1 Batch   69/575   train_loss = 4.930\n",
      "Epoch   1 Batch   70/575   train_loss = 5.555\n",
      "Epoch   1 Batch   71/575   train_loss = 5.226\n",
      "Epoch   1 Batch   72/575   train_loss = 5.704\n",
      "Epoch   1 Batch   73/575   train_loss = 5.640\n",
      "Epoch   1 Batch   74/575   train_loss = 5.986\n",
      "Epoch   1 Batch   75/575   train_loss = 5.565\n",
      "Epoch   1 Batch   76/575   train_loss = 5.922\n",
      "Epoch   1 Batch   77/575   train_loss = 5.870\n",
      "Epoch   1 Batch   78/575   train_loss = 5.709\n",
      "Epoch   1 Batch   79/575   train_loss = 5.618\n",
      "Epoch   1 Batch   80/575   train_loss = 5.350\n",
      "Epoch   1 Batch   81/575   train_loss = 5.352\n",
      "Epoch   1 Batch   82/575   train_loss = 5.163\n",
      "Epoch   1 Batch   83/575   train_loss = 5.240\n",
      "Epoch   1 Batch   84/575   train_loss = 5.463\n",
      "Epoch   1 Batch   85/575   train_loss = 6.006\n",
      "Epoch   1 Batch   86/575   train_loss = 5.363\n",
      "Epoch   1 Batch   87/575   train_loss = 5.909\n",
      "Epoch   1 Batch   88/575   train_loss = 4.809\n",
      "Epoch   1 Batch   89/575   train_loss = 5.274\n",
      "Epoch   1 Batch   90/575   train_loss = 5.450\n",
      "Epoch   1 Batch   91/575   train_loss = 5.535\n",
      "Epoch   1 Batch   92/575   train_loss = 6.144\n",
      "Epoch   1 Batch   93/575   train_loss = 5.632\n",
      "Epoch   1 Batch   94/575   train_loss = 5.156\n",
      "Epoch   1 Batch   95/575   train_loss = 5.557\n",
      "Epoch   1 Batch   96/575   train_loss = 5.587\n",
      "Epoch   1 Batch   97/575   train_loss = 5.711\n",
      "Epoch   1 Batch   98/575   train_loss = 5.363\n",
      "Epoch   1 Batch   99/575   train_loss = 6.153\n",
      "Epoch   1 Batch  100/575   train_loss = 5.911\n",
      "Epoch   1 Batch  101/575   train_loss = 5.337\n",
      "Epoch   1 Batch  102/575   train_loss = 5.174\n",
      "Epoch   1 Batch  103/575   train_loss = 5.905\n",
      "Epoch   1 Batch  104/575   train_loss = 5.501\n",
      "Epoch   1 Batch  105/575   train_loss = 5.630\n",
      "Epoch   1 Batch  106/575   train_loss = 5.404\n",
      "Epoch   1 Batch  107/575   train_loss = 5.185\n",
      "Epoch   1 Batch  108/575   train_loss = 5.696\n",
      "Epoch   1 Batch  109/575   train_loss = 5.516\n",
      "Epoch   1 Batch  110/575   train_loss = 4.743\n",
      "Epoch   1 Batch  111/575   train_loss = 5.417\n",
      "Epoch   1 Batch  112/575   train_loss = 5.663\n",
      "Epoch   1 Batch  113/575   train_loss = 6.073\n",
      "Epoch   1 Batch  114/575   train_loss = 5.715\n",
      "Epoch   1 Batch  115/575   train_loss = 5.197\n",
      "Epoch   1 Batch  116/575   train_loss = 5.620\n",
      "Epoch   1 Batch  117/575   train_loss = 5.124\n",
      "Epoch   1 Batch  118/575   train_loss = 5.717\n",
      "Epoch   1 Batch  119/575   train_loss = 6.119\n",
      "Epoch   1 Batch  120/575   train_loss = 5.625\n",
      "Epoch   1 Batch  121/575   train_loss = 5.736\n",
      "Epoch   1 Batch  122/575   train_loss = 5.882\n",
      "Epoch   1 Batch  123/575   train_loss = 5.513\n",
      "Epoch   1 Batch  124/575   train_loss = 5.839\n",
      "Epoch   1 Batch  125/575   train_loss = 5.917\n",
      "Epoch   1 Batch  126/575   train_loss = 5.695\n",
      "Epoch   1 Batch  127/575   train_loss = 5.043\n",
      "Epoch   1 Batch  128/575   train_loss = 5.246\n",
      "Epoch   1 Batch  129/575   train_loss = 5.473\n",
      "Epoch   1 Batch  130/575   train_loss = 5.629\n",
      "Epoch   1 Batch  131/575   train_loss = 4.981\n",
      "Epoch   1 Batch  132/575   train_loss = 5.341\n",
      "Epoch   1 Batch  133/575   train_loss = 5.528\n",
      "Epoch   1 Batch  134/575   train_loss = 5.457\n",
      "Epoch   1 Batch  135/575   train_loss = 5.193\n",
      "Epoch   1 Batch  136/575   train_loss = 6.585\n",
      "Epoch   1 Batch  137/575   train_loss = 5.494\n",
      "Epoch   1 Batch  138/575   train_loss = 6.063\n",
      "Epoch   1 Batch  139/575   train_loss = 5.953\n",
      "Epoch   1 Batch  140/575   train_loss = 5.965\n",
      "Epoch   1 Batch  141/575   train_loss = 6.182\n",
      "Epoch   1 Batch  142/575   train_loss = 6.842\n",
      "Epoch   1 Batch  143/575   train_loss = 5.724\n",
      "Epoch   1 Batch  144/575   train_loss = 5.277\n",
      "Epoch   1 Batch  145/575   train_loss = 5.424\n",
      "Epoch   1 Batch  146/575   train_loss = 5.415\n",
      "Epoch   1 Batch  147/575   train_loss = 5.311\n",
      "Epoch   1 Batch  148/575   train_loss = 4.905\n",
      "Epoch   1 Batch  149/575   train_loss = 5.573\n",
      "Epoch   1 Batch  150/575   train_loss = 5.802\n",
      "Epoch   1 Batch  151/575   train_loss = 5.446\n",
      "Epoch   1 Batch  152/575   train_loss = 5.259\n",
      "Epoch   1 Batch  153/575   train_loss = 4.974\n",
      "Epoch   1 Batch  154/575   train_loss = 5.005\n",
      "Epoch   1 Batch  155/575   train_loss = 5.592\n",
      "Epoch   1 Batch  156/575   train_loss = 5.409\n",
      "Epoch   1 Batch  157/575   train_loss = 5.581\n",
      "Epoch   1 Batch  158/575   train_loss = 5.472\n",
      "Epoch   1 Batch  159/575   train_loss = 5.861\n",
      "Epoch   1 Batch  160/575   train_loss = 5.253\n",
      "Epoch   1 Batch  161/575   train_loss = 5.848\n",
      "Epoch   1 Batch  162/575   train_loss = 5.837\n",
      "Epoch   1 Batch  163/575   train_loss = 5.454\n",
      "Epoch   1 Batch  164/575   train_loss = 5.827\n",
      "Epoch   1 Batch  165/575   train_loss = 5.724\n",
      "Epoch   1 Batch  166/575   train_loss = 5.198\n",
      "Epoch   1 Batch  167/575   train_loss = 5.313\n",
      "Epoch   1 Batch  168/575   train_loss = 5.640\n",
      "Epoch   1 Batch  169/575   train_loss = 5.305\n",
      "Epoch   1 Batch  170/575   train_loss = 5.715\n",
      "Epoch   1 Batch  171/575   train_loss = 5.386\n",
      "Epoch   1 Batch  172/575   train_loss = 5.803\n",
      "Epoch   1 Batch  173/575   train_loss = 5.673\n",
      "Epoch   1 Batch  174/575   train_loss = 5.995\n",
      "Epoch   1 Batch  175/575   train_loss = 5.770\n",
      "Epoch   1 Batch  176/575   train_loss = 5.682\n",
      "Epoch   1 Batch  177/575   train_loss = 5.825\n",
      "Epoch   1 Batch  178/575   train_loss = 5.622\n",
      "Epoch   1 Batch  179/575   train_loss = 5.453\n",
      "Epoch   1 Batch  180/575   train_loss = 5.411\n",
      "Epoch   1 Batch  181/575   train_loss = 5.819\n",
      "Epoch   1 Batch  182/575   train_loss = 5.413\n",
      "Epoch   1 Batch  183/575   train_loss = 5.928\n",
      "Epoch   1 Batch  184/575   train_loss = 5.137\n",
      "Epoch   1 Batch  185/575   train_loss = 5.513\n",
      "Epoch   1 Batch  186/575   train_loss = 5.823\n",
      "Epoch   1 Batch  187/575   train_loss = 5.260\n",
      "Epoch   1 Batch  188/575   train_loss = 5.459\n",
      "Epoch   1 Batch  189/575   train_loss = 5.262\n",
      "Epoch   1 Batch  190/575   train_loss = 5.553\n",
      "Epoch   1 Batch  191/575   train_loss = 6.076\n",
      "Epoch   1 Batch  192/575   train_loss = 5.785\n",
      "Epoch   1 Batch  193/575   train_loss = 5.531\n",
      "Epoch   1 Batch  194/575   train_loss = 5.663\n",
      "Epoch   1 Batch  195/575   train_loss = 5.309\n",
      "Epoch   1 Batch  196/575   train_loss = 5.191\n",
      "Epoch   1 Batch  197/575   train_loss = 5.185\n",
      "Epoch   1 Batch  198/575   train_loss = 5.817\n",
      "Epoch   1 Batch  199/575   train_loss = 5.052\n",
      "Epoch   1 Batch  200/575   train_loss = 5.278\n",
      "Epoch   1 Batch  201/575   train_loss = 5.489\n",
      "Epoch   1 Batch  202/575   train_loss = 4.958\n",
      "Epoch   1 Batch  203/575   train_loss = 5.329\n",
      "Epoch   1 Batch  204/575   train_loss = 5.382\n",
      "Epoch   1 Batch  205/575   train_loss = 5.241\n",
      "Epoch   1 Batch  206/575   train_loss = 5.576\n",
      "Epoch   1 Batch  207/575   train_loss = 5.131\n",
      "Epoch   1 Batch  208/575   train_loss = 4.917\n",
      "Epoch   1 Batch  209/575   train_loss = 5.187\n",
      "Epoch   1 Batch  210/575   train_loss = 5.390\n",
      "Epoch   1 Batch  211/575   train_loss = 6.207\n",
      "Epoch   1 Batch  212/575   train_loss = 5.766\n",
      "Epoch   1 Batch  213/575   train_loss = 5.599\n",
      "Epoch   1 Batch  214/575   train_loss = 5.298\n",
      "Epoch   1 Batch  215/575   train_loss = 4.580\n",
      "Epoch   1 Batch  216/575   train_loss = 5.315\n",
      "Epoch   1 Batch  217/575   train_loss = 5.002\n",
      "Epoch   1 Batch  218/575   train_loss = 5.571\n",
      "Epoch   1 Batch  219/575   train_loss = 5.382\n",
      "Epoch   1 Batch  220/575   train_loss = 4.915\n",
      "Epoch   1 Batch  221/575   train_loss = 5.646\n",
      "Epoch   1 Batch  222/575   train_loss = 5.644\n",
      "Epoch   1 Batch  223/575   train_loss = 5.703\n",
      "Epoch   1 Batch  224/575   train_loss = 5.095\n",
      "Epoch   1 Batch  225/575   train_loss = 5.401\n",
      "Epoch   1 Batch  226/575   train_loss = 5.763\n",
      "Epoch   1 Batch  227/575   train_loss = 5.794\n",
      "Epoch   1 Batch  228/575   train_loss = 5.772\n",
      "Epoch   1 Batch  229/575   train_loss = 4.623\n",
      "Epoch   1 Batch  230/575   train_loss = 4.808\n",
      "Epoch   1 Batch  231/575   train_loss = 5.341\n",
      "Epoch   1 Batch  232/575   train_loss = 5.292\n",
      "Epoch   1 Batch  233/575   train_loss = 5.521\n",
      "Epoch   1 Batch  234/575   train_loss = 5.512\n",
      "Epoch   1 Batch  235/575   train_loss = 5.128\n",
      "Epoch   1 Batch  236/575   train_loss = 4.699\n",
      "Epoch   1 Batch  237/575   train_loss = 5.210\n",
      "Epoch   1 Batch  238/575   train_loss = 5.205\n",
      "Epoch   1 Batch  239/575   train_loss = 5.507\n",
      "Epoch   1 Batch  240/575   train_loss = 5.643\n",
      "Epoch   1 Batch  241/575   train_loss = 5.976\n",
      "Epoch   1 Batch  242/575   train_loss = 5.200\n",
      "Epoch   1 Batch  243/575   train_loss = 5.541\n",
      "Epoch   1 Batch  244/575   train_loss = 5.340\n",
      "Epoch   1 Batch  245/575   train_loss = 5.524\n",
      "Epoch   1 Batch  246/575   train_loss = 5.809\n",
      "Epoch   1 Batch  247/575   train_loss = 5.151\n",
      "Epoch   1 Batch  248/575   train_loss = 5.526\n",
      "Epoch   1 Batch  249/575   train_loss = 5.537\n",
      "Epoch   1 Batch  250/575   train_loss = 5.163\n",
      "Epoch   1 Batch  251/575   train_loss = 5.722\n",
      "Epoch   1 Batch  252/575   train_loss = 5.657\n",
      "Epoch   1 Batch  253/575   train_loss = 5.343\n",
      "Epoch   1 Batch  254/575   train_loss = 5.067\n",
      "Epoch   1 Batch  255/575   train_loss = 5.610\n",
      "Epoch   1 Batch  256/575   train_loss = 5.270\n",
      "Epoch   1 Batch  257/575   train_loss = 5.446\n",
      "Epoch   1 Batch  258/575   train_loss = 5.740\n",
      "Epoch   1 Batch  259/575   train_loss = 5.034\n",
      "Epoch   1 Batch  260/575   train_loss = 5.084\n",
      "Epoch   1 Batch  261/575   train_loss = 6.105\n",
      "Epoch   1 Batch  262/575   train_loss = 5.754\n",
      "Epoch   1 Batch  263/575   train_loss = 5.395\n",
      "Epoch   1 Batch  264/575   train_loss = 4.910\n",
      "Epoch   1 Batch  265/575   train_loss = 4.634\n",
      "Epoch   1 Batch  266/575   train_loss = 4.987\n",
      "Epoch   1 Batch  267/575   train_loss = 5.369\n",
      "Epoch   1 Batch  268/575   train_loss = 5.475\n",
      "Epoch   1 Batch  269/575   train_loss = 5.416\n",
      "Epoch   1 Batch  270/575   train_loss = 5.905\n",
      "Epoch   1 Batch  271/575   train_loss = 5.132\n",
      "Epoch   1 Batch  272/575   train_loss = 5.726\n",
      "Epoch   1 Batch  273/575   train_loss = 5.572\n",
      "Epoch   1 Batch  274/575   train_loss = 5.492\n",
      "Epoch   1 Batch  275/575   train_loss = 5.537\n",
      "Epoch   1 Batch  276/575   train_loss = 5.994\n",
      "Epoch   1 Batch  277/575   train_loss = 4.907\n",
      "Epoch   1 Batch  278/575   train_loss = 5.303\n",
      "Epoch   1 Batch  279/575   train_loss = 5.426\n",
      "Epoch   1 Batch  280/575   train_loss = 5.915\n",
      "Epoch   1 Batch  281/575   train_loss = 5.072\n",
      "Epoch   1 Batch  282/575   train_loss = 5.368\n",
      "Epoch   1 Batch  283/575   train_loss = 5.739\n",
      "Epoch   1 Batch  284/575   train_loss = 5.828\n",
      "Epoch   1 Batch  285/575   train_loss = 5.233\n",
      "Epoch   1 Batch  286/575   train_loss = 5.472\n",
      "Epoch   1 Batch  287/575   train_loss = 5.616\n",
      "Epoch   1 Batch  288/575   train_loss = 5.412\n",
      "Epoch   1 Batch  289/575   train_loss = 5.560\n",
      "Epoch   1 Batch  290/575   train_loss = 5.710\n",
      "Epoch   1 Batch  291/575   train_loss = 6.285\n",
      "Epoch   1 Batch  292/575   train_loss = 5.769\n",
      "Epoch   1 Batch  293/575   train_loss = 6.115\n",
      "Epoch   1 Batch  294/575   train_loss = 5.789\n",
      "Epoch   1 Batch  295/575   train_loss = 5.322\n",
      "Epoch   1 Batch  296/575   train_loss = 5.300\n",
      "Epoch   1 Batch  297/575   train_loss = 5.469\n",
      "Epoch   1 Batch  298/575   train_loss = 5.659\n",
      "Epoch   1 Batch  299/575   train_loss = 5.561\n",
      "Epoch   1 Batch  300/575   train_loss = 5.320\n",
      "Epoch   1 Batch  301/575   train_loss = 5.220\n",
      "Epoch   1 Batch  302/575   train_loss = 5.741\n",
      "Epoch   1 Batch  303/575   train_loss = 5.129\n",
      "Epoch   1 Batch  304/575   train_loss = 5.197\n",
      "Epoch   1 Batch  305/575   train_loss = 5.242\n",
      "Epoch   1 Batch  306/575   train_loss = 5.698\n",
      "Epoch   1 Batch  307/575   train_loss = 5.703\n",
      "Epoch   1 Batch  308/575   train_loss = 5.683\n",
      "Epoch   1 Batch  309/575   train_loss = 5.754\n",
      "Epoch   1 Batch  310/575   train_loss = 5.626\n",
      "Epoch   1 Batch  311/575   train_loss = 5.080\n",
      "Epoch   1 Batch  312/575   train_loss = 5.632\n",
      "Epoch   1 Batch  313/575   train_loss = 5.973\n",
      "Epoch   1 Batch  314/575   train_loss = 5.680\n",
      "Epoch   1 Batch  315/575   train_loss = 6.042\n",
      "Epoch   1 Batch  316/575   train_loss = 5.425\n",
      "Epoch   1 Batch  317/575   train_loss = 5.008\n",
      "Epoch   1 Batch  318/575   train_loss = 4.804\n",
      "Epoch   1 Batch  319/575   train_loss = 5.453\n",
      "Epoch   1 Batch  320/575   train_loss = 5.120\n",
      "Epoch   1 Batch  321/575   train_loss = 5.982\n",
      "Epoch   1 Batch  322/575   train_loss = 5.423\n",
      "Epoch   1 Batch  323/575   train_loss = 5.749\n",
      "Epoch   1 Batch  324/575   train_loss = 5.297\n",
      "Epoch   1 Batch  325/575   train_loss = 5.474\n",
      "Epoch   1 Batch  326/575   train_loss = 5.268\n",
      "Epoch   1 Batch  327/575   train_loss = 5.785\n",
      "Epoch   1 Batch  328/575   train_loss = 5.660\n",
      "Epoch   1 Batch  329/575   train_loss = 5.925\n",
      "Epoch   1 Batch  330/575   train_loss = 5.560\n",
      "Epoch   1 Batch  331/575   train_loss = 6.064\n",
      "Epoch   1 Batch  332/575   train_loss = 5.744\n",
      "Epoch   1 Batch  333/575   train_loss = 5.774\n",
      "Epoch   1 Batch  334/575   train_loss = 5.245\n",
      "Epoch   1 Batch  335/575   train_loss = 5.268\n",
      "Epoch   1 Batch  336/575   train_loss = 5.567\n",
      "Epoch   1 Batch  337/575   train_loss = 5.408\n",
      "Epoch   1 Batch  338/575   train_loss = 5.193\n",
      "Epoch   1 Batch  339/575   train_loss = 5.561\n",
      "Epoch   1 Batch  340/575   train_loss = 5.338\n",
      "Epoch   1 Batch  341/575   train_loss = 5.324\n",
      "Epoch   1 Batch  342/575   train_loss = 5.085\n",
      "Epoch   1 Batch  343/575   train_loss = 5.622\n",
      "Epoch   1 Batch  344/575   train_loss = 5.607\n",
      "Epoch   1 Batch  345/575   train_loss = 5.182\n",
      "Epoch   1 Batch  346/575   train_loss = 5.991\n",
      "Epoch   1 Batch  347/575   train_loss = 5.202\n",
      "Epoch   1 Batch  348/575   train_loss = 5.190\n",
      "Epoch   1 Batch  349/575   train_loss = 4.936\n",
      "Epoch   1 Batch  350/575   train_loss = 5.571\n",
      "Epoch   1 Batch  351/575   train_loss = 5.903\n",
      "Epoch   1 Batch  352/575   train_loss = 5.695\n",
      "Epoch   1 Batch  353/575   train_loss = 5.592\n",
      "Epoch   1 Batch  354/575   train_loss = 5.341\n",
      "Epoch   1 Batch  355/575   train_loss = 5.926\n",
      "Epoch   1 Batch  356/575   train_loss = 5.471\n",
      "Epoch   1 Batch  357/575   train_loss = 5.321\n",
      "Epoch   1 Batch  358/575   train_loss = 5.656\n",
      "Epoch   1 Batch  359/575   train_loss = 6.011\n",
      "Epoch   1 Batch  360/575   train_loss = 5.005\n",
      "Epoch   1 Batch  361/575   train_loss = 5.757\n",
      "Epoch   1 Batch  362/575   train_loss = 5.398\n",
      "Epoch   1 Batch  363/575   train_loss = 5.485\n",
      "Epoch   1 Batch  364/575   train_loss = 5.037\n",
      "Epoch   1 Batch  365/575   train_loss = 5.596\n",
      "Epoch   1 Batch  366/575   train_loss = 5.113\n",
      "Epoch   1 Batch  367/575   train_loss = 5.498\n",
      "Epoch   1 Batch  368/575   train_loss = 6.026\n",
      "Epoch   1 Batch  369/575   train_loss = 5.750\n",
      "Epoch   1 Batch  370/575   train_loss = 5.756\n",
      "Epoch   1 Batch  371/575   train_loss = 5.798\n",
      "Epoch   1 Batch  372/575   train_loss = 5.354\n",
      "Epoch   1 Batch  373/575   train_loss = 5.678\n",
      "Epoch   1 Batch  374/575   train_loss = 5.008\n",
      "Epoch   1 Batch  375/575   train_loss = 5.190\n",
      "Epoch   1 Batch  376/575   train_loss = 5.507\n",
      "Epoch   1 Batch  377/575   train_loss = 5.831\n",
      "Epoch   1 Batch  378/575   train_loss = 5.828\n",
      "Epoch   1 Batch  379/575   train_loss = 5.452\n",
      "Epoch   1 Batch  380/575   train_loss = 5.046\n",
      "Epoch   1 Batch  381/575   train_loss = 5.442\n",
      "Epoch   1 Batch  382/575   train_loss = 5.673\n",
      "Epoch   1 Batch  383/575   train_loss = 5.309\n",
      "Epoch   1 Batch  384/575   train_loss = 5.597\n",
      "Epoch   1 Batch  385/575   train_loss = 5.185\n",
      "Epoch   1 Batch  386/575   train_loss = 5.144\n",
      "Epoch   1 Batch  387/575   train_loss = 5.865\n",
      "Epoch   1 Batch  388/575   train_loss = 5.329\n",
      "Epoch   1 Batch  389/575   train_loss = 5.211\n",
      "Epoch   1 Batch  390/575   train_loss = 5.554\n",
      "Epoch   1 Batch  391/575   train_loss = 5.826\n",
      "Epoch   1 Batch  392/575   train_loss = 5.355\n",
      "Epoch   1 Batch  393/575   train_loss = 5.425\n",
      "Epoch   1 Batch  394/575   train_loss = 5.732\n",
      "Epoch   1 Batch  395/575   train_loss = 4.825\n",
      "Epoch   1 Batch  396/575   train_loss = 5.407\n",
      "Epoch   1 Batch  397/575   train_loss = 5.757\n",
      "Epoch   1 Batch  398/575   train_loss = 5.738\n",
      "Epoch   1 Batch  399/575   train_loss = 5.897\n",
      "Epoch   1 Batch  400/575   train_loss = 5.657\n",
      "Epoch   1 Batch  401/575   train_loss = 5.089\n",
      "Epoch   1 Batch  402/575   train_loss = 5.395\n",
      "Epoch   1 Batch  403/575   train_loss = 5.603\n",
      "Epoch   1 Batch  404/575   train_loss = 5.825\n",
      "Epoch   1 Batch  405/575   train_loss = 5.754\n",
      "Epoch   1 Batch  406/575   train_loss = 5.166\n",
      "Epoch   1 Batch  407/575   train_loss = 5.431\n",
      "Epoch   1 Batch  408/575   train_loss = 5.546\n",
      "Epoch   1 Batch  409/575   train_loss = 5.331\n",
      "Epoch   1 Batch  410/575   train_loss = 5.360\n",
      "Epoch   1 Batch  411/575   train_loss = 5.390\n",
      "Epoch   1 Batch  412/575   train_loss = 5.969\n",
      "Epoch   1 Batch  413/575   train_loss = 5.942\n",
      "Epoch   1 Batch  414/575   train_loss = 5.704\n",
      "Epoch   1 Batch  415/575   train_loss = 6.085\n",
      "Epoch   1 Batch  416/575   train_loss = 5.431\n",
      "Epoch   1 Batch  417/575   train_loss = 5.249\n",
      "Epoch   1 Batch  418/575   train_loss = 5.384\n",
      "Epoch   1 Batch  419/575   train_loss = 5.403\n",
      "Epoch   1 Batch  420/575   train_loss = 5.445\n",
      "Epoch   1 Batch  421/575   train_loss = 5.981\n",
      "Epoch   1 Batch  422/575   train_loss = 5.566\n",
      "Epoch   1 Batch  423/575   train_loss = 6.225\n",
      "Epoch   1 Batch  424/575   train_loss = 6.200\n",
      "Epoch   1 Batch  425/575   train_loss = 5.863\n",
      "Epoch   1 Batch  426/575   train_loss = 5.613\n",
      "Epoch   1 Batch  427/575   train_loss = 5.463\n",
      "Epoch   1 Batch  428/575   train_loss = 5.624\n",
      "Epoch   1 Batch  429/575   train_loss = 5.175\n",
      "Epoch   1 Batch  430/575   train_loss = 5.498\n",
      "Epoch   1 Batch  431/575   train_loss = 5.471\n",
      "Epoch   1 Batch  432/575   train_loss = 5.933\n",
      "Epoch   1 Batch  433/575   train_loss = 5.887\n",
      "Epoch   1 Batch  434/575   train_loss = 5.688\n",
      "Epoch   1 Batch  435/575   train_loss = 5.694\n",
      "Epoch   1 Batch  436/575   train_loss = 5.204\n",
      "Epoch   1 Batch  437/575   train_loss = 5.652\n",
      "Epoch   1 Batch  438/575   train_loss = 5.816\n",
      "Epoch   1 Batch  439/575   train_loss = 5.567\n",
      "Epoch   1 Batch  440/575   train_loss = 5.709\n",
      "Epoch   1 Batch  441/575   train_loss = 5.579\n",
      "Epoch   1 Batch  442/575   train_loss = 5.592\n",
      "Epoch   1 Batch  443/575   train_loss = 5.590\n",
      "Epoch   1 Batch  444/575   train_loss = 5.685\n",
      "Epoch   1 Batch  445/575   train_loss = 5.424\n",
      "Epoch   1 Batch  446/575   train_loss = 5.315\n",
      "Epoch   1 Batch  447/575   train_loss = 5.729\n",
      "Epoch   1 Batch  448/575   train_loss = 5.424\n",
      "Epoch   1 Batch  449/575   train_loss = 5.654\n",
      "Epoch   1 Batch  450/575   train_loss = 6.286\n",
      "Epoch   1 Batch  451/575   train_loss = 6.060\n",
      "Epoch   1 Batch  452/575   train_loss = 6.176\n",
      "Epoch   1 Batch  453/575   train_loss = 5.709\n",
      "Epoch   1 Batch  454/575   train_loss = 5.950\n",
      "Epoch   1 Batch  455/575   train_loss = 5.340\n",
      "Epoch   1 Batch  456/575   train_loss = 6.004\n",
      "Epoch   1 Batch  457/575   train_loss = 5.241\n",
      "Epoch   1 Batch  458/575   train_loss = 5.911\n",
      "Epoch   1 Batch  459/575   train_loss = 5.901\n",
      "Epoch   1 Batch  460/575   train_loss = 5.673\n",
      "Epoch   1 Batch  461/575   train_loss = 5.910\n",
      "Epoch   1 Batch  462/575   train_loss = 5.465\n",
      "Epoch   1 Batch  463/575   train_loss = 5.430\n",
      "Epoch   1 Batch  464/575   train_loss = 5.146\n",
      "Epoch   1 Batch  465/575   train_loss = 5.449\n",
      "Epoch   1 Batch  466/575   train_loss = 5.365\n",
      "Epoch   1 Batch  467/575   train_loss = 4.999\n",
      "Epoch   1 Batch  468/575   train_loss = 5.273\n",
      "Epoch   1 Batch  469/575   train_loss = 5.801\n",
      "Epoch   1 Batch  470/575   train_loss = 6.064\n",
      "Epoch   1 Batch  471/575   train_loss = 5.345\n",
      "Epoch   1 Batch  472/575   train_loss = 5.808\n",
      "Epoch   1 Batch  473/575   train_loss = 5.539\n",
      "Epoch   1 Batch  474/575   train_loss = 5.603\n",
      "Epoch   1 Batch  475/575   train_loss = 5.354\n",
      "Epoch   1 Batch  476/575   train_loss = 5.437\n",
      "Epoch   1 Batch  477/575   train_loss = 5.917\n",
      "Epoch   1 Batch  478/575   train_loss = 6.053\n",
      "Epoch   1 Batch  479/575   train_loss = 5.605\n",
      "Epoch   1 Batch  480/575   train_loss = 5.856\n",
      "Epoch   1 Batch  481/575   train_loss = 5.839\n",
      "Epoch   1 Batch  482/575   train_loss = 4.964\n",
      "Epoch   1 Batch  483/575   train_loss = 6.433\n",
      "Epoch   1 Batch  484/575   train_loss = 5.623\n",
      "Epoch   1 Batch  485/575   train_loss = 5.839\n",
      "Epoch   1 Batch  486/575   train_loss = 5.733\n",
      "Epoch   1 Batch  487/575   train_loss = 5.691\n",
      "Epoch   1 Batch  488/575   train_loss = 5.620\n",
      "Epoch   1 Batch  489/575   train_loss = 5.827\n",
      "Epoch   1 Batch  490/575   train_loss = 5.891\n",
      "Epoch   1 Batch  491/575   train_loss = 5.210\n",
      "Epoch   1 Batch  492/575   train_loss = 4.780\n",
      "Epoch   1 Batch  493/575   train_loss = 5.058\n",
      "Epoch   1 Batch  494/575   train_loss = 5.256\n",
      "Epoch   1 Batch  495/575   train_loss = 5.484\n",
      "Epoch   1 Batch  496/575   train_loss = 5.671\n",
      "Epoch   1 Batch  497/575   train_loss = 5.872\n",
      "Epoch   1 Batch  498/575   train_loss = 5.255\n",
      "Epoch   1 Batch  499/575   train_loss = 5.286\n",
      "Epoch   1 Batch  500/575   train_loss = 5.433\n",
      "Epoch   1 Batch  501/575   train_loss = 5.505\n",
      "Epoch   1 Batch  502/575   train_loss = 4.945\n",
      "Epoch   1 Batch  503/575   train_loss = 5.672\n",
      "Epoch   1 Batch  504/575   train_loss = 5.125\n",
      "Epoch   1 Batch  505/575   train_loss = 5.410\n",
      "Epoch   1 Batch  506/575   train_loss = 5.731\n",
      "Epoch   1 Batch  507/575   train_loss = 5.884\n",
      "Epoch   1 Batch  508/575   train_loss = 5.703\n",
      "Epoch   1 Batch  509/575   train_loss = 5.643\n",
      "Epoch   1 Batch  510/575   train_loss = 5.081\n",
      "Epoch   1 Batch  511/575   train_loss = 4.997\n",
      "Epoch   1 Batch  512/575   train_loss = 5.921\n",
      "Epoch   1 Batch  513/575   train_loss = 5.697\n",
      "Epoch   1 Batch  514/575   train_loss = 5.487\n",
      "Epoch   1 Batch  515/575   train_loss = 5.671\n",
      "Epoch   1 Batch  516/575   train_loss = 5.871\n",
      "Epoch   1 Batch  517/575   train_loss = 5.340\n",
      "Epoch   1 Batch  518/575   train_loss = 5.434\n",
      "Epoch   1 Batch  519/575   train_loss = 5.328\n",
      "Epoch   1 Batch  520/575   train_loss = 4.673\n",
      "Epoch   1 Batch  521/575   train_loss = 5.299\n",
      "Epoch   1 Batch  522/575   train_loss = 5.253\n",
      "Epoch   1 Batch  523/575   train_loss = 6.111\n",
      "Epoch   1 Batch  524/575   train_loss = 7.232\n",
      "Epoch   1 Batch  525/575   train_loss = 5.377\n",
      "Epoch   1 Batch  526/575   train_loss = 5.331\n",
      "Epoch   1 Batch  527/575   train_loss = 5.599\n",
      "Epoch   1 Batch  528/575   train_loss = 5.273\n",
      "Epoch   1 Batch  529/575   train_loss = 5.556\n",
      "Epoch   1 Batch  530/575   train_loss = 5.713\n",
      "Epoch   1 Batch  531/575   train_loss = 5.665\n",
      "Epoch   1 Batch  532/575   train_loss = 5.324\n",
      "Epoch   1 Batch  533/575   train_loss = 5.328\n",
      "Epoch   1 Batch  534/575   train_loss = 5.443\n",
      "Epoch   1 Batch  535/575   train_loss = 5.994\n",
      "Epoch   1 Batch  536/575   train_loss = 5.672\n",
      "Epoch   1 Batch  537/575   train_loss = 5.030\n",
      "Epoch   1 Batch  538/575   train_loss = 5.549\n",
      "Epoch   1 Batch  539/575   train_loss = 5.203\n",
      "Epoch   1 Batch  540/575   train_loss = 5.660\n",
      "Epoch   1 Batch  541/575   train_loss = 6.085\n",
      "Epoch   1 Batch  542/575   train_loss = 5.639\n",
      "Epoch   1 Batch  543/575   train_loss = 5.918\n",
      "Epoch   1 Batch  544/575   train_loss = 5.567\n",
      "Epoch   1 Batch  545/575   train_loss = 5.871\n",
      "Epoch   1 Batch  546/575   train_loss = 5.718\n",
      "Epoch   1 Batch  547/575   train_loss = 5.702\n",
      "Epoch   1 Batch  548/575   train_loss = 5.682\n",
      "Epoch   1 Batch  549/575   train_loss = 5.301\n",
      "Epoch   1 Batch  550/575   train_loss = 5.406\n",
      "Epoch   1 Batch  551/575   train_loss = 5.376\n",
      "Epoch   1 Batch  552/575   train_loss = 5.593\n",
      "Epoch   1 Batch  553/575   train_loss = 5.574\n",
      "Epoch   1 Batch  554/575   train_loss = 5.554\n",
      "Epoch   1 Batch  555/575   train_loss = 5.851\n",
      "Epoch   1 Batch  556/575   train_loss = 5.397\n",
      "Epoch   1 Batch  557/575   train_loss = 4.994\n",
      "Epoch   1 Batch  558/575   train_loss = 5.445\n",
      "Epoch   1 Batch  559/575   train_loss = 5.546\n",
      "Epoch   1 Batch  560/575   train_loss = 5.099\n",
      "Epoch   1 Batch  561/575   train_loss = 5.409\n",
      "Epoch   1 Batch  562/575   train_loss = 4.965\n",
      "Epoch   1 Batch  563/575   train_loss = 5.525\n",
      "Epoch   1 Batch  564/575   train_loss = 5.021\n",
      "Epoch   1 Batch  565/575   train_loss = 4.938\n",
      "Epoch   1 Batch  566/575   train_loss = 5.207\n",
      "Epoch   1 Batch  567/575   train_loss = 4.906\n",
      "Epoch   1 Batch  568/575   train_loss = 5.200\n",
      "Epoch   1 Batch  569/575   train_loss = 5.333\n",
      "Epoch   1 Batch  570/575   train_loss = 5.019\n",
      "Epoch   1 Batch  571/575   train_loss = 5.057\n",
      "Epoch   1 Batch  572/575   train_loss = 4.533\n",
      "Epoch   1 Batch  573/575   train_loss = 5.001\n",
      "Epoch   1 Batch  574/575   train_loss = 5.729\n",
      "Epoch   2 Batch    0/575   train_loss = 5.376\n",
      "Epoch   2 Batch    1/575   train_loss = 5.202\n",
      "Epoch   2 Batch    2/575   train_loss = 5.373\n",
      "Epoch   2 Batch    3/575   train_loss = 5.123\n",
      "Epoch   2 Batch    4/575   train_loss = 5.027\n",
      "Epoch   2 Batch    5/575   train_loss = 5.024\n",
      "Epoch   2 Batch    6/575   train_loss = 4.941\n",
      "Epoch   2 Batch    7/575   train_loss = 5.330\n",
      "Epoch   2 Batch    8/575   train_loss = 5.221\n",
      "Epoch   2 Batch    9/575   train_loss = 5.352\n",
      "Epoch   2 Batch   10/575   train_loss = 4.750\n",
      "Epoch   2 Batch   11/575   train_loss = 5.127\n",
      "Epoch   2 Batch   12/575   train_loss = 5.224\n",
      "Epoch   2 Batch   13/575   train_loss = 5.448\n",
      "Epoch   2 Batch   14/575   train_loss = 5.660\n",
      "Epoch   2 Batch   15/575   train_loss = 5.508\n",
      "Epoch   2 Batch   16/575   train_loss = 5.454\n",
      "Epoch   2 Batch   17/575   train_loss = 4.622\n",
      "Epoch   2 Batch   18/575   train_loss = 5.697\n",
      "Epoch   2 Batch   19/575   train_loss = 5.404\n",
      "Epoch   2 Batch   20/575   train_loss = 4.997\n",
      "Epoch   2 Batch   21/575   train_loss = 4.988\n",
      "Epoch   2 Batch   22/575   train_loss = 5.812\n",
      "Epoch   2 Batch   23/575   train_loss = 5.445\n",
      "Epoch   2 Batch   24/575   train_loss = 5.243\n",
      "Epoch   2 Batch   25/575   train_loss = 5.395\n",
      "Epoch   2 Batch   26/575   train_loss = 5.547\n",
      "Epoch   2 Batch   27/575   train_loss = 5.480\n",
      "Epoch   2 Batch   28/575   train_loss = 4.834\n",
      "Epoch   2 Batch   29/575   train_loss = 5.013\n",
      "Epoch   2 Batch   30/575   train_loss = 5.839\n",
      "Epoch   2 Batch   31/575   train_loss = 6.113\n",
      "Epoch   2 Batch   32/575   train_loss = 5.305\n",
      "Epoch   2 Batch   33/575   train_loss = 5.649\n",
      "Epoch   2 Batch   34/575   train_loss = 5.191\n",
      "Epoch   2 Batch   35/575   train_loss = 5.168\n",
      "Epoch   2 Batch   36/575   train_loss = 5.403\n",
      "Epoch   2 Batch   37/575   train_loss = 4.694\n",
      "Epoch   2 Batch   38/575   train_loss = 5.422\n",
      "Epoch   2 Batch   39/575   train_loss = 4.748\n",
      "Epoch   2 Batch   40/575   train_loss = 4.626\n",
      "Epoch   2 Batch   41/575   train_loss = 6.329\n",
      "Epoch   2 Batch   42/575   train_loss = 5.331\n",
      "Epoch   2 Batch   43/575   train_loss = 5.425\n",
      "Epoch   2 Batch   44/575   train_loss = 5.211\n",
      "Epoch   2 Batch   45/575   train_loss = 5.109\n",
      "Epoch   2 Batch   46/575   train_loss = 5.092\n",
      "Epoch   2 Batch   47/575   train_loss = 5.294\n",
      "Epoch   2 Batch   48/575   train_loss = 4.583\n",
      "Epoch   2 Batch   49/575   train_loss = 4.801\n",
      "Epoch   2 Batch   50/575   train_loss = 5.092\n",
      "Epoch   2 Batch   51/575   train_loss = 4.957\n",
      "Epoch   2 Batch   52/575   train_loss = 4.965\n",
      "Epoch   2 Batch   53/575   train_loss = 4.894\n",
      "Epoch   2 Batch   54/575   train_loss = 5.422\n",
      "Epoch   2 Batch   55/575   train_loss = 5.594\n",
      "Epoch   2 Batch   56/575   train_loss = 4.977\n",
      "Epoch   2 Batch   57/575   train_loss = 5.261\n",
      "Epoch   2 Batch   58/575   train_loss = 4.902\n",
      "Epoch   2 Batch   59/575   train_loss = 4.704\n",
      "Epoch   2 Batch   60/575   train_loss = 5.385\n",
      "Epoch   2 Batch   61/575   train_loss = 5.291\n",
      "Epoch   2 Batch   62/575   train_loss = 4.952\n",
      "Epoch   2 Batch   63/575   train_loss = 5.592\n",
      "Epoch   2 Batch   64/575   train_loss = 5.495\n",
      "Epoch   2 Batch   65/575   train_loss = 5.094\n",
      "Epoch   2 Batch   66/575   train_loss = 5.040\n",
      "Epoch   2 Batch   67/575   train_loss = 4.826\n",
      "Epoch   2 Batch   68/575   train_loss = 5.029\n",
      "Epoch   2 Batch   69/575   train_loss = 4.639\n",
      "Epoch   2 Batch   70/575   train_loss = 5.286\n",
      "Epoch   2 Batch   71/575   train_loss = 4.908\n",
      "Epoch   2 Batch   72/575   train_loss = 5.485\n",
      "Epoch   2 Batch   73/575   train_loss = 5.448\n",
      "Epoch   2 Batch   74/575   train_loss = 5.764\n",
      "Epoch   2 Batch   75/575   train_loss = 5.325\n",
      "Epoch   2 Batch   76/575   train_loss = 5.683\n",
      "Epoch   2 Batch   77/575   train_loss = 5.669\n",
      "Epoch   2 Batch   78/575   train_loss = 5.455\n",
      "Epoch   2 Batch   79/575   train_loss = 5.382\n",
      "Epoch   2 Batch   80/575   train_loss = 5.137\n",
      "Epoch   2 Batch   81/575   train_loss = 5.125\n",
      "Epoch   2 Batch   82/575   train_loss = 4.904\n",
      "Epoch   2 Batch   83/575   train_loss = 5.022\n",
      "Epoch   2 Batch   84/575   train_loss = 5.215\n",
      "Epoch   2 Batch   85/575   train_loss = 5.796\n",
      "Epoch   2 Batch   86/575   train_loss = 5.127\n",
      "Epoch   2 Batch   87/575   train_loss = 5.689\n",
      "Epoch   2 Batch   88/575   train_loss = 4.553\n",
      "Epoch   2 Batch   89/575   train_loss = 4.942\n",
      "Epoch   2 Batch   90/575   train_loss = 5.119\n",
      "Epoch   2 Batch   91/575   train_loss = 5.281\n",
      "Epoch   2 Batch   92/575   train_loss = 5.884\n",
      "Epoch   2 Batch   93/575   train_loss = 5.401\n",
      "Epoch   2 Batch   94/575   train_loss = 4.910\n",
      "Epoch   2 Batch   95/575   train_loss = 5.302\n",
      "Epoch   2 Batch   96/575   train_loss = 5.320\n",
      "Epoch   2 Batch   97/575   train_loss = 5.457\n",
      "Epoch   2 Batch   98/575   train_loss = 5.125\n",
      "Epoch   2 Batch   99/575   train_loss = 5.845\n",
      "Epoch   2 Batch  100/575   train_loss = 5.591\n",
      "Epoch   2 Batch  101/575   train_loss = 5.071\n",
      "Epoch   2 Batch  102/575   train_loss = 4.881\n",
      "Epoch   2 Batch  103/575   train_loss = 5.660\n",
      "Epoch   2 Batch  104/575   train_loss = 5.293\n",
      "Epoch   2 Batch  105/575   train_loss = 5.416\n",
      "Epoch   2 Batch  106/575   train_loss = 5.160\n",
      "Epoch   2 Batch  107/575   train_loss = 4.894\n",
      "Epoch   2 Batch  108/575   train_loss = 5.454\n",
      "Epoch   2 Batch  109/575   train_loss = 5.220\n",
      "Epoch   2 Batch  110/575   train_loss = 4.408\n",
      "Epoch   2 Batch  111/575   train_loss = 5.139\n",
      "Epoch   2 Batch  112/575   train_loss = 5.384\n",
      "Epoch   2 Batch  113/575   train_loss = 5.839\n",
      "Epoch   2 Batch  114/575   train_loss = 5.448\n",
      "Epoch   2 Batch  115/575   train_loss = 4.917\n",
      "Epoch   2 Batch  116/575   train_loss = 5.388\n",
      "Epoch   2 Batch  117/575   train_loss = 4.874\n",
      "Epoch   2 Batch  118/575   train_loss = 5.411\n",
      "Epoch   2 Batch  119/575   train_loss = 5.925\n",
      "Epoch   2 Batch  120/575   train_loss = 5.381\n",
      "Epoch   2 Batch  121/575   train_loss = 5.468\n",
      "Epoch   2 Batch  122/575   train_loss = 5.626\n",
      "Epoch   2 Batch  123/575   train_loss = 5.300\n",
      "Epoch   2 Batch  124/575   train_loss = 5.586\n",
      "Epoch   2 Batch  125/575   train_loss = 5.588\n",
      "Epoch   2 Batch  126/575   train_loss = 5.394\n",
      "Epoch   2 Batch  127/575   train_loss = 4.740\n",
      "Epoch   2 Batch  128/575   train_loss = 4.999\n",
      "Epoch   2 Batch  129/575   train_loss = 5.128\n",
      "Epoch   2 Batch  130/575   train_loss = 5.461\n",
      "Epoch   2 Batch  131/575   train_loss = 4.707\n",
      "Epoch   2 Batch  132/575   train_loss = 5.036\n",
      "Epoch   2 Batch  133/575   train_loss = 5.252\n",
      "Epoch   2 Batch  134/575   train_loss = 5.151\n",
      "Epoch   2 Batch  135/575   train_loss = 4.836\n",
      "Epoch   2 Batch  136/575   train_loss = 6.334\n",
      "Epoch   2 Batch  137/575   train_loss = 5.235\n",
      "Epoch   2 Batch  138/575   train_loss = 5.816\n",
      "Epoch   2 Batch  139/575   train_loss = 5.638\n",
      "Epoch   2 Batch  140/575   train_loss = 5.606\n",
      "Epoch   2 Batch  141/575   train_loss = 5.889\n",
      "Epoch   2 Batch  142/575   train_loss = 6.455\n",
      "Epoch   2 Batch  143/575   train_loss = 5.445\n",
      "Epoch   2 Batch  144/575   train_loss = 4.982\n",
      "Epoch   2 Batch  145/575   train_loss = 5.131\n",
      "Epoch   2 Batch  146/575   train_loss = 5.086\n",
      "Epoch   2 Batch  147/575   train_loss = 5.062\n",
      "Epoch   2 Batch  148/575   train_loss = 4.645\n",
      "Epoch   2 Batch  149/575   train_loss = 5.392\n",
      "Epoch   2 Batch  150/575   train_loss = 5.624\n",
      "Epoch   2 Batch  151/575   train_loss = 5.100\n",
      "Epoch   2 Batch  152/575   train_loss = 4.919\n",
      "Epoch   2 Batch  153/575   train_loss = 4.544\n",
      "Epoch   2 Batch  154/575   train_loss = 4.631\n",
      "Epoch   2 Batch  155/575   train_loss = 5.326\n",
      "Epoch   2 Batch  156/575   train_loss = 5.142\n",
      "Epoch   2 Batch  157/575   train_loss = 5.285\n",
      "Epoch   2 Batch  158/575   train_loss = 5.244\n",
      "Epoch   2 Batch  159/575   train_loss = 5.574\n",
      "Epoch   2 Batch  160/575   train_loss = 4.967\n",
      "Epoch   2 Batch  161/575   train_loss = 5.631\n",
      "Epoch   2 Batch  162/575   train_loss = 5.513\n",
      "Epoch   2 Batch  163/575   train_loss = 5.132\n",
      "Epoch   2 Batch  164/575   train_loss = 5.466\n",
      "Epoch   2 Batch  165/575   train_loss = 5.417\n",
      "Epoch   2 Batch  166/575   train_loss = 4.955\n",
      "Epoch   2 Batch  167/575   train_loss = 5.053\n",
      "Epoch   2 Batch  168/575   train_loss = 5.405\n",
      "Epoch   2 Batch  169/575   train_loss = 5.011\n",
      "Epoch   2 Batch  170/575   train_loss = 5.424\n",
      "Epoch   2 Batch  171/575   train_loss = 5.095\n",
      "Epoch   2 Batch  172/575   train_loss = 5.420\n",
      "Epoch   2 Batch  173/575   train_loss = 5.336\n",
      "Epoch   2 Batch  174/575   train_loss = 5.689\n",
      "Epoch   2 Batch  175/575   train_loss = 5.470\n",
      "Epoch   2 Batch  176/575   train_loss = 5.384\n",
      "Epoch   2 Batch  177/575   train_loss = 5.552\n",
      "Epoch   2 Batch  178/575   train_loss = 5.309\n",
      "Epoch   2 Batch  179/575   train_loss = 5.087\n",
      "Epoch   2 Batch  180/575   train_loss = 5.138\n",
      "Epoch   2 Batch  181/575   train_loss = 5.553\n",
      "Epoch   2 Batch  182/575   train_loss = 5.045\n",
      "Epoch   2 Batch  183/575   train_loss = 5.545\n",
      "Epoch   2 Batch  184/575   train_loss = 4.709\n",
      "Epoch   2 Batch  185/575   train_loss = 5.183\n",
      "Epoch   2 Batch  186/575   train_loss = 5.474\n",
      "Epoch   2 Batch  187/575   train_loss = 5.047\n",
      "Epoch   2 Batch  188/575   train_loss = 5.138\n",
      "Epoch   2 Batch  189/575   train_loss = 4.994\n",
      "Epoch   2 Batch  190/575   train_loss = 5.274\n",
      "Epoch   2 Batch  191/575   train_loss = 5.784\n",
      "Epoch   2 Batch  192/575   train_loss = 5.541\n",
      "Epoch   2 Batch  193/575   train_loss = 5.228\n",
      "Epoch   2 Batch  194/575   train_loss = 5.381\n",
      "Epoch   2 Batch  195/575   train_loss = 5.020\n",
      "Epoch   2 Batch  196/575   train_loss = 4.955\n",
      "Epoch   2 Batch  197/575   train_loss = 4.877\n",
      "Epoch   2 Batch  198/575   train_loss = 5.542\n",
      "Epoch   2 Batch  199/575   train_loss = 4.713\n",
      "Epoch   2 Batch  200/575   train_loss = 4.974\n",
      "Epoch   2 Batch  201/575   train_loss = 5.224\n",
      "Epoch   2 Batch  202/575   train_loss = 4.730\n",
      "Epoch   2 Batch  203/575   train_loss = 5.051\n",
      "Epoch   2 Batch  204/575   train_loss = 5.080\n",
      "Epoch   2 Batch  205/575   train_loss = 4.867\n",
      "Epoch   2 Batch  206/575   train_loss = 5.259\n",
      "Epoch   2 Batch  207/575   train_loss = 4.870\n",
      "Epoch   2 Batch  208/575   train_loss = 4.666\n",
      "Epoch   2 Batch  209/575   train_loss = 4.857\n",
      "Epoch   2 Batch  210/575   train_loss = 5.103\n",
      "Epoch   2 Batch  211/575   train_loss = 5.871\n",
      "Epoch   2 Batch  212/575   train_loss = 5.424\n",
      "Epoch   2 Batch  213/575   train_loss = 5.284\n",
      "Epoch   2 Batch  214/575   train_loss = 5.041\n",
      "Epoch   2 Batch  215/575   train_loss = 4.304\n",
      "Epoch   2 Batch  216/575   train_loss = 5.007\n",
      "Epoch   2 Batch  217/575   train_loss = 4.647\n",
      "Epoch   2 Batch  218/575   train_loss = 5.288\n",
      "Epoch   2 Batch  219/575   train_loss = 5.029\n",
      "Epoch   2 Batch  220/575   train_loss = 4.548\n",
      "Epoch   2 Batch  221/575   train_loss = 5.287\n",
      "Epoch   2 Batch  222/575   train_loss = 5.338\n",
      "Epoch   2 Batch  223/575   train_loss = 5.510\n",
      "Epoch   2 Batch  224/575   train_loss = 4.765\n",
      "Epoch   2 Batch  225/575   train_loss = 5.060\n",
      "Epoch   2 Batch  226/575   train_loss = 5.349\n",
      "Epoch   2 Batch  227/575   train_loss = 5.551\n",
      "Epoch   2 Batch  228/575   train_loss = 5.447\n",
      "Epoch   2 Batch  229/575   train_loss = 4.287\n",
      "Epoch   2 Batch  230/575   train_loss = 4.479\n",
      "Epoch   2 Batch  231/575   train_loss = 5.075\n",
      "Epoch   2 Batch  232/575   train_loss = 5.079\n",
      "Epoch   2 Batch  233/575   train_loss = 5.261\n",
      "Epoch   2 Batch  234/575   train_loss = 5.237\n",
      "Epoch   2 Batch  235/575   train_loss = 4.842\n",
      "Epoch   2 Batch  236/575   train_loss = 4.470\n",
      "Epoch   2 Batch  237/575   train_loss = 4.850\n",
      "Epoch   2 Batch  238/575   train_loss = 4.902\n",
      "Epoch   2 Batch  239/575   train_loss = 5.282\n",
      "Epoch   2 Batch  240/575   train_loss = 5.351\n",
      "Epoch   2 Batch  241/575   train_loss = 5.643\n",
      "Epoch   2 Batch  242/575   train_loss = 4.959\n",
      "Epoch   2 Batch  243/575   train_loss = 5.208\n",
      "Epoch   2 Batch  244/575   train_loss = 5.119\n",
      "Epoch   2 Batch  245/575   train_loss = 5.241\n",
      "Epoch   2 Batch  246/575   train_loss = 5.533\n",
      "Epoch   2 Batch  247/575   train_loss = 4.921\n",
      "Epoch   2 Batch  248/575   train_loss = 5.288\n",
      "Epoch   2 Batch  249/575   train_loss = 5.255\n",
      "Epoch   2 Batch  250/575   train_loss = 4.969\n",
      "Epoch   2 Batch  251/575   train_loss = 5.300\n",
      "Epoch   2 Batch  252/575   train_loss = 5.437\n",
      "Epoch   2 Batch  253/575   train_loss = 5.082\n",
      "Epoch   2 Batch  254/575   train_loss = 4.728\n",
      "Epoch   2 Batch  255/575   train_loss = 5.368\n",
      "Epoch   2 Batch  256/575   train_loss = 4.977\n",
      "Epoch   2 Batch  257/575   train_loss = 5.174\n",
      "Epoch   2 Batch  258/575   train_loss = 5.511\n",
      "Epoch   2 Batch  259/575   train_loss = 4.756\n",
      "Epoch   2 Batch  260/575   train_loss = 4.761\n",
      "Epoch   2 Batch  261/575   train_loss = 5.803\n",
      "Epoch   2 Batch  262/575   train_loss = 5.487\n",
      "Epoch   2 Batch  263/575   train_loss = 5.152\n",
      "Epoch   2 Batch  264/575   train_loss = 4.651\n",
      "Epoch   2 Batch  265/575   train_loss = 4.340\n",
      "Epoch   2 Batch  266/575   train_loss = 4.656\n",
      "Epoch   2 Batch  267/575   train_loss = 5.122\n",
      "Epoch   2 Batch  268/575   train_loss = 5.157\n",
      "Epoch   2 Batch  269/575   train_loss = 5.160\n",
      "Epoch   2 Batch  270/575   train_loss = 5.654\n",
      "Epoch   2 Batch  271/575   train_loss = 4.833\n",
      "Epoch   2 Batch  272/575   train_loss = 5.431\n",
      "Epoch   2 Batch  273/575   train_loss = 5.311\n",
      "Epoch   2 Batch  274/575   train_loss = 5.251\n",
      "Epoch   2 Batch  275/575   train_loss = 5.325\n",
      "Epoch   2 Batch  276/575   train_loss = 5.682\n",
      "Epoch   2 Batch  277/575   train_loss = 4.433\n",
      "Epoch   2 Batch  278/575   train_loss = 5.041\n",
      "Epoch   2 Batch  279/575   train_loss = 5.165\n",
      "Epoch   2 Batch  280/575   train_loss = 5.522\n",
      "Epoch   2 Batch  281/575   train_loss = 4.865\n",
      "Epoch   2 Batch  282/575   train_loss = 5.160\n",
      "Epoch   2 Batch  283/575   train_loss = 5.468\n",
      "Epoch   2 Batch  284/575   train_loss = 5.625\n",
      "Epoch   2 Batch  285/575   train_loss = 4.961\n",
      "Epoch   2 Batch  286/575   train_loss = 5.240\n",
      "Epoch   2 Batch  287/575   train_loss = 5.432\n",
      "Epoch   2 Batch  288/575   train_loss = 5.144\n",
      "Epoch   2 Batch  289/575   train_loss = 5.242\n",
      "Epoch   2 Batch  290/575   train_loss = 5.335\n",
      "Epoch   2 Batch  291/575   train_loss = 5.905\n",
      "Epoch   2 Batch  292/575   train_loss = 5.594\n",
      "Epoch   2 Batch  293/575   train_loss = 5.874\n",
      "Epoch   2 Batch  294/575   train_loss = 5.558\n",
      "Epoch   2 Batch  295/575   train_loss = 5.089\n",
      "Epoch   2 Batch  296/575   train_loss = 5.041\n",
      "Epoch   2 Batch  297/575   train_loss = 5.271\n",
      "Epoch   2 Batch  298/575   train_loss = 5.420\n",
      "Epoch   2 Batch  299/575   train_loss = 5.298\n",
      "Epoch   2 Batch  300/575   train_loss = 5.067\n",
      "Epoch   2 Batch  301/575   train_loss = 4.968\n",
      "Epoch   2 Batch  302/575   train_loss = 5.498\n",
      "Epoch   2 Batch  303/575   train_loss = 4.844\n",
      "Epoch   2 Batch  304/575   train_loss = 4.947\n",
      "Epoch   2 Batch  305/575   train_loss = 4.992\n",
      "Epoch   2 Batch  306/575   train_loss = 5.406\n",
      "Epoch   2 Batch  307/575   train_loss = 5.420\n",
      "Epoch   2 Batch  308/575   train_loss = 5.367\n",
      "Epoch   2 Batch  309/575   train_loss = 5.596\n",
      "Epoch   2 Batch  310/575   train_loss = 5.494\n",
      "Epoch   2 Batch  311/575   train_loss = 4.828\n",
      "Epoch   2 Batch  312/575   train_loss = 5.414\n",
      "Epoch   2 Batch  313/575   train_loss = 5.703\n",
      "Epoch   2 Batch  314/575   train_loss = 5.487\n",
      "Epoch   2 Batch  315/575   train_loss = 5.829\n",
      "Epoch   2 Batch  316/575   train_loss = 5.152\n",
      "Epoch   2 Batch  317/575   train_loss = 4.754\n",
      "Epoch   2 Batch  318/575   train_loss = 4.507\n",
      "Epoch   2 Batch  319/575   train_loss = 5.192\n",
      "Epoch   2 Batch  320/575   train_loss = 4.849\n",
      "Epoch   2 Batch  321/575   train_loss = 5.697\n",
      "Epoch   2 Batch  322/575   train_loss = 5.134\n",
      "Epoch   2 Batch  323/575   train_loss = 5.536\n",
      "Epoch   2 Batch  324/575   train_loss = 5.027\n",
      "Epoch   2 Batch  325/575   train_loss = 5.169\n",
      "Epoch   2 Batch  326/575   train_loss = 5.064\n",
      "Epoch   2 Batch  327/575   train_loss = 5.506\n",
      "Epoch   2 Batch  328/575   train_loss = 5.338\n",
      "Epoch   2 Batch  329/575   train_loss = 5.679\n",
      "Epoch   2 Batch  330/575   train_loss = 5.285\n",
      "Epoch   2 Batch  331/575   train_loss = 5.785\n",
      "Epoch   2 Batch  332/575   train_loss = 5.524\n",
      "Epoch   2 Batch  333/575   train_loss = 5.470\n",
      "Epoch   2 Batch  334/575   train_loss = 5.075\n",
      "Epoch   2 Batch  335/575   train_loss = 5.043\n",
      "Epoch   2 Batch  336/575   train_loss = 5.309\n",
      "Epoch   2 Batch  337/575   train_loss = 5.186\n",
      "Epoch   2 Batch  338/575   train_loss = 4.887\n",
      "Epoch   2 Batch  339/575   train_loss = 5.252\n",
      "Epoch   2 Batch  340/575   train_loss = 5.076\n",
      "Epoch   2 Batch  341/575   train_loss = 5.000\n",
      "Epoch   2 Batch  342/575   train_loss = 4.870\n",
      "Epoch   2 Batch  343/575   train_loss = 5.300\n",
      "Epoch   2 Batch  344/575   train_loss = 5.333\n",
      "Epoch   2 Batch  345/575   train_loss = 4.946\n",
      "Epoch   2 Batch  346/575   train_loss = 5.776\n",
      "Epoch   2 Batch  347/575   train_loss = 4.925\n",
      "Epoch   2 Batch  348/575   train_loss = 4.915\n",
      "Epoch   2 Batch  349/575   train_loss = 4.651\n",
      "Epoch   2 Batch  350/575   train_loss = 5.294\n",
      "Epoch   2 Batch  351/575   train_loss = 5.670\n",
      "Epoch   2 Batch  352/575   train_loss = 5.460\n",
      "Epoch   2 Batch  353/575   train_loss = 5.380\n",
      "Epoch   2 Batch  354/575   train_loss = 5.095\n",
      "Epoch   2 Batch  355/575   train_loss = 5.678\n",
      "Epoch   2 Batch  356/575   train_loss = 5.228\n",
      "Epoch   2 Batch  357/575   train_loss = 5.139\n",
      "Epoch   2 Batch  358/575   train_loss = 5.433\n",
      "Epoch   2 Batch  359/575   train_loss = 5.770\n",
      "Epoch   2 Batch  360/575   train_loss = 4.724\n",
      "Epoch   2 Batch  361/575   train_loss = 5.462\n",
      "Epoch   2 Batch  362/575   train_loss = 5.197\n",
      "Epoch   2 Batch  363/575   train_loss = 5.225\n",
      "Epoch   2 Batch  364/575   train_loss = 4.786\n",
      "Epoch   2 Batch  365/575   train_loss = 5.322\n",
      "Epoch   2 Batch  366/575   train_loss = 4.802\n",
      "Epoch   2 Batch  367/575   train_loss = 5.178\n",
      "Epoch   2 Batch  368/575   train_loss = 5.791\n",
      "Epoch   2 Batch  369/575   train_loss = 5.498\n",
      "Epoch   2 Batch  370/575   train_loss = 5.450\n",
      "Epoch   2 Batch  371/575   train_loss = 5.563\n",
      "Epoch   2 Batch  372/575   train_loss = 5.106\n",
      "Epoch   2 Batch  373/575   train_loss = 5.414\n",
      "Epoch   2 Batch  374/575   train_loss = 4.661\n",
      "Epoch   2 Batch  375/575   train_loss = 4.923\n",
      "Epoch   2 Batch  376/575   train_loss = 5.278\n",
      "Epoch   2 Batch  377/575   train_loss = 5.625\n",
      "Epoch   2 Batch  378/575   train_loss = 5.628\n",
      "Epoch   2 Batch  379/575   train_loss = 5.073\n",
      "Epoch   2 Batch  380/575   train_loss = 4.766\n",
      "Epoch   2 Batch  381/575   train_loss = 5.065\n",
      "Epoch   2 Batch  382/575   train_loss = 5.399\n",
      "Epoch   2 Batch  383/575   train_loss = 5.075\n",
      "Epoch   2 Batch  384/575   train_loss = 5.247\n",
      "Epoch   2 Batch  385/575   train_loss = 4.936\n",
      "Epoch   2 Batch  386/575   train_loss = 4.903\n",
      "Epoch   2 Batch  387/575   train_loss = 5.656\n",
      "Epoch   2 Batch  388/575   train_loss = 5.023\n",
      "Epoch   2 Batch  389/575   train_loss = 4.993\n",
      "Epoch   2 Batch  390/575   train_loss = 5.237\n",
      "Epoch   2 Batch  391/575   train_loss = 5.464\n",
      "Epoch   2 Batch  392/575   train_loss = 5.115\n",
      "Epoch   2 Batch  393/575   train_loss = 5.152\n",
      "Epoch   2 Batch  394/575   train_loss = 5.478\n",
      "Epoch   2 Batch  395/575   train_loss = 4.542\n",
      "Epoch   2 Batch  396/575   train_loss = 5.011\n",
      "Epoch   2 Batch  397/575   train_loss = 5.455\n",
      "Epoch   2 Batch  398/575   train_loss = 5.522\n",
      "Epoch   2 Batch  399/575   train_loss = 5.641\n",
      "Epoch   2 Batch  400/575   train_loss = 5.432\n",
      "Epoch   2 Batch  401/575   train_loss = 4.790\n",
      "Epoch   2 Batch  402/575   train_loss = 5.045\n",
      "Epoch   2 Batch  403/575   train_loss = 5.403\n",
      "Epoch   2 Batch  404/575   train_loss = 5.494\n",
      "Epoch   2 Batch  405/575   train_loss = 5.522\n",
      "Epoch   2 Batch  406/575   train_loss = 4.882\n",
      "Epoch   2 Batch  407/575   train_loss = 5.188\n",
      "Epoch   2 Batch  408/575   train_loss = 5.254\n",
      "Epoch   2 Batch  409/575   train_loss = 5.090\n",
      "Epoch   2 Batch  410/575   train_loss = 4.948\n",
      "Epoch   2 Batch  411/575   train_loss = 5.160\n",
      "Epoch   2 Batch  412/575   train_loss = 5.772\n",
      "Epoch   2 Batch  413/575   train_loss = 5.520\n",
      "Epoch   2 Batch  414/575   train_loss = 5.339\n",
      "Epoch   2 Batch  415/575   train_loss = 5.788\n",
      "Epoch   2 Batch  416/575   train_loss = 5.129\n",
      "Epoch   2 Batch  417/575   train_loss = 4.862\n",
      "Epoch   2 Batch  418/575   train_loss = 5.159\n",
      "Epoch   2 Batch  419/575   train_loss = 5.131\n",
      "Epoch   2 Batch  420/575   train_loss = 5.149\n",
      "Epoch   2 Batch  421/575   train_loss = 5.754\n",
      "Epoch   2 Batch  422/575   train_loss = 5.379\n",
      "Epoch   2 Batch  423/575   train_loss = 5.907\n",
      "Epoch   2 Batch  424/575   train_loss = 5.920\n",
      "Epoch   2 Batch  425/575   train_loss = 5.551\n",
      "Epoch   2 Batch  426/575   train_loss = 5.337\n",
      "Epoch   2 Batch  427/575   train_loss = 5.177\n",
      "Epoch   2 Batch  428/575   train_loss = 5.301\n",
      "Epoch   2 Batch  429/575   train_loss = 4.848\n",
      "Epoch   2 Batch  430/575   train_loss = 5.258\n",
      "Epoch   2 Batch  431/575   train_loss = 5.194\n",
      "Epoch   2 Batch  432/575   train_loss = 5.610\n",
      "Epoch   2 Batch  433/575   train_loss = 5.616\n",
      "Epoch   2 Batch  434/575   train_loss = 5.379\n",
      "Epoch   2 Batch  435/575   train_loss = 5.349\n",
      "Epoch   2 Batch  436/575   train_loss = 4.928\n",
      "Epoch   2 Batch  437/575   train_loss = 5.301\n",
      "Epoch   2 Batch  438/575   train_loss = 5.435\n",
      "Epoch   2 Batch  439/575   train_loss = 5.260\n",
      "Epoch   2 Batch  440/575   train_loss = 5.495\n",
      "Epoch   2 Batch  441/575   train_loss = 5.300\n",
      "Epoch   2 Batch  442/575   train_loss = 5.365\n",
      "Epoch   2 Batch  443/575   train_loss = 5.274\n",
      "Epoch   2 Batch  444/575   train_loss = 5.465\n",
      "Epoch   2 Batch  445/575   train_loss = 5.041\n",
      "Epoch   2 Batch  446/575   train_loss = 4.984\n",
      "Epoch   2 Batch  447/575   train_loss = 5.531\n",
      "Epoch   2 Batch  448/575   train_loss = 5.199\n",
      "Epoch   2 Batch  449/575   train_loss = 5.311\n",
      "Epoch   2 Batch  450/575   train_loss = 5.970\n",
      "Epoch   2 Batch  451/575   train_loss = 5.771\n",
      "Epoch   2 Batch  452/575   train_loss = 5.903\n",
      "Epoch   2 Batch  453/575   train_loss = 5.399\n",
      "Epoch   2 Batch  454/575   train_loss = 5.734\n",
      "Epoch   2 Batch  455/575   train_loss = 5.005\n",
      "Epoch   2 Batch  456/575   train_loss = 5.713\n",
      "Epoch   2 Batch  457/575   train_loss = 4.881\n",
      "Epoch   2 Batch  458/575   train_loss = 5.633\n",
      "Epoch   2 Batch  459/575   train_loss = 5.532\n",
      "Epoch   2 Batch  460/575   train_loss = 5.296\n",
      "Epoch   2 Batch  461/575   train_loss = 5.502\n",
      "Epoch   2 Batch  462/575   train_loss = 5.206\n",
      "Epoch   2 Batch  463/575   train_loss = 5.153\n",
      "Epoch   2 Batch  464/575   train_loss = 4.955\n",
      "Epoch   2 Batch  465/575   train_loss = 5.099\n",
      "Epoch   2 Batch  466/575   train_loss = 5.093\n",
      "Epoch   2 Batch  467/575   train_loss = 4.635\n",
      "Epoch   2 Batch  468/575   train_loss = 5.001\n",
      "Epoch   2 Batch  469/575   train_loss = 5.603\n",
      "Epoch   2 Batch  470/575   train_loss = 5.829\n",
      "Epoch   2 Batch  471/575   train_loss = 5.028\n",
      "Epoch   2 Batch  472/575   train_loss = 5.508\n",
      "Epoch   2 Batch  473/575   train_loss = 5.261\n",
      "Epoch   2 Batch  474/575   train_loss = 5.273\n",
      "Epoch   2 Batch  475/575   train_loss = 4.995\n",
      "Epoch   2 Batch  476/575   train_loss = 5.187\n",
      "Epoch   2 Batch  477/575   train_loss = 5.702\n",
      "Epoch   2 Batch  478/575   train_loss = 5.822\n",
      "Epoch   2 Batch  479/575   train_loss = 5.254\n",
      "Epoch   2 Batch  480/575   train_loss = 5.490\n",
      "Epoch   2 Batch  481/575   train_loss = 5.522\n",
      "Epoch   2 Batch  482/575   train_loss = 4.743\n",
      "Epoch   2 Batch  483/575   train_loss = 6.170\n",
      "Epoch   2 Batch  484/575   train_loss = 5.310\n",
      "Epoch   2 Batch  485/575   train_loss = 5.652\n",
      "Epoch   2 Batch  486/575   train_loss = 5.445\n",
      "Epoch   2 Batch  487/575   train_loss = 5.501\n",
      "Epoch   2 Batch  488/575   train_loss = 5.296\n",
      "Epoch   2 Batch  489/575   train_loss = 5.534\n",
      "Epoch   2 Batch  490/575   train_loss = 5.598\n",
      "Epoch   2 Batch  491/575   train_loss = 4.782\n",
      "Epoch   2 Batch  492/575   train_loss = 4.531\n",
      "Epoch   2 Batch  493/575   train_loss = 4.744\n",
      "Epoch   2 Batch  494/575   train_loss = 4.974\n",
      "Epoch   2 Batch  495/575   train_loss = 5.220\n",
      "Epoch   2 Batch  496/575   train_loss = 5.367\n",
      "Epoch   2 Batch  497/575   train_loss = 5.593\n",
      "Epoch   2 Batch  498/575   train_loss = 5.008\n",
      "Epoch   2 Batch  499/575   train_loss = 5.018\n",
      "Epoch   2 Batch  500/575   train_loss = 5.209\n",
      "Epoch   2 Batch  501/575   train_loss = 5.205\n",
      "Epoch   2 Batch  502/575   train_loss = 4.602\n",
      "Epoch   2 Batch  503/575   train_loss = 5.396\n",
      "Epoch   2 Batch  504/575   train_loss = 4.814\n",
      "Epoch   2 Batch  505/575   train_loss = 5.162\n",
      "Epoch   2 Batch  506/575   train_loss = 5.493\n",
      "Epoch   2 Batch  507/575   train_loss = 5.577\n",
      "Epoch   2 Batch  508/575   train_loss = 5.437\n",
      "Epoch   2 Batch  509/575   train_loss = 5.339\n",
      "Epoch   2 Batch  510/575   train_loss = 4.719\n",
      "Epoch   2 Batch  511/575   train_loss = 4.635\n",
      "Epoch   2 Batch  512/575   train_loss = 5.660\n",
      "Epoch   2 Batch  513/575   train_loss = 5.382\n",
      "Epoch   2 Batch  514/575   train_loss = 5.206\n",
      "Epoch   2 Batch  515/575   train_loss = 5.321\n",
      "Epoch   2 Batch  516/575   train_loss = 5.580\n",
      "Epoch   2 Batch  517/575   train_loss = 4.895\n",
      "Epoch   2 Batch  518/575   train_loss = 5.088\n",
      "Epoch   2 Batch  519/575   train_loss = 5.082\n",
      "Epoch   2 Batch  520/575   train_loss = 4.341\n",
      "Epoch   2 Batch  521/575   train_loss = 4.924\n",
      "Epoch   2 Batch  522/575   train_loss = 4.913\n",
      "Epoch   2 Batch  523/575   train_loss = 5.771\n",
      "Epoch   2 Batch  524/575   train_loss = 6.837\n",
      "Epoch   2 Batch  525/575   train_loss = 5.201\n",
      "Epoch   2 Batch  526/575   train_loss = 5.010\n",
      "Epoch   2 Batch  527/575   train_loss = 5.351\n",
      "Epoch   2 Batch  528/575   train_loss = 5.025\n",
      "Epoch   2 Batch  529/575   train_loss = 5.375\n",
      "Epoch   2 Batch  530/575   train_loss = 5.382\n",
      "Epoch   2 Batch  531/575   train_loss = 5.388\n",
      "Epoch   2 Batch  532/575   train_loss = 5.035\n",
      "Epoch   2 Batch  533/575   train_loss = 5.036\n",
      "Epoch   2 Batch  534/575   train_loss = 5.052\n",
      "Epoch   2 Batch  535/575   train_loss = 5.589\n",
      "Epoch   2 Batch  536/575   train_loss = 5.238\n",
      "Epoch   2 Batch  537/575   train_loss = 4.766\n",
      "Epoch   2 Batch  538/575   train_loss = 5.238\n",
      "Epoch   2 Batch  539/575   train_loss = 4.979\n",
      "Epoch   2 Batch  540/575   train_loss = 5.347\n",
      "Epoch   2 Batch  541/575   train_loss = 5.804\n",
      "Epoch   2 Batch  542/575   train_loss = 5.321\n",
      "Epoch   2 Batch  543/575   train_loss = 5.579\n",
      "Epoch   2 Batch  544/575   train_loss = 5.318\n",
      "Epoch   2 Batch  545/575   train_loss = 5.435\n",
      "Epoch   2 Batch  546/575   train_loss = 5.416\n",
      "Epoch   2 Batch  547/575   train_loss = 5.325\n",
      "Epoch   2 Batch  548/575   train_loss = 5.393\n",
      "Epoch   2 Batch  549/575   train_loss = 5.063\n",
      "Epoch   2 Batch  550/575   train_loss = 5.109\n",
      "Epoch   2 Batch  551/575   train_loss = 5.051\n",
      "Epoch   2 Batch  552/575   train_loss = 5.292\n",
      "Epoch   2 Batch  553/575   train_loss = 5.279\n",
      "Epoch   2 Batch  554/575   train_loss = 5.081\n",
      "Epoch   2 Batch  555/575   train_loss = 5.429\n",
      "Epoch   2 Batch  556/575   train_loss = 5.031\n",
      "Epoch   2 Batch  557/575   train_loss = 4.780\n",
      "Epoch   2 Batch  558/575   train_loss = 5.218\n",
      "Epoch   2 Batch  559/575   train_loss = 5.297\n",
      "Epoch   2 Batch  560/575   train_loss = 4.774\n",
      "Epoch   2 Batch  561/575   train_loss = 5.181\n",
      "Epoch   2 Batch  562/575   train_loss = 4.722\n",
      "Epoch   2 Batch  563/575   train_loss = 5.196\n",
      "Epoch   2 Batch  564/575   train_loss = 4.787\n",
      "Epoch   2 Batch  565/575   train_loss = 4.527\n",
      "Epoch   2 Batch  566/575   train_loss = 4.887\n",
      "Epoch   2 Batch  567/575   train_loss = 4.628\n",
      "Epoch   2 Batch  568/575   train_loss = 4.869\n",
      "Epoch   2 Batch  569/575   train_loss = 5.129\n",
      "Epoch   2 Batch  570/575   train_loss = 4.767\n",
      "Epoch   2 Batch  571/575   train_loss = 4.686\n",
      "Epoch   2 Batch  572/575   train_loss = 4.298\n",
      "Epoch   2 Batch  573/575   train_loss = 4.774\n",
      "Epoch   2 Batch  574/575   train_loss = 5.423\n",
      "Epoch   3 Batch    0/575   train_loss = 5.014\n",
      "Epoch   3 Batch    1/575   train_loss = 4.951\n",
      "Epoch   3 Batch    2/575   train_loss = 5.170\n",
      "Epoch   3 Batch    3/575   train_loss = 4.885\n",
      "Epoch   3 Batch    4/575   train_loss = 4.790\n",
      "Epoch   3 Batch    5/575   train_loss = 4.821\n",
      "Epoch   3 Batch    6/575   train_loss = 4.668\n",
      "Epoch   3 Batch    7/575   train_loss = 5.008\n",
      "Epoch   3 Batch    8/575   train_loss = 4.964\n",
      "Epoch   3 Batch    9/575   train_loss = 5.138\n",
      "Epoch   3 Batch   10/575   train_loss = 4.594\n",
      "Epoch   3 Batch   11/575   train_loss = 4.828\n",
      "Epoch   3 Batch   12/575   train_loss = 4.939\n",
      "Epoch   3 Batch   13/575   train_loss = 5.171\n",
      "Epoch   3 Batch   14/575   train_loss = 5.506\n",
      "Epoch   3 Batch   15/575   train_loss = 5.276\n",
      "Epoch   3 Batch   16/575   train_loss = 5.171\n",
      "Epoch   3 Batch   17/575   train_loss = 4.396\n",
      "Epoch   3 Batch   18/575   train_loss = 5.489\n",
      "Epoch   3 Batch   19/575   train_loss = 5.105\n",
      "Epoch   3 Batch   20/575   train_loss = 4.667\n",
      "Epoch   3 Batch   21/575   train_loss = 4.775\n",
      "Epoch   3 Batch   22/575   train_loss = 5.555\n",
      "Epoch   3 Batch   23/575   train_loss = 5.141\n",
      "Epoch   3 Batch   24/575   train_loss = 4.952\n",
      "Epoch   3 Batch   25/575   train_loss = 5.119\n",
      "Epoch   3 Batch   26/575   train_loss = 5.307\n",
      "Epoch   3 Batch   27/575   train_loss = 5.145\n",
      "Epoch   3 Batch   28/575   train_loss = 4.592\n",
      "Epoch   3 Batch   29/575   train_loss = 4.736\n",
      "Epoch   3 Batch   30/575   train_loss = 5.666\n",
      "Epoch   3 Batch   31/575   train_loss = 5.921\n",
      "Epoch   3 Batch   32/575   train_loss = 5.067\n",
      "Epoch   3 Batch   33/575   train_loss = 5.400\n",
      "Epoch   3 Batch   34/575   train_loss = 4.967\n",
      "Epoch   3 Batch   35/575   train_loss = 4.931\n",
      "Epoch   3 Batch   36/575   train_loss = 5.217\n",
      "Epoch   3 Batch   37/575   train_loss = 4.447\n",
      "Epoch   3 Batch   38/575   train_loss = 5.190\n",
      "Epoch   3 Batch   39/575   train_loss = 4.506\n",
      "Epoch   3 Batch   40/575   train_loss = 4.330\n",
      "Epoch   3 Batch   41/575   train_loss = 6.195\n",
      "Epoch   3 Batch   42/575   train_loss = 5.121\n",
      "Epoch   3 Batch   43/575   train_loss = 5.113\n",
      "Epoch   3 Batch   44/575   train_loss = 4.989\n",
      "Epoch   3 Batch   45/575   train_loss = 4.894\n",
      "Epoch   3 Batch   46/575   train_loss = 4.817\n",
      "Epoch   3 Batch   47/575   train_loss = 5.122\n",
      "Epoch   3 Batch   48/575   train_loss = 4.324\n",
      "Epoch   3 Batch   49/575   train_loss = 4.586\n",
      "Epoch   3 Batch   50/575   train_loss = 4.867\n",
      "Epoch   3 Batch   51/575   train_loss = 4.737\n",
      "Epoch   3 Batch   52/575   train_loss = 4.702\n",
      "Epoch   3 Batch   53/575   train_loss = 4.485\n",
      "Epoch   3 Batch   54/575   train_loss = 5.057\n",
      "Epoch   3 Batch   55/575   train_loss = 5.338\n",
      "Epoch   3 Batch   56/575   train_loss = 4.652\n",
      "Epoch   3 Batch   57/575   train_loss = 5.004\n",
      "Epoch   3 Batch   58/575   train_loss = 4.688\n",
      "Epoch   3 Batch   59/575   train_loss = 4.403\n",
      "Epoch   3 Batch   60/575   train_loss = 5.144\n",
      "Epoch   3 Batch   61/575   train_loss = 5.041\n",
      "Epoch   3 Batch   62/575   train_loss = 4.686\n",
      "Epoch   3 Batch   63/575   train_loss = 5.386\n",
      "Epoch   3 Batch   64/575   train_loss = 5.288\n",
      "Epoch   3 Batch   65/575   train_loss = 4.778\n",
      "Epoch   3 Batch   66/575   train_loss = 4.797\n",
      "Epoch   3 Batch   67/575   train_loss = 4.588\n",
      "Epoch   3 Batch   68/575   train_loss = 4.759\n",
      "Epoch   3 Batch   69/575   train_loss = 4.406\n",
      "Epoch   3 Batch   70/575   train_loss = 4.970\n",
      "Epoch   3 Batch   71/575   train_loss = 4.691\n",
      "Epoch   3 Batch   72/575   train_loss = 5.268\n",
      "Epoch   3 Batch   73/575   train_loss = 5.252\n",
      "Epoch   3 Batch   74/575   train_loss = 5.548\n",
      "Epoch   3 Batch   75/575   train_loss = 5.079\n",
      "Epoch   3 Batch   76/575   train_loss = 5.438\n",
      "Epoch   3 Batch   77/575   train_loss = 5.501\n",
      "Epoch   3 Batch   78/575   train_loss = 5.184\n",
      "Epoch   3 Batch   79/575   train_loss = 5.059\n",
      "Epoch   3 Batch   80/575   train_loss = 4.917\n",
      "Epoch   3 Batch   81/575   train_loss = 4.887\n",
      "Epoch   3 Batch   82/575   train_loss = 4.641\n",
      "Epoch   3 Batch   83/575   train_loss = 4.830\n",
      "Epoch   3 Batch   84/575   train_loss = 4.915\n",
      "Epoch   3 Batch   85/575   train_loss = 5.615\n",
      "Epoch   3 Batch   86/575   train_loss = 4.879\n",
      "Epoch   3 Batch   87/575   train_loss = 5.502\n",
      "Epoch   3 Batch   88/575   train_loss = 4.296\n",
      "Epoch   3 Batch   89/575   train_loss = 4.590\n",
      "Epoch   3 Batch   90/575   train_loss = 4.825\n",
      "Epoch   3 Batch   91/575   train_loss = 5.053\n",
      "Epoch   3 Batch   92/575   train_loss = 5.651\n",
      "Epoch   3 Batch   93/575   train_loss = 5.149\n",
      "Epoch   3 Batch   94/575   train_loss = 4.733\n",
      "Epoch   3 Batch   95/575   train_loss = 5.072\n",
      "Epoch   3 Batch   96/575   train_loss = 5.059\n",
      "Epoch   3 Batch   97/575   train_loss = 5.206\n",
      "Epoch   3 Batch   98/575   train_loss = 4.849\n",
      "Epoch   3 Batch   99/575   train_loss = 5.580\n",
      "Epoch   3 Batch  100/575   train_loss = 5.294\n",
      "Epoch   3 Batch  101/575   train_loss = 4.816\n",
      "Epoch   3 Batch  102/575   train_loss = 4.599\n",
      "Epoch   3 Batch  103/575   train_loss = 5.349\n",
      "Epoch   3 Batch  104/575   train_loss = 5.030\n",
      "Epoch   3 Batch  105/575   train_loss = 5.199\n",
      "Epoch   3 Batch  106/575   train_loss = 4.901\n",
      "Epoch   3 Batch  107/575   train_loss = 4.687\n",
      "Epoch   3 Batch  108/575   train_loss = 5.177\n",
      "Epoch   3 Batch  109/575   train_loss = 4.971\n",
      "Epoch   3 Batch  110/575   train_loss = 4.077\n",
      "Epoch   3 Batch  111/575   train_loss = 4.899\n",
      "Epoch   3 Batch  112/575   train_loss = 5.088\n",
      "Epoch   3 Batch  113/575   train_loss = 5.630\n",
      "Epoch   3 Batch  114/575   train_loss = 5.193\n",
      "Epoch   3 Batch  115/575   train_loss = 4.698\n",
      "Epoch   3 Batch  116/575   train_loss = 5.207\n",
      "Epoch   3 Batch  117/575   train_loss = 4.567\n",
      "Epoch   3 Batch  118/575   train_loss = 5.133\n",
      "Epoch   3 Batch  119/575   train_loss = 5.684\n",
      "Epoch   3 Batch  120/575   train_loss = 5.145\n",
      "Epoch   3 Batch  121/575   train_loss = 5.272\n",
      "Epoch   3 Batch  122/575   train_loss = 5.344\n",
      "Epoch   3 Batch  123/575   train_loss = 5.051\n",
      "Epoch   3 Batch  124/575   train_loss = 5.388\n",
      "Epoch   3 Batch  125/575   train_loss = 5.288\n",
      "Epoch   3 Batch  126/575   train_loss = 5.143\n",
      "Epoch   3 Batch  127/575   train_loss = 4.514\n",
      "Epoch   3 Batch  128/575   train_loss = 4.824\n",
      "Epoch   3 Batch  129/575   train_loss = 4.904\n",
      "Epoch   3 Batch  130/575   train_loss = 5.283\n",
      "Epoch   3 Batch  131/575   train_loss = 4.461\n",
      "Epoch   3 Batch  132/575   train_loss = 4.778\n",
      "Epoch   3 Batch  133/575   train_loss = 5.031\n",
      "Epoch   3 Batch  134/575   train_loss = 4.880\n",
      "Epoch   3 Batch  135/575   train_loss = 4.542\n",
      "Epoch   3 Batch  136/575   train_loss = 6.074\n",
      "Epoch   3 Batch  137/575   train_loss = 5.048\n",
      "Epoch   3 Batch  138/575   train_loss = 5.575\n",
      "Epoch   3 Batch  139/575   train_loss = 5.304\n",
      "Epoch   3 Batch  140/575   train_loss = 5.260\n",
      "Epoch   3 Batch  141/575   train_loss = 5.593\n",
      "Epoch   3 Batch  142/575   train_loss = 6.096\n",
      "Epoch   3 Batch  143/575   train_loss = 5.172\n",
      "Epoch   3 Batch  144/575   train_loss = 4.710\n",
      "Epoch   3 Batch  145/575   train_loss = 4.777\n",
      "Epoch   3 Batch  146/575   train_loss = 4.831\n",
      "Epoch   3 Batch  147/575   train_loss = 4.827\n",
      "Epoch   3 Batch  148/575   train_loss = 4.397\n",
      "Epoch   3 Batch  149/575   train_loss = 5.170\n",
      "Epoch   3 Batch  150/575   train_loss = 5.438\n",
      "Epoch   3 Batch  151/575   train_loss = 4.760\n",
      "Epoch   3 Batch  152/575   train_loss = 4.645\n",
      "Epoch   3 Batch  153/575   train_loss = 4.177\n",
      "Epoch   3 Batch  154/575   train_loss = 4.313\n",
      "Epoch   3 Batch  155/575   train_loss = 5.115\n",
      "Epoch   3 Batch  156/575   train_loss = 4.884\n",
      "Epoch   3 Batch  157/575   train_loss = 5.058\n",
      "Epoch   3 Batch  158/575   train_loss = 5.009\n",
      "Epoch   3 Batch  159/575   train_loss = 5.292\n",
      "Epoch   3 Batch  160/575   train_loss = 4.708\n",
      "Epoch   3 Batch  161/575   train_loss = 5.453\n",
      "Epoch   3 Batch  162/575   train_loss = 5.269\n",
      "Epoch   3 Batch  163/575   train_loss = 4.883\n",
      "Epoch   3 Batch  164/575   train_loss = 5.260\n",
      "Epoch   3 Batch  165/575   train_loss = 5.174\n",
      "Epoch   3 Batch  166/575   train_loss = 4.723\n",
      "Epoch   3 Batch  167/575   train_loss = 4.775\n",
      "Epoch   3 Batch  168/575   train_loss = 5.205\n",
      "Epoch   3 Batch  169/575   train_loss = 4.781\n",
      "Epoch   3 Batch  170/575   train_loss = 5.267\n",
      "Epoch   3 Batch  171/575   train_loss = 4.861\n",
      "Epoch   3 Batch  172/575   train_loss = 5.109\n",
      "Epoch   3 Batch  173/575   train_loss = 5.117\n",
      "Epoch   3 Batch  174/575   train_loss = 5.434\n",
      "Epoch   3 Batch  175/575   train_loss = 5.191\n",
      "Epoch   3 Batch  176/575   train_loss = 5.200\n",
      "Epoch   3 Batch  177/575   train_loss = 5.399\n",
      "Epoch   3 Batch  178/575   train_loss = 5.050\n",
      "Epoch   3 Batch  179/575   train_loss = 4.831\n",
      "Epoch   3 Batch  180/575   train_loss = 4.918\n",
      "Epoch   3 Batch  181/575   train_loss = 5.228\n",
      "Epoch   3 Batch  182/575   train_loss = 4.818\n",
      "Epoch   3 Batch  183/575   train_loss = 5.271\n",
      "Epoch   3 Batch  184/575   train_loss = 4.394\n",
      "Epoch   3 Batch  185/575   train_loss = 4.918\n",
      "Epoch   3 Batch  186/575   train_loss = 5.207\n",
      "Epoch   3 Batch  187/575   train_loss = 4.895\n",
      "Epoch   3 Batch  188/575   train_loss = 4.947\n",
      "Epoch   3 Batch  189/575   train_loss = 4.826\n",
      "Epoch   3 Batch  190/575   train_loss = 5.090\n",
      "Epoch   3 Batch  191/575   train_loss = 5.561\n",
      "Epoch   3 Batch  192/575   train_loss = 5.362\n",
      "Epoch   3 Batch  193/575   train_loss = 5.012\n",
      "Epoch   3 Batch  194/575   train_loss = 5.184\n",
      "Epoch   3 Batch  195/575   train_loss = 4.786\n",
      "Epoch   3 Batch  196/575   train_loss = 4.720\n",
      "Epoch   3 Batch  197/575   train_loss = 4.666\n",
      "Epoch   3 Batch  198/575   train_loss = 5.325\n",
      "Epoch   3 Batch  199/575   train_loss = 4.443\n",
      "Epoch   3 Batch  200/575   train_loss = 4.749\n",
      "Epoch   3 Batch  201/575   train_loss = 5.016\n",
      "Epoch   3 Batch  202/575   train_loss = 4.510\n",
      "Epoch   3 Batch  203/575   train_loss = 4.834\n",
      "Epoch   3 Batch  204/575   train_loss = 4.841\n",
      "Epoch   3 Batch  205/575   train_loss = 4.605\n",
      "Epoch   3 Batch  206/575   train_loss = 4.997\n",
      "Epoch   3 Batch  207/575   train_loss = 4.666\n",
      "Epoch   3 Batch  208/575   train_loss = 4.505\n",
      "Epoch   3 Batch  209/575   train_loss = 4.610\n",
      "Epoch   3 Batch  210/575   train_loss = 4.900\n",
      "Epoch   3 Batch  211/575   train_loss = 5.681\n",
      "Epoch   3 Batch  212/575   train_loss = 5.214\n",
      "Epoch   3 Batch  213/575   train_loss = 5.093\n",
      "Epoch   3 Batch  214/575   train_loss = 4.740\n",
      "Epoch   3 Batch  215/575   train_loss = 4.113\n",
      "Epoch   3 Batch  216/575   train_loss = 4.735\n",
      "Epoch   3 Batch  217/575   train_loss = 4.434\n",
      "Epoch   3 Batch  218/575   train_loss = 5.093\n",
      "Epoch   3 Batch  219/575   train_loss = 4.795\n",
      "Epoch   3 Batch  220/575   train_loss = 4.332\n",
      "Epoch   3 Batch  221/575   train_loss = 5.084\n",
      "Epoch   3 Batch  222/575   train_loss = 5.109\n",
      "Epoch   3 Batch  223/575   train_loss = 5.325\n",
      "Epoch   3 Batch  224/575   train_loss = 4.571\n",
      "Epoch   3 Batch  225/575   train_loss = 4.801\n",
      "Epoch   3 Batch  226/575   train_loss = 5.090\n",
      "Epoch   3 Batch  227/575   train_loss = 5.385\n",
      "Epoch   3 Batch  228/575   train_loss = 5.248\n",
      "Epoch   3 Batch  229/575   train_loss = 4.094\n",
      "Epoch   3 Batch  230/575   train_loss = 4.249\n",
      "Epoch   3 Batch  231/575   train_loss = 4.883\n",
      "Epoch   3 Batch  232/575   train_loss = 4.930\n",
      "Epoch   3 Batch  233/575   train_loss = 5.069\n",
      "Epoch   3 Batch  234/575   train_loss = 5.073\n",
      "Epoch   3 Batch  235/575   train_loss = 4.665\n",
      "Epoch   3 Batch  236/575   train_loss = 4.286\n",
      "Epoch   3 Batch  237/575   train_loss = 4.625\n",
      "Epoch   3 Batch  238/575   train_loss = 4.665\n",
      "Epoch   3 Batch  239/575   train_loss = 5.059\n",
      "Epoch   3 Batch  240/575   train_loss = 5.225\n",
      "Epoch   3 Batch  241/575   train_loss = 5.367\n",
      "Epoch   3 Batch  242/575   train_loss = 4.758\n",
      "Epoch   3 Batch  243/575   train_loss = 4.965\n",
      "Epoch   3 Batch  244/575   train_loss = 4.904\n",
      "Epoch   3 Batch  245/575   train_loss = 5.026\n",
      "Epoch   3 Batch  246/575   train_loss = 5.344\n",
      "Epoch   3 Batch  247/575   train_loss = 4.723\n",
      "Epoch   3 Batch  248/575   train_loss = 5.093\n",
      "Epoch   3 Batch  249/575   train_loss = 5.005\n",
      "Epoch   3 Batch  250/575   train_loss = 4.793\n",
      "Epoch   3 Batch  251/575   train_loss = 5.060\n",
      "Epoch   3 Batch  252/575   train_loss = 5.240\n",
      "Epoch   3 Batch  253/575   train_loss = 4.923\n",
      "Epoch   3 Batch  254/575   train_loss = 4.496\n",
      "Epoch   3 Batch  255/575   train_loss = 5.175\n",
      "Epoch   3 Batch  256/575   train_loss = 4.804\n",
      "Epoch   3 Batch  257/575   train_loss = 4.973\n",
      "Epoch   3 Batch  258/575   train_loss = 5.338\n",
      "Epoch   3 Batch  259/575   train_loss = 4.537\n",
      "Epoch   3 Batch  260/575   train_loss = 4.584\n",
      "Epoch   3 Batch  261/575   train_loss = 5.564\n",
      "Epoch   3 Batch  262/575   train_loss = 5.261\n",
      "Epoch   3 Batch  263/575   train_loss = 4.973\n",
      "Epoch   3 Batch  264/575   train_loss = 4.384\n",
      "Epoch   3 Batch  265/575   train_loss = 4.152\n",
      "Epoch   3 Batch  266/575   train_loss = 4.445\n",
      "Epoch   3 Batch  267/575   train_loss = 4.896\n",
      "Epoch   3 Batch  268/575   train_loss = 4.916\n",
      "Epoch   3 Batch  269/575   train_loss = 5.012\n",
      "Epoch   3 Batch  270/575   train_loss = 5.450\n",
      "Epoch   3 Batch  271/575   train_loss = 4.596\n",
      "Epoch   3 Batch  272/575   train_loss = 5.210\n",
      "Epoch   3 Batch  273/575   train_loss = 5.123\n",
      "Epoch   3 Batch  274/575   train_loss = 5.073\n",
      "Epoch   3 Batch  275/575   train_loss = 5.066\n",
      "Epoch   3 Batch  276/575   train_loss = 5.468\n",
      "Epoch   3 Batch  277/575   train_loss = 4.143\n",
      "Epoch   3 Batch  278/575   train_loss = 4.868\n",
      "Epoch   3 Batch  279/575   train_loss = 4.982\n",
      "Epoch   3 Batch  280/575   train_loss = 5.270\n",
      "Epoch   3 Batch  281/575   train_loss = 4.725\n",
      "Epoch   3 Batch  282/575   train_loss = 4.980\n",
      "Epoch   3 Batch  283/575   train_loss = 5.250\n",
      "Epoch   3 Batch  284/575   train_loss = 5.431\n",
      "Epoch   3 Batch  285/575   train_loss = 4.753\n",
      "Epoch   3 Batch  286/575   train_loss = 4.988\n",
      "Epoch   3 Batch  287/575   train_loss = 5.223\n",
      "Epoch   3 Batch  288/575   train_loss = 4.904\n",
      "Epoch   3 Batch  289/575   train_loss = 4.968\n",
      "Epoch   3 Batch  290/575   train_loss = 5.056\n",
      "Epoch   3 Batch  291/575   train_loss = 5.658\n",
      "Epoch   3 Batch  292/575   train_loss = 5.406\n",
      "Epoch   3 Batch  293/575   train_loss = 5.646\n",
      "Epoch   3 Batch  294/575   train_loss = 5.326\n",
      "Epoch   3 Batch  295/575   train_loss = 4.881\n",
      "Epoch   3 Batch  296/575   train_loss = 4.874\n",
      "Epoch   3 Batch  297/575   train_loss = 5.085\n",
      "Epoch   3 Batch  298/575   train_loss = 5.168\n",
      "Epoch   3 Batch  299/575   train_loss = 5.062\n",
      "Epoch   3 Batch  300/575   train_loss = 4.850\n",
      "Epoch   3 Batch  301/575   train_loss = 4.747\n",
      "Epoch   3 Batch  302/575   train_loss = 5.263\n",
      "Epoch   3 Batch  303/575   train_loss = 4.629\n",
      "Epoch   3 Batch  304/575   train_loss = 4.790\n",
      "Epoch   3 Batch  305/575   train_loss = 4.813\n",
      "Epoch   3 Batch  306/575   train_loss = 5.173\n",
      "Epoch   3 Batch  307/575   train_loss = 5.214\n",
      "Epoch   3 Batch  308/575   train_loss = 5.111\n",
      "Epoch   3 Batch  309/575   train_loss = 5.409\n",
      "Epoch   3 Batch  310/575   train_loss = 5.322\n",
      "Epoch   3 Batch  311/575   train_loss = 4.623\n",
      "Epoch   3 Batch  312/575   train_loss = 5.189\n",
      "Epoch   3 Batch  313/575   train_loss = 5.504\n",
      "Epoch   3 Batch  314/575   train_loss = 5.339\n",
      "Epoch   3 Batch  315/575   train_loss = 5.600\n",
      "Epoch   3 Batch  316/575   train_loss = 4.938\n",
      "Epoch   3 Batch  317/575   train_loss = 4.533\n",
      "Epoch   3 Batch  318/575   train_loss = 4.259\n",
      "Epoch   3 Batch  319/575   train_loss = 4.994\n",
      "Epoch   3 Batch  320/575   train_loss = 4.685\n",
      "Epoch   3 Batch  321/575   train_loss = 5.461\n",
      "Epoch   3 Batch  322/575   train_loss = 4.923\n",
      "Epoch   3 Batch  323/575   train_loss = 5.338\n",
      "Epoch   3 Batch  324/575   train_loss = 4.843\n",
      "Epoch   3 Batch  325/575   train_loss = 5.006\n",
      "Epoch   3 Batch  326/575   train_loss = 4.811\n",
      "Epoch   3 Batch  327/575   train_loss = 5.232\n",
      "Epoch   3 Batch  328/575   train_loss = 5.015\n",
      "Epoch   3 Batch  329/575   train_loss = 5.404\n",
      "Epoch   3 Batch  330/575   train_loss = 5.054\n",
      "Epoch   3 Batch  331/575   train_loss = 5.598\n",
      "Epoch   3 Batch  332/575   train_loss = 5.308\n",
      "Epoch   3 Batch  333/575   train_loss = 5.218\n",
      "Epoch   3 Batch  334/575   train_loss = 4.920\n",
      "Epoch   3 Batch  335/575   train_loss = 4.778\n",
      "Epoch   3 Batch  336/575   train_loss = 5.104\n",
      "Epoch   3 Batch  337/575   train_loss = 4.998\n",
      "Epoch   3 Batch  338/575   train_loss = 4.695\n",
      "Epoch   3 Batch  339/575   train_loss = 5.054\n",
      "Epoch   3 Batch  340/575   train_loss = 4.858\n",
      "Epoch   3 Batch  341/575   train_loss = 4.830\n",
      "Epoch   3 Batch  342/575   train_loss = 4.702\n",
      "Epoch   3 Batch  343/575   train_loss = 5.100\n",
      "Epoch   3 Batch  344/575   train_loss = 5.147\n",
      "Epoch   3 Batch  345/575   train_loss = 4.804\n",
      "Epoch   3 Batch  346/575   train_loss = 5.585\n",
      "Epoch   3 Batch  347/575   train_loss = 4.711\n",
      "Epoch   3 Batch  348/575   train_loss = 4.729\n",
      "Epoch   3 Batch  349/575   train_loss = 4.484\n",
      "Epoch   3 Batch  350/575   train_loss = 5.092\n",
      "Epoch   3 Batch  351/575   train_loss = 5.491\n",
      "Epoch   3 Batch  352/575   train_loss = 5.230\n",
      "Epoch   3 Batch  353/575   train_loss = 5.195\n",
      "Epoch   3 Batch  354/575   train_loss = 4.869\n",
      "Epoch   3 Batch  355/575   train_loss = 5.428\n",
      "Epoch   3 Batch  356/575   train_loss = 5.073\n",
      "Epoch   3 Batch  357/575   train_loss = 4.975\n",
      "Epoch   3 Batch  358/575   train_loss = 5.289\n",
      "Epoch   3 Batch  359/575   train_loss = 5.554\n",
      "Epoch   3 Batch  360/575   train_loss = 4.500\n",
      "Epoch   3 Batch  361/575   train_loss = 5.237\n",
      "Epoch   3 Batch  362/575   train_loss = 4.993\n",
      "Epoch   3 Batch  363/575   train_loss = 5.005\n",
      "Epoch   3 Batch  364/575   train_loss = 4.618\n",
      "Epoch   3 Batch  365/575   train_loss = 5.111\n",
      "Epoch   3 Batch  366/575   train_loss = 4.556\n",
      "Epoch   3 Batch  367/575   train_loss = 4.980\n",
      "Epoch   3 Batch  368/575   train_loss = 5.595\n",
      "Epoch   3 Batch  369/575   train_loss = 5.295\n",
      "Epoch   3 Batch  370/575   train_loss = 5.233\n",
      "Epoch   3 Batch  371/575   train_loss = 5.342\n",
      "Epoch   3 Batch  372/575   train_loss = 4.879\n",
      "Epoch   3 Batch  373/575   train_loss = 5.247\n",
      "Epoch   3 Batch  374/575   train_loss = 4.419\n",
      "Epoch   3 Batch  375/575   train_loss = 4.765\n",
      "Epoch   3 Batch  376/575   train_loss = 5.056\n",
      "Epoch   3 Batch  377/575   train_loss = 5.440\n",
      "Epoch   3 Batch  378/575   train_loss = 5.484\n",
      "Epoch   3 Batch  379/575   train_loss = 4.840\n",
      "Epoch   3 Batch  380/575   train_loss = 4.544\n",
      "Epoch   3 Batch  381/575   train_loss = 4.832\n",
      "Epoch   3 Batch  382/575   train_loss = 5.143\n",
      "Epoch   3 Batch  383/575   train_loss = 4.892\n",
      "Epoch   3 Batch  384/575   train_loss = 4.991\n",
      "Epoch   3 Batch  385/575   train_loss = 4.726\n",
      "Epoch   3 Batch  386/575   train_loss = 4.742\n",
      "Epoch   3 Batch  387/575   train_loss = 5.458\n",
      "Epoch   3 Batch  388/575   train_loss = 4.820\n",
      "Epoch   3 Batch  389/575   train_loss = 4.846\n",
      "Epoch   3 Batch  390/575   train_loss = 5.061\n",
      "Epoch   3 Batch  391/575   train_loss = 5.210\n",
      "Epoch   3 Batch  392/575   train_loss = 4.950\n",
      "Epoch   3 Batch  393/575   train_loss = 4.975\n",
      "Epoch   3 Batch  394/575   train_loss = 5.292\n",
      "Epoch   3 Batch  395/575   train_loss = 4.379\n",
      "Epoch   3 Batch  396/575   train_loss = 4.769\n",
      "Epoch   3 Batch  397/575   train_loss = 5.209\n",
      "Epoch   3 Batch  398/575   train_loss = 5.422\n",
      "Epoch   3 Batch  399/575   train_loss = 5.421\n",
      "Epoch   3 Batch  400/575   train_loss = 5.280\n",
      "Epoch   3 Batch  401/575   train_loss = 4.597\n",
      "Epoch   3 Batch  402/575   train_loss = 4.775\n",
      "Epoch   3 Batch  403/575   train_loss = 5.198\n",
      "Epoch   3 Batch  404/575   train_loss = 5.281\n",
      "Epoch   3 Batch  405/575   train_loss = 5.332\n",
      "Epoch   3 Batch  406/575   train_loss = 4.695\n",
      "Epoch   3 Batch  407/575   train_loss = 5.041\n",
      "Epoch   3 Batch  408/575   train_loss = 5.058\n",
      "Epoch   3 Batch  409/575   train_loss = 4.915\n",
      "Epoch   3 Batch  410/575   train_loss = 4.762\n",
      "Epoch   3 Batch  411/575   train_loss = 4.984\n",
      "Epoch   3 Batch  412/575   train_loss = 5.633\n",
      "Epoch   3 Batch  413/575   train_loss = 5.287\n",
      "Epoch   3 Batch  414/575   train_loss = 5.039\n",
      "Epoch   3 Batch  415/575   train_loss = 5.529\n",
      "Epoch   3 Batch  416/575   train_loss = 4.906\n",
      "Epoch   3 Batch  417/575   train_loss = 4.599\n",
      "Epoch   3 Batch  418/575   train_loss = 4.984\n",
      "Epoch   3 Batch  419/575   train_loss = 4.945\n",
      "Epoch   3 Batch  420/575   train_loss = 4.849\n",
      "Epoch   3 Batch  421/575   train_loss = 5.514\n",
      "Epoch   3 Batch  422/575   train_loss = 5.183\n",
      "Epoch   3 Batch  423/575   train_loss = 5.618\n",
      "Epoch   3 Batch  424/575   train_loss = 5.667\n",
      "Epoch   3 Batch  425/575   train_loss = 5.340\n",
      "Epoch   3 Batch  426/575   train_loss = 5.146\n",
      "Epoch   3 Batch  427/575   train_loss = 4.976\n",
      "Epoch   3 Batch  428/575   train_loss = 5.012\n",
      "Epoch   3 Batch  429/575   train_loss = 4.682\n",
      "Epoch   3 Batch  430/575   train_loss = 5.065\n",
      "Epoch   3 Batch  431/575   train_loss = 4.958\n",
      "Epoch   3 Batch  432/575   train_loss = 5.403\n",
      "Epoch   3 Batch  433/575   train_loss = 5.375\n",
      "Epoch   3 Batch  434/575   train_loss = 5.197\n",
      "Epoch   3 Batch  435/575   train_loss = 5.077\n",
      "Epoch   3 Batch  436/575   train_loss = 4.756\n",
      "Epoch   3 Batch  437/575   train_loss = 5.054\n",
      "Epoch   3 Batch  438/575   train_loss = 5.239\n",
      "Epoch   3 Batch  439/575   train_loss = 5.079\n",
      "Epoch   3 Batch  440/575   train_loss = 5.296\n",
      "Epoch   3 Batch  441/575   train_loss = 5.064\n",
      "Epoch   3 Batch  442/575   train_loss = 5.178\n",
      "Epoch   3 Batch  443/575   train_loss = 5.105\n",
      "Epoch   3 Batch  444/575   train_loss = 5.238\n",
      "Epoch   3 Batch  445/575   train_loss = 4.841\n",
      "Epoch   3 Batch  446/575   train_loss = 4.738\n",
      "Epoch   3 Batch  447/575   train_loss = 5.310\n",
      "Epoch   3 Batch  448/575   train_loss = 5.043\n",
      "Epoch   3 Batch  449/575   train_loss = 5.123\n",
      "Epoch   3 Batch  450/575   train_loss = 5.754\n",
      "Epoch   3 Batch  451/575   train_loss = 5.587\n",
      "Epoch   3 Batch  452/575   train_loss = 5.673\n",
      "Epoch   3 Batch  453/575   train_loss = 5.161\n",
      "Epoch   3 Batch  454/575   train_loss = 5.520\n",
      "Epoch   3 Batch  455/575   train_loss = 4.796\n",
      "Epoch   3 Batch  456/575   train_loss = 5.531\n",
      "Epoch   3 Batch  457/575   train_loss = 4.703\n",
      "Epoch   3 Batch  458/575   train_loss = 5.439\n",
      "Epoch   3 Batch  459/575   train_loss = 5.272\n",
      "Epoch   3 Batch  460/575   train_loss = 5.018\n",
      "Epoch   3 Batch  461/575   train_loss = 5.217\n",
      "Epoch   3 Batch  462/575   train_loss = 4.990\n",
      "Epoch   3 Batch  463/575   train_loss = 4.975\n",
      "Epoch   3 Batch  464/575   train_loss = 4.805\n",
      "Epoch   3 Batch  465/575   train_loss = 4.907\n",
      "Epoch   3 Batch  466/575   train_loss = 4.918\n",
      "Epoch   3 Batch  467/575   train_loss = 4.437\n",
      "Epoch   3 Batch  468/575   train_loss = 4.848\n",
      "Epoch   3 Batch  469/575   train_loss = 5.406\n",
      "Epoch   3 Batch  470/575   train_loss = 5.645\n",
      "Epoch   3 Batch  471/575   train_loss = 4.856\n",
      "Epoch   3 Batch  472/575   train_loss = 5.292\n",
      "Epoch   3 Batch  473/575   train_loss = 5.117\n",
      "Epoch   3 Batch  474/575   train_loss = 5.090\n",
      "Epoch   3 Batch  475/575   train_loss = 4.837\n",
      "Epoch   3 Batch  476/575   train_loss = 5.017\n",
      "Epoch   3 Batch  477/575   train_loss = 5.565\n",
      "Epoch   3 Batch  478/575   train_loss = 5.622\n",
      "Epoch   3 Batch  479/575   train_loss = 4.996\n",
      "Epoch   3 Batch  480/575   train_loss = 5.267\n",
      "Epoch   3 Batch  481/575   train_loss = 5.308\n",
      "Epoch   3 Batch  482/575   train_loss = 4.586\n",
      "Epoch   3 Batch  483/575   train_loss = 5.979\n",
      "Epoch   3 Batch  484/575   train_loss = 5.064\n",
      "Epoch   3 Batch  485/575   train_loss = 5.554\n",
      "Epoch   3 Batch  486/575   train_loss = 5.191\n",
      "Epoch   3 Batch  487/575   train_loss = 5.366\n",
      "Epoch   3 Batch  488/575   train_loss = 5.086\n",
      "Epoch   3 Batch  489/575   train_loss = 5.337\n",
      "Epoch   3 Batch  490/575   train_loss = 5.366\n",
      "Epoch   3 Batch  491/575   train_loss = 4.560\n",
      "Epoch   3 Batch  492/575   train_loss = 4.396\n",
      "Epoch   3 Batch  493/575   train_loss = 4.581\n",
      "Epoch   3 Batch  494/575   train_loss = 4.811\n",
      "Epoch   3 Batch  495/575   train_loss = 5.025\n",
      "Epoch   3 Batch  496/575   train_loss = 5.153\n",
      "Epoch   3 Batch  497/575   train_loss = 5.391\n",
      "Epoch   3 Batch  498/575   train_loss = 4.824\n",
      "Epoch   3 Batch  499/575   train_loss = 4.846\n",
      "Epoch   3 Batch  500/575   train_loss = 5.091\n",
      "Epoch   3 Batch  501/575   train_loss = 5.021\n",
      "Epoch   3 Batch  502/575   train_loss = 4.416\n",
      "Epoch   3 Batch  503/575   train_loss = 5.225\n",
      "Epoch   3 Batch  504/575   train_loss = 4.604\n",
      "Epoch   3 Batch  505/575   train_loss = 4.954\n",
      "Epoch   3 Batch  506/575   train_loss = 5.329\n",
      "Epoch   3 Batch  507/575   train_loss = 5.373\n",
      "Epoch   3 Batch  508/575   train_loss = 5.282\n",
      "Epoch   3 Batch  509/575   train_loss = 5.112\n",
      "Epoch   3 Batch  510/575   train_loss = 4.486\n",
      "Epoch   3 Batch  511/575   train_loss = 4.480\n",
      "Epoch   3 Batch  512/575   train_loss = 5.473\n",
      "Epoch   3 Batch  513/575   train_loss = 5.186\n",
      "Epoch   3 Batch  514/575   train_loss = 5.008\n",
      "Epoch   3 Batch  515/575   train_loss = 5.113\n",
      "Epoch   3 Batch  516/575   train_loss = 5.394\n",
      "Epoch   3 Batch  517/575   train_loss = 4.691\n",
      "Epoch   3 Batch  518/575   train_loss = 4.843\n",
      "Epoch   3 Batch  519/575   train_loss = 4.875\n",
      "Epoch   3 Batch  520/575   train_loss = 4.182\n",
      "Epoch   3 Batch  521/575   train_loss = 4.676\n",
      "Epoch   3 Batch  522/575   train_loss = 4.748\n",
      "Epoch   3 Batch  523/575   train_loss = 5.559\n",
      "Epoch   3 Batch  524/575   train_loss = 6.546\n",
      "Epoch   3 Batch  525/575   train_loss = 5.069\n",
      "Epoch   3 Batch  526/575   train_loss = 4.802\n",
      "Epoch   3 Batch  527/575   train_loss = 5.217\n",
      "Epoch   3 Batch  528/575   train_loss = 4.815\n",
      "Epoch   3 Batch  529/575   train_loss = 5.241\n",
      "Epoch   3 Batch  530/575   train_loss = 5.146\n",
      "Epoch   3 Batch  531/575   train_loss = 5.196\n",
      "Epoch   3 Batch  532/575   train_loss = 4.831\n",
      "Epoch   3 Batch  533/575   train_loss = 4.846\n",
      "Epoch   3 Batch  534/575   train_loss = 4.850\n",
      "Epoch   3 Batch  535/575   train_loss = 5.320\n",
      "Epoch   3 Batch  536/575   train_loss = 5.067\n",
      "Epoch   3 Batch  537/575   train_loss = 4.624\n",
      "Epoch   3 Batch  538/575   train_loss = 5.007\n",
      "Epoch   3 Batch  539/575   train_loss = 4.829\n",
      "Epoch   3 Batch  540/575   train_loss = 5.134\n",
      "Epoch   3 Batch  541/575   train_loss = 5.601\n",
      "Epoch   3 Batch  542/575   train_loss = 5.098\n",
      "Epoch   3 Batch  543/575   train_loss = 5.349\n",
      "Epoch   3 Batch  544/575   train_loss = 5.062\n",
      "Epoch   3 Batch  545/575   train_loss = 5.175\n",
      "Epoch   3 Batch  546/575   train_loss = 5.147\n",
      "Epoch   3 Batch  547/575   train_loss = 5.074\n",
      "Epoch   3 Batch  548/575   train_loss = 5.175\n",
      "Epoch   3 Batch  549/575   train_loss = 4.876\n",
      "Epoch   3 Batch  550/575   train_loss = 4.899\n",
      "Epoch   3 Batch  551/575   train_loss = 4.856\n",
      "Epoch   3 Batch  552/575   train_loss = 5.067\n",
      "Epoch   3 Batch  553/575   train_loss = 5.160\n",
      "Epoch   3 Batch  554/575   train_loss = 4.829\n",
      "Epoch   3 Batch  555/575   train_loss = 5.155\n",
      "Epoch   3 Batch  556/575   train_loss = 4.789\n",
      "Epoch   3 Batch  557/575   train_loss = 4.603\n",
      "Epoch   3 Batch  558/575   train_loss = 5.011\n",
      "Epoch   3 Batch  559/575   train_loss = 5.122\n",
      "Epoch   3 Batch  560/575   train_loss = 4.541\n",
      "Epoch   3 Batch  561/575   train_loss = 4.999\n",
      "Epoch   3 Batch  562/575   train_loss = 4.517\n",
      "Epoch   3 Batch  563/575   train_loss = 4.990\n",
      "Epoch   3 Batch  564/575   train_loss = 4.611\n",
      "Epoch   3 Batch  565/575   train_loss = 4.323\n",
      "Epoch   3 Batch  566/575   train_loss = 4.717\n",
      "Epoch   3 Batch  567/575   train_loss = 4.488\n",
      "Epoch   3 Batch  568/575   train_loss = 4.695\n",
      "Epoch   3 Batch  569/575   train_loss = 5.005\n",
      "Epoch   3 Batch  570/575   train_loss = 4.607\n",
      "Epoch   3 Batch  571/575   train_loss = 4.485\n",
      "Epoch   3 Batch  572/575   train_loss = 4.131\n",
      "Epoch   3 Batch  573/575   train_loss = 4.587\n",
      "Epoch   3 Batch  574/575   train_loss = 5.200\n",
      "Epoch   4 Batch    0/575   train_loss = 4.793\n",
      "Epoch   4 Batch    1/575   train_loss = 4.778\n",
      "Epoch   4 Batch    2/575   train_loss = 5.032\n",
      "Epoch   4 Batch    3/575   train_loss = 4.715\n",
      "Epoch   4 Batch    4/575   train_loss = 4.632\n",
      "Epoch   4 Batch    5/575   train_loss = 4.636\n",
      "Epoch   4 Batch    6/575   train_loss = 4.500\n",
      "Epoch   4 Batch    7/575   train_loss = 4.812\n",
      "Epoch   4 Batch    8/575   train_loss = 4.813\n",
      "Epoch   4 Batch    9/575   train_loss = 4.991\n",
      "Epoch   4 Batch   10/575   train_loss = 4.474\n",
      "Epoch   4 Batch   11/575   train_loss = 4.652\n",
      "Epoch   4 Batch   12/575   train_loss = 4.771\n",
      "Epoch   4 Batch   13/575   train_loss = 5.009\n",
      "Epoch   4 Batch   14/575   train_loss = 5.399\n",
      "Epoch   4 Batch   15/575   train_loss = 5.122\n",
      "Epoch   4 Batch   16/575   train_loss = 4.970\n",
      "Epoch   4 Batch   17/575   train_loss = 4.271\n",
      "Epoch   4 Batch   18/575   train_loss = 5.325\n",
      "Epoch   4 Batch   19/575   train_loss = 4.925\n",
      "Epoch   4 Batch   20/575   train_loss = 4.433\n",
      "Epoch   4 Batch   21/575   train_loss = 4.660\n",
      "Epoch   4 Batch   22/575   train_loss = 5.404\n",
      "Epoch   4 Batch   23/575   train_loss = 4.922\n",
      "Epoch   4 Batch   24/575   train_loss = 4.780\n",
      "Epoch   4 Batch   25/575   train_loss = 4.956\n",
      "Epoch   4 Batch   26/575   train_loss = 5.139\n",
      "Epoch   4 Batch   27/575   train_loss = 4.985\n",
      "Epoch   4 Batch   28/575   train_loss = 4.438\n",
      "Epoch   4 Batch   29/575   train_loss = 4.598\n",
      "Epoch   4 Batch   30/575   train_loss = 5.469\n",
      "Epoch   4 Batch   31/575   train_loss = 5.719\n",
      "Epoch   4 Batch   32/575   train_loss = 4.913\n",
      "Epoch   4 Batch   33/575   train_loss = 5.248\n",
      "Epoch   4 Batch   34/575   train_loss = 4.819\n",
      "Epoch   4 Batch   35/575   train_loss = 4.724\n",
      "Epoch   4 Batch   36/575   train_loss = 5.091\n",
      "Epoch   4 Batch   37/575   train_loss = 4.271\n",
      "Epoch   4 Batch   38/575   train_loss = 5.015\n",
      "Epoch   4 Batch   39/575   train_loss = 4.348\n",
      "Epoch   4 Batch   40/575   train_loss = 4.151\n",
      "Epoch   4 Batch   41/575   train_loss = 6.115\n",
      "Epoch   4 Batch   42/575   train_loss = 4.966\n",
      "Epoch   4 Batch   43/575   train_loss = 4.934\n",
      "Epoch   4 Batch   44/575   train_loss = 4.854\n",
      "Epoch   4 Batch   45/575   train_loss = 4.693\n",
      "Epoch   4 Batch   46/575   train_loss = 4.671\n",
      "Epoch   4 Batch   47/575   train_loss = 5.037\n",
      "Epoch   4 Batch   48/575   train_loss = 4.200\n",
      "Epoch   4 Batch   49/575   train_loss = 4.461\n",
      "Epoch   4 Batch   50/575   train_loss = 4.713\n",
      "Epoch   4 Batch   51/575   train_loss = 4.580\n",
      "Epoch   4 Batch   52/575   train_loss = 4.560\n",
      "Epoch   4 Batch   53/575   train_loss = 4.329\n",
      "Epoch   4 Batch   54/575   train_loss = 4.877\n",
      "Epoch   4 Batch   55/575   train_loss = 5.164\n",
      "Epoch   4 Batch   56/575   train_loss = 4.496\n",
      "Epoch   4 Batch   57/575   train_loss = 4.855\n",
      "Epoch   4 Batch   58/575   train_loss = 4.531\n",
      "Epoch   4 Batch   59/575   train_loss = 4.273\n",
      "Epoch   4 Batch   60/575   train_loss = 4.985\n",
      "Epoch   4 Batch   61/575   train_loss = 4.893\n",
      "Epoch   4 Batch   62/575   train_loss = 4.478\n",
      "Epoch   4 Batch   63/575   train_loss = 5.265\n",
      "Epoch   4 Batch   64/575   train_loss = 5.120\n",
      "Epoch   4 Batch   65/575   train_loss = 4.611\n",
      "Epoch   4 Batch   66/575   train_loss = 4.610\n",
      "Epoch   4 Batch   67/575   train_loss = 4.465\n",
      "Epoch   4 Batch   68/575   train_loss = 4.586\n",
      "Epoch   4 Batch   69/575   train_loss = 4.256\n",
      "Epoch   4 Batch   70/575   train_loss = 4.815\n",
      "Epoch   4 Batch   71/575   train_loss = 4.537\n",
      "Epoch   4 Batch   72/575   train_loss = 5.120\n",
      "Epoch   4 Batch   73/575   train_loss = 5.124\n",
      "Epoch   4 Batch   74/575   train_loss = 5.370\n",
      "Epoch   4 Batch   75/575   train_loss = 4.923\n",
      "Epoch   4 Batch   76/575   train_loss = 5.271\n",
      "Epoch   4 Batch   77/575   train_loss = 5.331\n",
      "Epoch   4 Batch   78/575   train_loss = 5.028\n",
      "Epoch   4 Batch   79/575   train_loss = 4.881\n",
      "Epoch   4 Batch   80/575   train_loss = 4.755\n",
      "Epoch   4 Batch   81/575   train_loss = 4.708\n",
      "Epoch   4 Batch   82/575   train_loss = 4.473\n",
      "Epoch   4 Batch   83/575   train_loss = 4.697\n",
      "Epoch   4 Batch   84/575   train_loss = 4.756\n",
      "Epoch   4 Batch   85/575   train_loss = 5.484\n",
      "Epoch   4 Batch   86/575   train_loss = 4.703\n",
      "Epoch   4 Batch   87/575   train_loss = 5.342\n",
      "Epoch   4 Batch   88/575   train_loss = 4.154\n",
      "Epoch   4 Batch   89/575   train_loss = 4.398\n",
      "Epoch   4 Batch   90/575   train_loss = 4.649\n",
      "Epoch   4 Batch   91/575   train_loss = 4.890\n",
      "Epoch   4 Batch   92/575   train_loss = 5.443\n",
      "Epoch   4 Batch   93/575   train_loss = 4.995\n",
      "Epoch   4 Batch   94/575   train_loss = 4.540\n",
      "Epoch   4 Batch   95/575   train_loss = 4.933\n",
      "Epoch   4 Batch   96/575   train_loss = 4.908\n",
      "Epoch   4 Batch   97/575   train_loss = 5.000\n",
      "Epoch   4 Batch   98/575   train_loss = 4.687\n",
      "Epoch   4 Batch   99/575   train_loss = 5.398\n",
      "Epoch   4 Batch  100/575   train_loss = 5.094\n",
      "Epoch   4 Batch  101/575   train_loss = 4.673\n",
      "Epoch   4 Batch  102/575   train_loss = 4.466\n",
      "Epoch   4 Batch  103/575   train_loss = 5.127\n",
      "Epoch   4 Batch  104/575   train_loss = 4.862\n",
      "Epoch   4 Batch  105/575   train_loss = 5.036\n",
      "Epoch   4 Batch  106/575   train_loss = 4.721\n",
      "Epoch   4 Batch  107/575   train_loss = 4.549\n",
      "Epoch   4 Batch  108/575   train_loss = 5.011\n",
      "Epoch   4 Batch  109/575   train_loss = 4.752\n",
      "Epoch   4 Batch  110/575   train_loss = 3.899\n",
      "Epoch   4 Batch  111/575   train_loss = 4.737\n",
      "Epoch   4 Batch  112/575   train_loss = 4.929\n",
      "Epoch   4 Batch  113/575   train_loss = 5.488\n",
      "Epoch   4 Batch  114/575   train_loss = 5.054\n",
      "Epoch   4 Batch  115/575   train_loss = 4.567\n",
      "Epoch   4 Batch  116/575   train_loss = 5.073\n",
      "Epoch   4 Batch  117/575   train_loss = 4.409\n",
      "Epoch   4 Batch  118/575   train_loss = 4.963\n",
      "Epoch   4 Batch  119/575   train_loss = 5.476\n",
      "Epoch   4 Batch  120/575   train_loss = 4.981\n",
      "Epoch   4 Batch  121/575   train_loss = 5.090\n",
      "Epoch   4 Batch  122/575   train_loss = 5.112\n",
      "Epoch   4 Batch  123/575   train_loss = 4.867\n",
      "Epoch   4 Batch  124/575   train_loss = 5.240\n",
      "Epoch   4 Batch  125/575   train_loss = 5.074\n",
      "Epoch   4 Batch  126/575   train_loss = 4.941\n",
      "Epoch   4 Batch  127/575   train_loss = 4.350\n",
      "Epoch   4 Batch  128/575   train_loss = 4.712\n",
      "Epoch   4 Batch  129/575   train_loss = 4.746\n",
      "Epoch   4 Batch  130/575   train_loss = 5.120\n",
      "Epoch   4 Batch  131/575   train_loss = 4.289\n",
      "Epoch   4 Batch  132/575   train_loss = 4.622\n",
      "Epoch   4 Batch  133/575   train_loss = 4.868\n",
      "Epoch   4 Batch  134/575   train_loss = 4.726\n",
      "Epoch   4 Batch  135/575   train_loss = 4.393\n",
      "Epoch   4 Batch  136/575   train_loss = 5.847\n",
      "Epoch   4 Batch  137/575   train_loss = 4.909\n",
      "Epoch   4 Batch  138/575   train_loss = 5.383\n",
      "Epoch   4 Batch  139/575   train_loss = 5.102\n",
      "Epoch   4 Batch  140/575   train_loss = 5.035\n",
      "Epoch   4 Batch  141/575   train_loss = 5.377\n",
      "Epoch   4 Batch  142/575   train_loss = 5.752\n",
      "Epoch   4 Batch  143/575   train_loss = 5.024\n",
      "Epoch   4 Batch  144/575   train_loss = 4.577\n",
      "Epoch   4 Batch  145/575   train_loss = 4.611\n",
      "Epoch   4 Batch  146/575   train_loss = 4.633\n",
      "Epoch   4 Batch  147/575   train_loss = 4.681\n",
      "Epoch   4 Batch  148/575   train_loss = 4.233\n",
      "Epoch   4 Batch  149/575   train_loss = 4.952\n",
      "Epoch   4 Batch  150/575   train_loss = 5.259\n",
      "Epoch   4 Batch  151/575   train_loss = 4.552\n",
      "Epoch   4 Batch  152/575   train_loss = 4.472\n",
      "Epoch   4 Batch  153/575   train_loss = 3.996\n",
      "Epoch   4 Batch  154/575   train_loss = 4.160\n",
      "Epoch   4 Batch  155/575   train_loss = 4.935\n",
      "Epoch   4 Batch  156/575   train_loss = 4.745\n",
      "Epoch   4 Batch  157/575   train_loss = 4.921\n",
      "Epoch   4 Batch  158/575   train_loss = 4.844\n",
      "Epoch   4 Batch  159/575   train_loss = 5.073\n",
      "Epoch   4 Batch  160/575   train_loss = 4.541\n",
      "Epoch   4 Batch  161/575   train_loss = 5.263\n",
      "Epoch   4 Batch  162/575   train_loss = 5.071\n",
      "Epoch   4 Batch  163/575   train_loss = 4.742\n",
      "Epoch   4 Batch  164/575   train_loss = 5.122\n",
      "Epoch   4 Batch  165/575   train_loss = 5.010\n",
      "Epoch   4 Batch  166/575   train_loss = 4.562\n",
      "Epoch   4 Batch  167/575   train_loss = 4.600\n",
      "Epoch   4 Batch  168/575   train_loss = 5.048\n",
      "Epoch   4 Batch  169/575   train_loss = 4.617\n",
      "Epoch   4 Batch  170/575   train_loss = 5.120\n",
      "Epoch   4 Batch  171/575   train_loss = 4.697\n",
      "Epoch   4 Batch  172/575   train_loss = 4.945\n",
      "Epoch   4 Batch  173/575   train_loss = 4.911\n",
      "Epoch   4 Batch  174/575   train_loss = 5.230\n",
      "Epoch   4 Batch  175/575   train_loss = 4.967\n",
      "Epoch   4 Batch  176/575   train_loss = 5.014\n",
      "Epoch   4 Batch  177/575   train_loss = 5.168\n",
      "Epoch   4 Batch  178/575   train_loss = 4.855\n",
      "Epoch   4 Batch  179/575   train_loss = 4.622\n",
      "Epoch   4 Batch  180/575   train_loss = 4.725\n",
      "Epoch   4 Batch  181/575   train_loss = 4.971\n",
      "Epoch   4 Batch  182/575   train_loss = 4.669\n",
      "Epoch   4 Batch  183/575   train_loss = 5.078\n",
      "Epoch   4 Batch  184/575   train_loss = 4.207\n",
      "Epoch   4 Batch  185/575   train_loss = 4.725\n",
      "Epoch   4 Batch  186/575   train_loss = 5.022\n",
      "Epoch   4 Batch  187/575   train_loss = 4.746\n",
      "Epoch   4 Batch  188/575   train_loss = 4.806\n",
      "Epoch   4 Batch  189/575   train_loss = 4.698\n",
      "Epoch   4 Batch  190/575   train_loss = 4.895\n",
      "Epoch   4 Batch  191/575   train_loss = 5.353\n",
      "Epoch   4 Batch  192/575   train_loss = 5.180\n",
      "Epoch   4 Batch  193/575   train_loss = 4.835\n",
      "Epoch   4 Batch  194/575   train_loss = 4.995\n",
      "Epoch   4 Batch  195/575   train_loss = 4.594\n",
      "Epoch   4 Batch  196/575   train_loss = 4.582\n",
      "Epoch   4 Batch  197/575   train_loss = 4.531\n",
      "Epoch   4 Batch  198/575   train_loss = 5.170\n",
      "Epoch   4 Batch  199/575   train_loss = 4.283\n",
      "Epoch   4 Batch  200/575   train_loss = 4.576\n",
      "Epoch   4 Batch  201/575   train_loss = 4.851\n",
      "Epoch   4 Batch  202/575   train_loss = 4.385\n",
      "Epoch   4 Batch  203/575   train_loss = 4.686\n",
      "Epoch   4 Batch  204/575   train_loss = 4.682\n",
      "Epoch   4 Batch  205/575   train_loss = 4.405\n",
      "Epoch   4 Batch  206/575   train_loss = 4.821\n",
      "Epoch   4 Batch  207/575   train_loss = 4.545\n",
      "Epoch   4 Batch  208/575   train_loss = 4.374\n",
      "Epoch   4 Batch  209/575   train_loss = 4.466\n",
      "Epoch   4 Batch  210/575   train_loss = 4.755\n",
      "Epoch   4 Batch  211/575   train_loss = 5.484\n",
      "Epoch   4 Batch  212/575   train_loss = 5.031\n",
      "Epoch   4 Batch  213/575   train_loss = 4.920\n",
      "Epoch   4 Batch  214/575   train_loss = 4.540\n",
      "Epoch   4 Batch  215/575   train_loss = 4.004\n",
      "Epoch   4 Batch  216/575   train_loss = 4.547\n",
      "Epoch   4 Batch  217/575   train_loss = 4.268\n",
      "Epoch   4 Batch  218/575   train_loss = 4.940\n",
      "Epoch   4 Batch  219/575   train_loss = 4.647\n",
      "Epoch   4 Batch  220/575   train_loss = 4.198\n",
      "Epoch   4 Batch  221/575   train_loss = 4.885\n",
      "Epoch   4 Batch  222/575   train_loss = 4.914\n",
      "Epoch   4 Batch  223/575   train_loss = 5.143\n",
      "Epoch   4 Batch  224/575   train_loss = 4.417\n",
      "Epoch   4 Batch  225/575   train_loss = 4.622\n",
      "Epoch   4 Batch  226/575   train_loss = 4.911\n",
      "Epoch   4 Batch  227/575   train_loss = 5.222\n",
      "Epoch   4 Batch  228/575   train_loss = 5.088\n",
      "Epoch   4 Batch  229/575   train_loss = 3.972\n",
      "Epoch   4 Batch  230/575   train_loss = 4.146\n",
      "Epoch   4 Batch  231/575   train_loss = 4.724\n",
      "Epoch   4 Batch  232/575   train_loss = 4.834\n",
      "Epoch   4 Batch  233/575   train_loss = 4.915\n",
      "Epoch   4 Batch  234/575   train_loss = 4.942\n",
      "Epoch   4 Batch  235/575   train_loss = 4.530\n",
      "Epoch   4 Batch  236/575   train_loss = 4.180\n",
      "Epoch   4 Batch  237/575   train_loss = 4.481\n",
      "Epoch   4 Batch  238/575   train_loss = 4.528\n",
      "Epoch   4 Batch  239/575   train_loss = 4.868\n",
      "Epoch   4 Batch  240/575   train_loss = 5.094\n",
      "Epoch   4 Batch  241/575   train_loss = 5.178\n",
      "Epoch   4 Batch  242/575   train_loss = 4.617\n",
      "Epoch   4 Batch  243/575   train_loss = 4.816\n",
      "Epoch   4 Batch  244/575   train_loss = 4.765\n",
      "Epoch   4 Batch  245/575   train_loss = 4.892\n",
      "Epoch   4 Batch  246/575   train_loss = 5.193\n",
      "Epoch   4 Batch  247/575   train_loss = 4.601\n",
      "Epoch   4 Batch  248/575   train_loss = 4.951\n",
      "Epoch   4 Batch  249/575   train_loss = 4.835\n",
      "Epoch   4 Batch  250/575   train_loss = 4.625\n",
      "Epoch   4 Batch  251/575   train_loss = 4.892\n",
      "Epoch   4 Batch  252/575   train_loss = 5.102\n",
      "Epoch   4 Batch  253/575   train_loss = 4.776\n",
      "Epoch   4 Batch  254/575   train_loss = 4.338\n",
      "Epoch   4 Batch  255/575   train_loss = 5.026\n",
      "Epoch   4 Batch  256/575   train_loss = 4.646\n",
      "Epoch   4 Batch  257/575   train_loss = 4.813\n",
      "Epoch   4 Batch  258/575   train_loss = 5.181\n",
      "Epoch   4 Batch  259/575   train_loss = 4.363\n",
      "Epoch   4 Batch  260/575   train_loss = 4.471\n",
      "Epoch   4 Batch  261/575   train_loss = 5.407\n",
      "Epoch   4 Batch  262/575   train_loss = 5.069\n",
      "Epoch   4 Batch  263/575   train_loss = 4.863\n",
      "Epoch   4 Batch  264/575   train_loss = 4.209\n",
      "Epoch   4 Batch  265/575   train_loss = 4.038\n",
      "Epoch   4 Batch  266/575   train_loss = 4.265\n",
      "Epoch   4 Batch  267/575   train_loss = 4.735\n",
      "Epoch   4 Batch  268/575   train_loss = 4.753\n",
      "Epoch   4 Batch  269/575   train_loss = 4.888\n",
      "Epoch   4 Batch  270/575   train_loss = 5.289\n",
      "Epoch   4 Batch  271/575   train_loss = 4.442\n",
      "Epoch   4 Batch  272/575   train_loss = 5.028\n",
      "Epoch   4 Batch  273/575   train_loss = 4.969\n",
      "Epoch   4 Batch  274/575   train_loss = 4.933\n",
      "Epoch   4 Batch  275/575   train_loss = 4.858\n",
      "Epoch   4 Batch  276/575   train_loss = 5.340\n",
      "Epoch   4 Batch  277/575   train_loss = 4.016\n",
      "Epoch   4 Batch  278/575   train_loss = 4.758\n",
      "Epoch   4 Batch  279/575   train_loss = 4.866\n",
      "Epoch   4 Batch  280/575   train_loss = 5.116\n",
      "Epoch   4 Batch  281/575   train_loss = 4.582\n",
      "Epoch   4 Batch  282/575   train_loss = 4.833\n",
      "Epoch   4 Batch  283/575   train_loss = 5.091\n",
      "Epoch   4 Batch  284/575   train_loss = 5.289\n",
      "Epoch   4 Batch  285/575   train_loss = 4.587\n",
      "Epoch   4 Batch  286/575   train_loss = 4.847\n",
      "Epoch   4 Batch  287/575   train_loss = 5.054\n",
      "Epoch   4 Batch  288/575   train_loss = 4.747\n",
      "Epoch   4 Batch  289/575   train_loss = 4.790\n",
      "Epoch   4 Batch  290/575   train_loss = 4.871\n",
      "Epoch   4 Batch  291/575   train_loss = 5.475\n",
      "Epoch   4 Batch  292/575   train_loss = 5.266\n",
      "Epoch   4 Batch  293/575   train_loss = 5.502\n",
      "Epoch   4 Batch  294/575   train_loss = 5.151\n",
      "Epoch   4 Batch  295/575   train_loss = 4.717\n",
      "Epoch   4 Batch  296/575   train_loss = 4.770\n",
      "Epoch   4 Batch  297/575   train_loss = 4.909\n",
      "Epoch   4 Batch  298/575   train_loss = 5.005\n",
      "Epoch   4 Batch  299/575   train_loss = 4.926\n",
      "Epoch   4 Batch  300/575   train_loss = 4.736\n",
      "Epoch   4 Batch  301/575   train_loss = 4.628\n",
      "Epoch   4 Batch  302/575   train_loss = 5.098\n",
      "Epoch   4 Batch  303/575   train_loss = 4.480\n",
      "Epoch   4 Batch  304/575   train_loss = 4.655\n",
      "Epoch   4 Batch  305/575   train_loss = 4.664\n",
      "Epoch   4 Batch  306/575   train_loss = 4.991\n",
      "Epoch   4 Batch  307/575   train_loss = 5.043\n",
      "Epoch   4 Batch  308/575   train_loss = 4.951\n",
      "Epoch   4 Batch  309/575   train_loss = 5.208\n",
      "Epoch   4 Batch  310/575   train_loss = 5.179\n",
      "Epoch   4 Batch  311/575   train_loss = 4.473\n",
      "Epoch   4 Batch  312/575   train_loss = 4.998\n",
      "Epoch   4 Batch  313/575   train_loss = 5.344\n",
      "Epoch   4 Batch  314/575   train_loss = 5.207\n",
      "Epoch   4 Batch  315/575   train_loss = 5.428\n",
      "Epoch   4 Batch  316/575   train_loss = 4.770\n",
      "Epoch   4 Batch  317/575   train_loss = 4.404\n",
      "Epoch   4 Batch  318/575   train_loss = 4.115\n",
      "Epoch   4 Batch  319/575   train_loss = 4.833\n",
      "Epoch   4 Batch  320/575   train_loss = 4.582\n",
      "Epoch   4 Batch  321/575   train_loss = 5.268\n",
      "Epoch   4 Batch  322/575   train_loss = 4.767\n",
      "Epoch   4 Batch  323/575   train_loss = 5.209\n",
      "Epoch   4 Batch  324/575   train_loss = 4.716\n",
      "Epoch   4 Batch  325/575   train_loss = 4.901\n",
      "Epoch   4 Batch  326/575   train_loss = 4.649\n",
      "Epoch   4 Batch  327/575   train_loss = 5.047\n",
      "Epoch   4 Batch  328/575   train_loss = 4.774\n",
      "Epoch   4 Batch  329/575   train_loss = 5.261\n",
      "Epoch   4 Batch  330/575   train_loss = 4.892\n",
      "Epoch   4 Batch  331/575   train_loss = 5.430\n",
      "Epoch   4 Batch  332/575   train_loss = 5.118\n",
      "Epoch   4 Batch  333/575   train_loss = 5.018\n",
      "Epoch   4 Batch  334/575   train_loss = 4.806\n",
      "Epoch   4 Batch  335/575   train_loss = 4.585\n",
      "Epoch   4 Batch  336/575   train_loss = 4.933\n",
      "Epoch   4 Batch  337/575   train_loss = 4.853\n",
      "Epoch   4 Batch  338/575   train_loss = 4.579\n",
      "Epoch   4 Batch  339/575   train_loss = 4.910\n",
      "Epoch   4 Batch  340/575   train_loss = 4.698\n",
      "Epoch   4 Batch  341/575   train_loss = 4.662\n",
      "Epoch   4 Batch  342/575   train_loss = 4.560\n",
      "Epoch   4 Batch  343/575   train_loss = 4.981\n",
      "Epoch   4 Batch  344/575   train_loss = 5.041\n",
      "Epoch   4 Batch  345/575   train_loss = 4.664\n",
      "Epoch   4 Batch  346/575   train_loss = 5.425\n",
      "Epoch   4 Batch  347/575   train_loss = 4.553\n",
      "Epoch   4 Batch  348/575   train_loss = 4.579\n",
      "Epoch   4 Batch  349/575   train_loss = 4.347\n",
      "Epoch   4 Batch  350/575   train_loss = 4.966\n",
      "Epoch   4 Batch  351/575   train_loss = 5.329\n",
      "Epoch   4 Batch  352/575   train_loss = 5.099\n",
      "Epoch   4 Batch  353/575   train_loss = 5.052\n",
      "Epoch   4 Batch  354/575   train_loss = 4.713\n",
      "Epoch   4 Batch  355/575   train_loss = 5.269\n",
      "Epoch   4 Batch  356/575   train_loss = 4.963\n",
      "Epoch   4 Batch  357/575   train_loss = 4.864\n",
      "Epoch   4 Batch  358/575   train_loss = 5.169\n",
      "Epoch   4 Batch  359/575   train_loss = 5.397\n",
      "Epoch   4 Batch  360/575   train_loss = 4.393\n",
      "Epoch   4 Batch  361/575   train_loss = 5.062\n",
      "Epoch   4 Batch  362/575   train_loss = 4.818\n",
      "Epoch   4 Batch  363/575   train_loss = 4.852\n",
      "Epoch   4 Batch  364/575   train_loss = 4.510\n",
      "Epoch   4 Batch  365/575   train_loss = 4.984\n",
      "Epoch   4 Batch  366/575   train_loss = 4.432\n",
      "Epoch   4 Batch  367/575   train_loss = 4.798\n",
      "Epoch   4 Batch  368/575   train_loss = 5.465\n",
      "Epoch   4 Batch  369/575   train_loss = 5.150\n",
      "Epoch   4 Batch  370/575   train_loss = 5.065\n",
      "Epoch   4 Batch  371/575   train_loss = 5.182\n",
      "Epoch   4 Batch  372/575   train_loss = 4.699\n",
      "Epoch   4 Batch  373/575   train_loss = 5.114\n",
      "Epoch   4 Batch  374/575   train_loss = 4.277\n",
      "Epoch   4 Batch  375/575   train_loss = 4.624\n",
      "Epoch   4 Batch  376/575   train_loss = 4.927\n",
      "Epoch   4 Batch  377/575   train_loss = 5.302\n",
      "Epoch   4 Batch  378/575   train_loss = 5.381\n",
      "Epoch   4 Batch  379/575   train_loss = 4.697\n",
      "Epoch   4 Batch  380/575   train_loss = 4.410\n",
      "Epoch   4 Batch  381/575   train_loss = 4.670\n",
      "Epoch   4 Batch  382/575   train_loss = 4.951\n",
      "Epoch   4 Batch  383/575   train_loss = 4.718\n",
      "Epoch   4 Batch  384/575   train_loss = 4.847\n",
      "Epoch   4 Batch  385/575   train_loss = 4.623\n",
      "Epoch   4 Batch  386/575   train_loss = 4.613\n",
      "Epoch   4 Batch  387/575   train_loss = 5.326\n",
      "Epoch   4 Batch  388/575   train_loss = 4.712\n",
      "Epoch   4 Batch  389/575   train_loss = 4.714\n",
      "Epoch   4 Batch  390/575   train_loss = 4.964\n",
      "Epoch   4 Batch  391/575   train_loss = 5.038\n",
      "Epoch   4 Batch  392/575   train_loss = 4.850\n",
      "Epoch   4 Batch  393/575   train_loss = 4.869\n",
      "Epoch   4 Batch  394/575   train_loss = 5.174\n",
      "Epoch   4 Batch  395/575   train_loss = 4.253\n",
      "Epoch   4 Batch  396/575   train_loss = 4.592\n",
      "Epoch   4 Batch  397/575   train_loss = 5.015\n",
      "Epoch   4 Batch  398/575   train_loss = 5.326\n",
      "Epoch   4 Batch  399/575   train_loss = 5.288\n",
      "Epoch   4 Batch  400/575   train_loss = 5.162\n",
      "Epoch   4 Batch  401/575   train_loss = 4.482\n",
      "Epoch   4 Batch  402/575   train_loss = 4.601\n",
      "Epoch   4 Batch  403/575   train_loss = 5.035\n",
      "Epoch   4 Batch  404/575   train_loss = 5.104\n",
      "Epoch   4 Batch  405/575   train_loss = 5.135\n",
      "Epoch   4 Batch  406/575   train_loss = 4.563\n",
      "Epoch   4 Batch  407/575   train_loss = 4.927\n",
      "Epoch   4 Batch  408/575   train_loss = 4.855\n",
      "Epoch   4 Batch  409/575   train_loss = 4.797\n",
      "Epoch   4 Batch  410/575   train_loss = 4.628\n",
      "Epoch   4 Batch  411/575   train_loss = 4.856\n",
      "Epoch   4 Batch  412/575   train_loss = 5.479\n",
      "Epoch   4 Batch  413/575   train_loss = 5.109\n",
      "Epoch   4 Batch  414/575   train_loss = 4.805\n",
      "Epoch   4 Batch  415/575   train_loss = 5.334\n",
      "Epoch   4 Batch  416/575   train_loss = 4.769\n",
      "Epoch   4 Batch  417/575   train_loss = 4.423\n",
      "Epoch   4 Batch  418/575   train_loss = 4.873\n",
      "Epoch   4 Batch  419/575   train_loss = 4.785\n",
      "Epoch   4 Batch  420/575   train_loss = 4.658\n",
      "Epoch   4 Batch  421/575   train_loss = 5.347\n",
      "Epoch   4 Batch  422/575   train_loss = 5.034\n",
      "Epoch   4 Batch  423/575   train_loss = 5.426\n",
      "Epoch   4 Batch  424/575   train_loss = 5.458\n",
      "Epoch   4 Batch  425/575   train_loss = 5.162\n",
      "Epoch   4 Batch  426/575   train_loss = 4.996\n",
      "Epoch   4 Batch  427/575   train_loss = 4.828\n",
      "Epoch   4 Batch  428/575   train_loss = 4.830\n",
      "Epoch   4 Batch  429/575   train_loss = 4.576\n",
      "Epoch   4 Batch  430/575   train_loss = 4.921\n",
      "Epoch   4 Batch  431/575   train_loss = 4.826\n",
      "Epoch   4 Batch  432/575   train_loss = 5.273\n",
      "Epoch   4 Batch  433/575   train_loss = 5.211\n",
      "Epoch   4 Batch  434/575   train_loss = 5.047\n",
      "Epoch   4 Batch  435/575   train_loss = 4.885\n",
      "Epoch   4 Batch  436/575   train_loss = 4.601\n",
      "Epoch   4 Batch  437/575   train_loss = 4.855\n",
      "Epoch   4 Batch  438/575   train_loss = 5.093\n",
      "Epoch   4 Batch  439/575   train_loss = 4.923\n",
      "Epoch   4 Batch  440/575   train_loss = 5.147\n",
      "Epoch   4 Batch  441/575   train_loss = 4.890\n",
      "Epoch   4 Batch  442/575   train_loss = 5.040\n",
      "Epoch   4 Batch  443/575   train_loss = 5.004\n",
      "Epoch   4 Batch  444/575   train_loss = 5.075\n",
      "Epoch   4 Batch  445/575   train_loss = 4.660\n",
      "Epoch   4 Batch  446/575   train_loss = 4.583\n",
      "Epoch   4 Batch  447/575   train_loss = 5.115\n",
      "Epoch   4 Batch  448/575   train_loss = 4.928\n",
      "Epoch   4 Batch  449/575   train_loss = 4.956\n",
      "Epoch   4 Batch  450/575   train_loss = 5.529\n",
      "Epoch   4 Batch  451/575   train_loss = 5.423\n",
      "Epoch   4 Batch  452/575   train_loss = 5.502\n",
      "Epoch   4 Batch  453/575   train_loss = 4.989\n",
      "Epoch   4 Batch  454/575   train_loss = 5.354\n",
      "Epoch   4 Batch  455/575   train_loss = 4.641\n",
      "Epoch   4 Batch  456/575   train_loss = 5.362\n",
      "Epoch   4 Batch  457/575   train_loss = 4.539\n",
      "Epoch   4 Batch  458/575   train_loss = 5.262\n",
      "Epoch   4 Batch  459/575   train_loss = 5.048\n",
      "Epoch   4 Batch  460/575   train_loss = 4.803\n",
      "Epoch   4 Batch  461/575   train_loss = 4.998\n",
      "Epoch   4 Batch  462/575   train_loss = 4.856\n",
      "Epoch   4 Batch  463/575   train_loss = 4.781\n",
      "Epoch   4 Batch  464/575   train_loss = 4.643\n",
      "Epoch   4 Batch  465/575   train_loss = 4.778\n",
      "Epoch   4 Batch  466/575   train_loss = 4.782\n",
      "Epoch   4 Batch  467/575   train_loss = 4.254\n",
      "Epoch   4 Batch  468/575   train_loss = 4.719\n",
      "Epoch   4 Batch  469/575   train_loss = 5.236\n",
      "Epoch   4 Batch  470/575   train_loss = 5.478\n",
      "Epoch   4 Batch  471/575   train_loss = 4.716\n",
      "Epoch   4 Batch  472/575   train_loss = 5.156\n",
      "Epoch   4 Batch  473/575   train_loss = 4.987\n",
      "Epoch   4 Batch  474/575   train_loss = 4.985\n",
      "Epoch   4 Batch  475/575   train_loss = 4.707\n",
      "Epoch   4 Batch  476/575   train_loss = 4.867\n",
      "Epoch   4 Batch  477/575   train_loss = 5.416\n",
      "Epoch   4 Batch  478/575   train_loss = 5.484\n",
      "Epoch   4 Batch  479/575   train_loss = 4.809\n",
      "Epoch   4 Batch  480/575   train_loss = 5.103\n",
      "Epoch   4 Batch  481/575   train_loss = 5.157\n",
      "Epoch   4 Batch  482/575   train_loss = 4.439\n",
      "Epoch   4 Batch  483/575   train_loss = 5.785\n",
      "Epoch   4 Batch  484/575   train_loss = 4.906\n",
      "Epoch   4 Batch  485/575   train_loss = 5.447\n",
      "Epoch   4 Batch  486/575   train_loss = 4.993\n",
      "Epoch   4 Batch  487/575   train_loss = 5.225\n",
      "Epoch   4 Batch  488/575   train_loss = 4.934\n",
      "Epoch   4 Batch  489/575   train_loss = 5.182\n",
      "Epoch   4 Batch  490/575   train_loss = 5.184\n",
      "Epoch   4 Batch  491/575   train_loss = 4.414\n",
      "Epoch   4 Batch  492/575   train_loss = 4.293\n",
      "Epoch   4 Batch  493/575   train_loss = 4.441\n",
      "Epoch   4 Batch  494/575   train_loss = 4.700\n",
      "Epoch   4 Batch  495/575   train_loss = 4.862\n",
      "Epoch   4 Batch  496/575   train_loss = 5.001\n",
      "Epoch   4 Batch  497/575   train_loss = 5.204\n",
      "Epoch   4 Batch  498/575   train_loss = 4.722\n",
      "Epoch   4 Batch  499/575   train_loss = 4.695\n",
      "Epoch   4 Batch  500/575   train_loss = 4.978\n",
      "Epoch   4 Batch  501/575   train_loss = 4.905\n",
      "Epoch   4 Batch  502/575   train_loss = 4.288\n",
      "Epoch   4 Batch  503/575   train_loss = 5.099\n",
      "Epoch   4 Batch  504/575   train_loss = 4.429\n",
      "Epoch   4 Batch  505/575   train_loss = 4.797\n",
      "Epoch   4 Batch  506/575   train_loss = 5.173\n",
      "Epoch   4 Batch  507/575   train_loss = 5.218\n",
      "Epoch   4 Batch  508/575   train_loss = 5.175\n",
      "Epoch   4 Batch  509/575   train_loss = 4.933\n",
      "Epoch   4 Batch  510/575   train_loss = 4.312\n",
      "Epoch   4 Batch  511/575   train_loss = 4.355\n",
      "Epoch   4 Batch  512/575   train_loss = 5.292\n",
      "Epoch   4 Batch  513/575   train_loss = 5.019\n",
      "Epoch   4 Batch  514/575   train_loss = 4.827\n",
      "Epoch   4 Batch  515/575   train_loss = 4.946\n",
      "Epoch   4 Batch  516/575   train_loss = 5.247\n",
      "Epoch   4 Batch  517/575   train_loss = 4.505\n",
      "Epoch   4 Batch  518/575   train_loss = 4.631\n",
      "Epoch   4 Batch  519/575   train_loss = 4.760\n",
      "Epoch   4 Batch  520/575   train_loss = 4.079\n",
      "Epoch   4 Batch  521/575   train_loss = 4.511\n",
      "Epoch   4 Batch  522/575   train_loss = 4.614\n",
      "Epoch   4 Batch  523/575   train_loss = 5.364\n",
      "Epoch   4 Batch  524/575   train_loss = 6.247\n",
      "Epoch   4 Batch  525/575   train_loss = 4.954\n",
      "Epoch   4 Batch  526/575   train_loss = 4.642\n",
      "Epoch   4 Batch  527/575   train_loss = 5.113\n",
      "Epoch   4 Batch  528/575   train_loss = 4.666\n",
      "Epoch   4 Batch  529/575   train_loss = 5.150\n",
      "Epoch   4 Batch  530/575   train_loss = 4.983\n",
      "Epoch   4 Batch  531/575   train_loss = 5.058\n",
      "Epoch   4 Batch  532/575   train_loss = 4.682\n",
      "Epoch   4 Batch  533/575   train_loss = 4.672\n",
      "Epoch   4 Batch  534/575   train_loss = 4.701\n",
      "Epoch   4 Batch  535/575   train_loss = 5.087\n",
      "Epoch   4 Batch  536/575   train_loss = 4.820\n",
      "Epoch   4 Batch  537/575   train_loss = 4.497\n",
      "Epoch   4 Batch  538/575   train_loss = 4.856\n",
      "Epoch   4 Batch  539/575   train_loss = 4.697\n",
      "Epoch   4 Batch  540/575   train_loss = 4.963\n",
      "Epoch   4 Batch  541/575   train_loss = 5.448\n",
      "Epoch   4 Batch  542/575   train_loss = 4.919\n",
      "Epoch   4 Batch  543/575   train_loss = 5.126\n",
      "Epoch   4 Batch  544/575   train_loss = 4.879\n",
      "Epoch   4 Batch  545/575   train_loss = 4.953\n",
      "Epoch   4 Batch  546/575   train_loss = 4.942\n",
      "Epoch   4 Batch  547/575   train_loss = 4.919\n",
      "Epoch   4 Batch  548/575   train_loss = 5.001\n",
      "Epoch   4 Batch  549/575   train_loss = 4.747\n",
      "Epoch   4 Batch  550/575   train_loss = 4.719\n",
      "Epoch   4 Batch  551/575   train_loss = 4.688\n",
      "Epoch   4 Batch  552/575   train_loss = 4.897\n",
      "Epoch   4 Batch  553/575   train_loss = 5.034\n",
      "Epoch   4 Batch  554/575   train_loss = 4.644\n",
      "Epoch   4 Batch  555/575   train_loss = 4.935\n",
      "Epoch   4 Batch  556/575   train_loss = 4.608\n",
      "Epoch   4 Batch  557/575   train_loss = 4.488\n",
      "Epoch   4 Batch  558/575   train_loss = 4.863\n",
      "Epoch   4 Batch  559/575   train_loss = 4.990\n",
      "Epoch   4 Batch  560/575   train_loss = 4.396\n",
      "Epoch   4 Batch  561/575   train_loss = 4.836\n",
      "Epoch   4 Batch  562/575   train_loss = 4.351\n",
      "Epoch   4 Batch  563/575   train_loss = 4.827\n",
      "Epoch   4 Batch  564/575   train_loss = 4.495\n",
      "Epoch   4 Batch  565/575   train_loss = 4.101\n",
      "Epoch   4 Batch  566/575   train_loss = 4.538\n",
      "Epoch   4 Batch  567/575   train_loss = 4.386\n",
      "Epoch   4 Batch  568/575   train_loss = 4.555\n",
      "Epoch   4 Batch  569/575   train_loss = 4.890\n",
      "Epoch   4 Batch  570/575   train_loss = 4.451\n",
      "Epoch   4 Batch  571/575   train_loss = 4.327\n",
      "Epoch   4 Batch  572/575   train_loss = 4.007\n",
      "Epoch   4 Batch  573/575   train_loss = 4.431\n",
      "Epoch   4 Batch  574/575   train_loss = 5.044\n",
      "Epoch   5 Batch    0/575   train_loss = 4.653\n",
      "Epoch   5 Batch    1/575   train_loss = 4.633\n",
      "Epoch   5 Batch    2/575   train_loss = 4.873\n",
      "Epoch   5 Batch    3/575   train_loss = 4.602\n",
      "Epoch   5 Batch    4/575   train_loss = 4.518\n",
      "Epoch   5 Batch    5/575   train_loss = 4.474\n",
      "Epoch   5 Batch    6/575   train_loss = 4.376\n",
      "Epoch   5 Batch    7/575   train_loss = 4.685\n",
      "Epoch   5 Batch    8/575   train_loss = 4.684\n",
      "Epoch   5 Batch    9/575   train_loss = 4.861\n",
      "Epoch   5 Batch   10/575   train_loss = 4.349\n",
      "Epoch   5 Batch   11/575   train_loss = 4.514\n",
      "Epoch   5 Batch   12/575   train_loss = 4.643\n",
      "Epoch   5 Batch   13/575   train_loss = 4.830\n",
      "Epoch   5 Batch   14/575   train_loss = 5.281\n",
      "Epoch   5 Batch   15/575   train_loss = 4.986\n",
      "Epoch   5 Batch   16/575   train_loss = 4.790\n",
      "Epoch   5 Batch   17/575   train_loss = 4.161\n",
      "Epoch   5 Batch   18/575   train_loss = 5.133\n",
      "Epoch   5 Batch   19/575   train_loss = 4.781\n",
      "Epoch   5 Batch   20/575   train_loss = 4.244\n",
      "Epoch   5 Batch   21/575   train_loss = 4.582\n",
      "Epoch   5 Batch   22/575   train_loss = 5.252\n",
      "Epoch   5 Batch   23/575   train_loss = 4.766\n",
      "Epoch   5 Batch   24/575   train_loss = 4.662\n",
      "Epoch   5 Batch   25/575   train_loss = 4.822\n",
      "Epoch   5 Batch   26/575   train_loss = 5.014\n",
      "Epoch   5 Batch   27/575   train_loss = 4.799\n",
      "Epoch   5 Batch   28/575   train_loss = 4.303\n",
      "Epoch   5 Batch   29/575   train_loss = 4.487\n",
      "Epoch   5 Batch   30/575   train_loss = 5.261\n",
      "Epoch   5 Batch   31/575   train_loss = 5.547\n",
      "Epoch   5 Batch   32/575   train_loss = 4.810\n",
      "Epoch   5 Batch   33/575   train_loss = 5.101\n",
      "Epoch   5 Batch   34/575   train_loss = 4.686\n",
      "Epoch   5 Batch   35/575   train_loss = 4.574\n",
      "Epoch   5 Batch   36/575   train_loss = 4.978\n",
      "Epoch   5 Batch   37/575   train_loss = 4.108\n",
      "Epoch   5 Batch   38/575   train_loss = 4.859\n",
      "Epoch   5 Batch   39/575   train_loss = 4.204\n",
      "Epoch   5 Batch   40/575   train_loss = 4.013\n",
      "Epoch   5 Batch   41/575   train_loss = 5.959\n",
      "Epoch   5 Batch   42/575   train_loss = 4.837\n",
      "Epoch   5 Batch   43/575   train_loss = 4.780\n",
      "Epoch   5 Batch   44/575   train_loss = 4.704\n",
      "Epoch   5 Batch   45/575   train_loss = 4.529\n",
      "Epoch   5 Batch   46/575   train_loss = 4.524\n",
      "Epoch   5 Batch   47/575   train_loss = 4.927\n",
      "Epoch   5 Batch   48/575   train_loss = 4.078\n",
      "Epoch   5 Batch   49/575   train_loss = 4.356\n",
      "Epoch   5 Batch   50/575   train_loss = 4.583\n",
      "Epoch   5 Batch   51/575   train_loss = 4.458\n",
      "Epoch   5 Batch   52/575   train_loss = 4.448\n",
      "Epoch   5 Batch   53/575   train_loss = 4.204\n",
      "Epoch   5 Batch   54/575   train_loss = 4.710\n",
      "Epoch   5 Batch   55/575   train_loss = 5.017\n",
      "Epoch   5 Batch   56/575   train_loss = 4.368\n",
      "Epoch   5 Batch   57/575   train_loss = 4.716\n",
      "Epoch   5 Batch   58/575   train_loss = 4.372\n",
      "Epoch   5 Batch   59/575   train_loss = 4.171\n",
      "Epoch   5 Batch   60/575   train_loss = 4.843\n",
      "Epoch   5 Batch   61/575   train_loss = 4.801\n",
      "Epoch   5 Batch   62/575   train_loss = 4.315\n",
      "Epoch   5 Batch   63/575   train_loss = 5.109\n",
      "Epoch   5 Batch   64/575   train_loss = 4.996\n",
      "Epoch   5 Batch   65/575   train_loss = 4.507\n",
      "Epoch   5 Batch   66/575   train_loss = 4.466\n",
      "Epoch   5 Batch   67/575   train_loss = 4.372\n",
      "Epoch   5 Batch   68/575   train_loss = 4.453\n",
      "Epoch   5 Batch   69/575   train_loss = 4.155\n",
      "Epoch   5 Batch   70/575   train_loss = 4.653\n",
      "Epoch   5 Batch   71/575   train_loss = 4.405\n",
      "Epoch   5 Batch   72/575   train_loss = 5.007\n",
      "Epoch   5 Batch   73/575   train_loss = 5.022\n",
      "Epoch   5 Batch   74/575   train_loss = 5.201\n",
      "Epoch   5 Batch   75/575   train_loss = 4.793\n",
      "Epoch   5 Batch   76/575   train_loss = 5.113\n",
      "Epoch   5 Batch   77/575   train_loss = 5.165\n",
      "Epoch   5 Batch   78/575   train_loss = 4.888\n",
      "Epoch   5 Batch   79/575   train_loss = 4.701\n",
      "Epoch   5 Batch   80/575   train_loss = 4.629\n",
      "Epoch   5 Batch   81/575   train_loss = 4.553\n",
      "Epoch   5 Batch   82/575   train_loss = 4.312\n",
      "Epoch   5 Batch   83/575   train_loss = 4.586\n",
      "Epoch   5 Batch   84/575   train_loss = 4.603\n",
      "Epoch   5 Batch   85/575   train_loss = 5.327\n",
      "Epoch   5 Batch   86/575   train_loss = 4.581\n",
      "Epoch   5 Batch   87/575   train_loss = 5.199\n",
      "Epoch   5 Batch   88/575   train_loss = 4.022\n",
      "Epoch   5 Batch   89/575   train_loss = 4.252\n",
      "Epoch   5 Batch   90/575   train_loss = 4.470\n",
      "Epoch   5 Batch   91/575   train_loss = 4.740\n",
      "Epoch   5 Batch   92/575   train_loss = 5.264\n",
      "Epoch   5 Batch   93/575   train_loss = 4.838\n",
      "Epoch   5 Batch   94/575   train_loss = 4.364\n",
      "Epoch   5 Batch   95/575   train_loss = 4.761\n",
      "Epoch   5 Batch   96/575   train_loss = 4.759\n",
      "Epoch   5 Batch   97/575   train_loss = 4.857\n",
      "Epoch   5 Batch   98/575   train_loss = 4.567\n",
      "Epoch   5 Batch   99/575   train_loss = 5.219\n",
      "Epoch   5 Batch  100/575   train_loss = 4.940\n",
      "Epoch   5 Batch  101/575   train_loss = 4.541\n",
      "Epoch   5 Batch  102/575   train_loss = 4.329\n",
      "Epoch   5 Batch  103/575   train_loss = 4.950\n",
      "Epoch   5 Batch  104/575   train_loss = 4.689\n",
      "Epoch   5 Batch  105/575   train_loss = 4.865\n",
      "Epoch   5 Batch  106/575   train_loss = 4.558\n",
      "Epoch   5 Batch  107/575   train_loss = 4.411\n",
      "Epoch   5 Batch  108/575   train_loss = 4.857\n",
      "Epoch   5 Batch  109/575   train_loss = 4.588\n",
      "Epoch   5 Batch  110/575   train_loss = 3.752\n",
      "Epoch   5 Batch  111/575   train_loss = 4.621\n",
      "Epoch   5 Batch  112/575   train_loss = 4.796\n",
      "Epoch   5 Batch  113/575   train_loss = 5.334\n",
      "Epoch   5 Batch  114/575   train_loss = 4.954\n",
      "Epoch   5 Batch  115/575   train_loss = 4.479\n",
      "Epoch   5 Batch  116/575   train_loss = 4.942\n",
      "Epoch   5 Batch  117/575   train_loss = 4.284\n",
      "Epoch   5 Batch  118/575   train_loss = 4.797\n",
      "Epoch   5 Batch  119/575   train_loss = 5.312\n",
      "Epoch   5 Batch  120/575   train_loss = 4.823\n",
      "Epoch   5 Batch  121/575   train_loss = 4.930\n",
      "Epoch   5 Batch  122/575   train_loss = 4.933\n",
      "Epoch   5 Batch  123/575   train_loss = 4.690\n",
      "Epoch   5 Batch  124/575   train_loss = 5.124\n",
      "Epoch   5 Batch  125/575   train_loss = 4.906\n",
      "Epoch   5 Batch  126/575   train_loss = 4.779\n",
      "Epoch   5 Batch  127/575   train_loss = 4.222\n",
      "Epoch   5 Batch  128/575   train_loss = 4.630\n",
      "Epoch   5 Batch  129/575   train_loss = 4.622\n",
      "Epoch   5 Batch  130/575   train_loss = 4.966\n",
      "Epoch   5 Batch  131/575   train_loss = 4.143\n",
      "Epoch   5 Batch  132/575   train_loss = 4.515\n",
      "Epoch   5 Batch  133/575   train_loss = 4.735\n",
      "Epoch   5 Batch  134/575   train_loss = 4.580\n",
      "Epoch   5 Batch  135/575   train_loss = 4.269\n",
      "Epoch   5 Batch  136/575   train_loss = 5.671\n",
      "Epoch   5 Batch  137/575   train_loss = 4.813\n",
      "Epoch   5 Batch  138/575   train_loss = 5.227\n",
      "Epoch   5 Batch  139/575   train_loss = 4.935\n",
      "Epoch   5 Batch  140/575   train_loss = 4.870\n",
      "Epoch   5 Batch  141/575   train_loss = 5.203\n",
      "Epoch   5 Batch  142/575   train_loss = 5.500\n",
      "Epoch   5 Batch  143/575   train_loss = 4.902\n",
      "Epoch   5 Batch  144/575   train_loss = 4.452\n",
      "Epoch   5 Batch  145/575   train_loss = 4.466\n",
      "Epoch   5 Batch  146/575   train_loss = 4.464\n",
      "Epoch   5 Batch  147/575   train_loss = 4.582\n",
      "Epoch   5 Batch  148/575   train_loss = 4.106\n",
      "Epoch   5 Batch  149/575   train_loss = 4.795\n",
      "Epoch   5 Batch  150/575   train_loss = 5.096\n",
      "Epoch   5 Batch  151/575   train_loss = 4.370\n",
      "Epoch   5 Batch  152/575   train_loss = 4.315\n",
      "Epoch   5 Batch  153/575   train_loss = 3.876\n",
      "Epoch   5 Batch  154/575   train_loss = 4.037\n",
      "Epoch   5 Batch  155/575   train_loss = 4.799\n",
      "Epoch   5 Batch  156/575   train_loss = 4.608\n",
      "Epoch   5 Batch  157/575   train_loss = 4.784\n",
      "Epoch   5 Batch  158/575   train_loss = 4.700\n",
      "Epoch   5 Batch  159/575   train_loss = 4.899\n",
      "Epoch   5 Batch  160/575   train_loss = 4.416\n",
      "Epoch   5 Batch  161/575   train_loss = 5.076\n",
      "Epoch   5 Batch  162/575   train_loss = 4.905\n",
      "Epoch   5 Batch  163/575   train_loss = 4.636\n",
      "Epoch   5 Batch  164/575   train_loss = 4.974\n",
      "Epoch   5 Batch  165/575   train_loss = 4.836\n",
      "Epoch   5 Batch  166/575   train_loss = 4.407\n",
      "Epoch   5 Batch  167/575   train_loss = 4.467\n",
      "Epoch   5 Batch  168/575   train_loss = 4.884\n",
      "Epoch   5 Batch  169/575   train_loss = 4.477\n",
      "Epoch   5 Batch  170/575   train_loss = 4.973\n",
      "Epoch   5 Batch  171/575   train_loss = 4.527\n",
      "Epoch   5 Batch  172/575   train_loss = 4.803\n",
      "Epoch   5 Batch  173/575   train_loss = 4.775\n",
      "Epoch   5 Batch  174/575   train_loss = 5.056\n",
      "Epoch   5 Batch  175/575   train_loss = 4.782\n",
      "Epoch   5 Batch  176/575   train_loss = 4.850\n",
      "Epoch   5 Batch  177/575   train_loss = 4.951\n",
      "Epoch   5 Batch  178/575   train_loss = 4.696\n",
      "Epoch   5 Batch  179/575   train_loss = 4.460\n",
      "Epoch   5 Batch  180/575   train_loss = 4.550\n",
      "Epoch   5 Batch  181/575   train_loss = 4.781\n",
      "Epoch   5 Batch  182/575   train_loss = 4.545\n",
      "Epoch   5 Batch  183/575   train_loss = 4.944\n",
      "Epoch   5 Batch  184/575   train_loss = 4.056\n",
      "Epoch   5 Batch  185/575   train_loss = 4.594\n",
      "Epoch   5 Batch  186/575   train_loss = 4.863\n",
      "Epoch   5 Batch  187/575   train_loss = 4.641\n",
      "Epoch   5 Batch  188/575   train_loss = 4.651\n",
      "Epoch   5 Batch  189/575   train_loss = 4.581\n",
      "Epoch   5 Batch  190/575   train_loss = 4.721\n",
      "Epoch   5 Batch  191/575   train_loss = 5.185\n",
      "Epoch   5 Batch  192/575   train_loss = 5.012\n",
      "Epoch   5 Batch  193/575   train_loss = 4.693\n",
      "Epoch   5 Batch  194/575   train_loss = 4.837\n",
      "Epoch   5 Batch  195/575   train_loss = 4.447\n",
      "Epoch   5 Batch  196/575   train_loss = 4.468\n",
      "Epoch   5 Batch  197/575   train_loss = 4.421\n",
      "Epoch   5 Batch  198/575   train_loss = 4.999\n",
      "Epoch   5 Batch  199/575   train_loss = 4.156\n",
      "Epoch   5 Batch  200/575   train_loss = 4.448\n",
      "Epoch   5 Batch  201/575   train_loss = 4.726\n",
      "Epoch   5 Batch  202/575   train_loss = 4.308\n",
      "Epoch   5 Batch  203/575   train_loss = 4.567\n",
      "Epoch   5 Batch  204/575   train_loss = 4.544\n",
      "Epoch   5 Batch  205/575   train_loss = 4.263\n",
      "Epoch   5 Batch  206/575   train_loss = 4.664\n",
      "Epoch   5 Batch  207/575   train_loss = 4.436\n",
      "Epoch   5 Batch  208/575   train_loss = 4.251\n",
      "Epoch   5 Batch  209/575   train_loss = 4.342\n",
      "Epoch   5 Batch  210/575   train_loss = 4.673\n",
      "Epoch   5 Batch  211/575   train_loss = 5.318\n",
      "Epoch   5 Batch  212/575   train_loss = 4.905\n",
      "Epoch   5 Batch  213/575   train_loss = 4.748\n",
      "Epoch   5 Batch  214/575   train_loss = 4.385\n",
      "Epoch   5 Batch  215/575   train_loss = 3.927\n",
      "Epoch   5 Batch  216/575   train_loss = 4.394\n",
      "Epoch   5 Batch  217/575   train_loss = 4.119\n",
      "Epoch   5 Batch  218/575   train_loss = 4.806\n",
      "Epoch   5 Batch  219/575   train_loss = 4.486\n",
      "Epoch   5 Batch  220/575   train_loss = 4.079\n",
      "Epoch   5 Batch  221/575   train_loss = 4.702\n",
      "Epoch   5 Batch  222/575   train_loss = 4.743\n",
      "Epoch   5 Batch  223/575   train_loss = 4.993\n",
      "Epoch   5 Batch  224/575   train_loss = 4.284\n",
      "Epoch   5 Batch  225/575   train_loss = 4.447\n",
      "Epoch   5 Batch  226/575   train_loss = 4.754\n",
      "Epoch   5 Batch  227/575   train_loss = 5.088\n",
      "Epoch   5 Batch  228/575   train_loss = 4.929\n",
      "Epoch   5 Batch  229/575   train_loss = 3.880\n",
      "Epoch   5 Batch  230/575   train_loss = 4.054\n",
      "Epoch   5 Batch  231/575   train_loss = 4.582\n",
      "Epoch   5 Batch  232/575   train_loss = 4.717\n",
      "Epoch   5 Batch  233/575   train_loss = 4.738\n",
      "Epoch   5 Batch  234/575   train_loss = 4.795\n",
      "Epoch   5 Batch  235/575   train_loss = 4.420\n",
      "Epoch   5 Batch  236/575   train_loss = 4.045\n",
      "Epoch   5 Batch  237/575   train_loss = 4.360\n",
      "Epoch   5 Batch  238/575   train_loss = 4.403\n",
      "Epoch   5 Batch  239/575   train_loss = 4.687\n",
      "Epoch   5 Batch  240/575   train_loss = 4.950\n",
      "Epoch   5 Batch  241/575   train_loss = 5.015\n",
      "Epoch   5 Batch  242/575   train_loss = 4.499\n",
      "Epoch   5 Batch  243/575   train_loss = 4.677\n",
      "Epoch   5 Batch  244/575   train_loss = 4.589\n",
      "Epoch   5 Batch  245/575   train_loss = 4.760\n",
      "Epoch   5 Batch  246/575   train_loss = 5.001\n",
      "Epoch   5 Batch  247/575   train_loss = 4.503\n",
      "Epoch   5 Batch  248/575   train_loss = 4.790\n",
      "Epoch   5 Batch  249/575   train_loss = 4.685\n",
      "Epoch   5 Batch  250/575   train_loss = 4.495\n",
      "Epoch   5 Batch  251/575   train_loss = 4.729\n",
      "Epoch   5 Batch  252/575   train_loss = 4.949\n",
      "Epoch   5 Batch  253/575   train_loss = 4.626\n",
      "Epoch   5 Batch  254/575   train_loss = 4.205\n",
      "Epoch   5 Batch  255/575   train_loss = 4.874\n",
      "Epoch   5 Batch  256/575   train_loss = 4.512\n",
      "Epoch   5 Batch  257/575   train_loss = 4.672\n",
      "Epoch   5 Batch  258/575   train_loss = 5.039\n",
      "Epoch   5 Batch  259/575   train_loss = 4.204\n",
      "Epoch   5 Batch  260/575   train_loss = 4.364\n",
      "Epoch   5 Batch  261/575   train_loss = 5.211\n",
      "Epoch   5 Batch  262/575   train_loss = 4.895\n",
      "Epoch   5 Batch  263/575   train_loss = 4.777\n",
      "Epoch   5 Batch  264/575   train_loss = 4.046\n",
      "Epoch   5 Batch  265/575   train_loss = 3.943\n",
      "Epoch   5 Batch  266/575   train_loss = 4.111\n",
      "Epoch   5 Batch  267/575   train_loss = 4.574\n",
      "Epoch   5 Batch  268/575   train_loss = 4.613\n",
      "Epoch   5 Batch  269/575   train_loss = 4.774\n",
      "Epoch   5 Batch  270/575   train_loss = 5.151\n",
      "Epoch   5 Batch  271/575   train_loss = 4.320\n",
      "Epoch   5 Batch  272/575   train_loss = 4.855\n",
      "Epoch   5 Batch  273/575   train_loss = 4.836\n",
      "Epoch   5 Batch  274/575   train_loss = 4.803\n",
      "Epoch   5 Batch  275/575   train_loss = 4.661\n",
      "Epoch   5 Batch  276/575   train_loss = 5.195\n",
      "Epoch   5 Batch  277/575   train_loss = 3.914\n",
      "Epoch   5 Batch  278/575   train_loss = 4.677\n",
      "Epoch   5 Batch  279/575   train_loss = 4.782\n",
      "Epoch   5 Batch  280/575   train_loss = 4.973\n",
      "Epoch   5 Batch  281/575   train_loss = 4.459\n",
      "Epoch   5 Batch  282/575   train_loss = 4.717\n",
      "Epoch   5 Batch  283/575   train_loss = 4.962\n",
      "Epoch   5 Batch  284/575   train_loss = 5.159\n",
      "Epoch   5 Batch  285/575   train_loss = 4.437\n",
      "Epoch   5 Batch  286/575   train_loss = 4.717\n",
      "Epoch   5 Batch  287/575   train_loss = 4.898\n",
      "Epoch   5 Batch  288/575   train_loss = 4.618\n",
      "Epoch   5 Batch  289/575   train_loss = 4.646\n",
      "Epoch   5 Batch  290/575   train_loss = 4.731\n",
      "Epoch   5 Batch  291/575   train_loss = 5.306\n",
      "Epoch   5 Batch  292/575   train_loss = 5.196\n",
      "Epoch   5 Batch  293/575   train_loss = 5.387\n",
      "Epoch   5 Batch  294/575   train_loss = 5.015\n",
      "Epoch   5 Batch  295/575   train_loss = 4.571\n",
      "Epoch   5 Batch  296/575   train_loss = 4.675\n",
      "Epoch   5 Batch  297/575   train_loss = 4.767\n",
      "Epoch   5 Batch  298/575   train_loss = 4.871\n",
      "Epoch   5 Batch  299/575   train_loss = 4.790\n",
      "Epoch   5 Batch  300/575   train_loss = 4.632\n",
      "Epoch   5 Batch  301/575   train_loss = 4.506\n",
      "Epoch   5 Batch  302/575   train_loss = 4.953\n",
      "Epoch   5 Batch  303/575   train_loss = 4.364\n",
      "Epoch   5 Batch  304/575   train_loss = 4.520\n",
      "Epoch   5 Batch  305/575   train_loss = 4.518\n",
      "Epoch   5 Batch  306/575   train_loss = 4.827\n",
      "Epoch   5 Batch  307/575   train_loss = 4.878\n",
      "Epoch   5 Batch  308/575   train_loss = 4.792\n",
      "Epoch   5 Batch  309/575   train_loss = 5.034\n",
      "Epoch   5 Batch  310/575   train_loss = 5.060\n",
      "Epoch   5 Batch  311/575   train_loss = 4.353\n",
      "Epoch   5 Batch  312/575   train_loss = 4.817\n",
      "Epoch   5 Batch  313/575   train_loss = 5.159\n",
      "Epoch   5 Batch  314/575   train_loss = 5.075\n",
      "Epoch   5 Batch  315/575   train_loss = 5.256\n",
      "Epoch   5 Batch  316/575   train_loss = 4.608\n",
      "Epoch   5 Batch  317/575   train_loss = 4.269\n",
      "Epoch   5 Batch  318/575   train_loss = 3.976\n",
      "Epoch   5 Batch  319/575   train_loss = 4.692\n",
      "Epoch   5 Batch  320/575   train_loss = 4.487\n",
      "Epoch   5 Batch  321/575   train_loss = 5.078\n",
      "Epoch   5 Batch  322/575   train_loss = 4.622\n",
      "Epoch   5 Batch  323/575   train_loss = 5.087\n",
      "Epoch   5 Batch  324/575   train_loss = 4.590\n",
      "Epoch   5 Batch  325/575   train_loss = 4.758\n",
      "Epoch   5 Batch  326/575   train_loss = 4.511\n",
      "Epoch   5 Batch  327/575   train_loss = 4.904\n",
      "Epoch   5 Batch  328/575   train_loss = 4.575\n",
      "Epoch   5 Batch  329/575   train_loss = 5.154\n",
      "Epoch   5 Batch  330/575   train_loss = 4.745\n",
      "Epoch   5 Batch  331/575   train_loss = 5.272\n",
      "Epoch   5 Batch  332/575   train_loss = 4.940\n",
      "Epoch   5 Batch  333/575   train_loss = 4.839\n",
      "Epoch   5 Batch  334/575   train_loss = 4.661\n",
      "Epoch   5 Batch  335/575   train_loss = 4.417\n",
      "Epoch   5 Batch  336/575   train_loss = 4.786\n",
      "Epoch   5 Batch  337/575   train_loss = 4.691\n",
      "Epoch   5 Batch  338/575   train_loss = 4.465\n",
      "Epoch   5 Batch  339/575   train_loss = 4.772\n",
      "Epoch   5 Batch  340/575   train_loss = 4.578\n",
      "Epoch   5 Batch  341/575   train_loss = 4.568\n",
      "Epoch   5 Batch  342/575   train_loss = 4.420\n",
      "Epoch   5 Batch  343/575   train_loss = 4.876\n",
      "Epoch   5 Batch  344/575   train_loss = 4.912\n",
      "Epoch   5 Batch  345/575   train_loss = 4.538\n",
      "Epoch   5 Batch  346/575   train_loss = 5.279\n",
      "Epoch   5 Batch  347/575   train_loss = 4.396\n",
      "Epoch   5 Batch  348/575   train_loss = 4.429\n",
      "Epoch   5 Batch  349/575   train_loss = 4.217\n",
      "Epoch   5 Batch  350/575   train_loss = 4.833\n",
      "Epoch   5 Batch  351/575   train_loss = 5.181\n",
      "Epoch   5 Batch  352/575   train_loss = 4.992\n",
      "Epoch   5 Batch  353/575   train_loss = 4.911\n",
      "Epoch   5 Batch  354/575   train_loss = 4.553\n",
      "Epoch   5 Batch  355/575   train_loss = 5.141\n",
      "Epoch   5 Batch  356/575   train_loss = 4.855\n",
      "Epoch   5 Batch  357/575   train_loss = 4.778\n",
      "Epoch   5 Batch  358/575   train_loss = 5.017\n",
      "Epoch   5 Batch  359/575   train_loss = 5.241\n",
      "Epoch   5 Batch  360/575   train_loss = 4.283\n",
      "Epoch   5 Batch  361/575   train_loss = 4.871\n",
      "Epoch   5 Batch  362/575   train_loss = 4.657\n",
      "Epoch   5 Batch  363/575   train_loss = 4.708\n",
      "Epoch   5 Batch  364/575   train_loss = 4.411\n",
      "Epoch   5 Batch  365/575   train_loss = 4.863\n",
      "Epoch   5 Batch  366/575   train_loss = 4.296\n",
      "Epoch   5 Batch  367/575   train_loss = 4.623\n",
      "Epoch   5 Batch  368/575   train_loss = 5.328\n",
      "Epoch   5 Batch  369/575   train_loss = 5.028\n",
      "Epoch   5 Batch  370/575   train_loss = 4.908\n",
      "Epoch   5 Batch  371/575   train_loss = 5.000\n",
      "Epoch   5 Batch  372/575   train_loss = 4.538\n",
      "Epoch   5 Batch  373/575   train_loss = 5.005\n",
      "Epoch   5 Batch  374/575   train_loss = 4.140\n",
      "Epoch   5 Batch  375/575   train_loss = 4.499\n",
      "Epoch   5 Batch  376/575   train_loss = 4.819\n",
      "Epoch   5 Batch  377/575   train_loss = 5.153\n",
      "Epoch   5 Batch  378/575   train_loss = 5.263\n",
      "Epoch   5 Batch  379/575   train_loss = 4.590\n",
      "Epoch   5 Batch  380/575   train_loss = 4.297\n",
      "Epoch   5 Batch  381/575   train_loss = 4.508\n",
      "Epoch   5 Batch  382/575   train_loss = 4.786\n",
      "Epoch   5 Batch  383/575   train_loss = 4.576\n",
      "Epoch   5 Batch  384/575   train_loss = 4.739\n",
      "Epoch   5 Batch  385/575   train_loss = 4.541\n",
      "Epoch   5 Batch  386/575   train_loss = 4.491\n",
      "Epoch   5 Batch  387/575   train_loss = 5.185\n",
      "Epoch   5 Batch  388/575   train_loss = 4.629\n",
      "Epoch   5 Batch  389/575   train_loss = 4.590\n",
      "Epoch   5 Batch  390/575   train_loss = 4.845\n",
      "Epoch   5 Batch  391/575   train_loss = 4.888\n",
      "Epoch   5 Batch  392/575   train_loss = 4.766\n",
      "Epoch   5 Batch  393/575   train_loss = 4.775\n",
      "Epoch   5 Batch  394/575   train_loss = 5.038\n",
      "Epoch   5 Batch  395/575   train_loss = 4.118\n",
      "Epoch   5 Batch  396/575   train_loss = 4.428\n",
      "Epoch   5 Batch  397/575   train_loss = 4.829\n",
      "Epoch   5 Batch  398/575   train_loss = 5.241\n",
      "Epoch   5 Batch  399/575   train_loss = 5.159\n",
      "Epoch   5 Batch  400/575   train_loss = 5.008\n",
      "Epoch   5 Batch  401/575   train_loss = 4.357\n",
      "Epoch   5 Batch  402/575   train_loss = 4.456\n",
      "Epoch   5 Batch  403/575   train_loss = 4.877\n",
      "Epoch   5 Batch  404/575   train_loss = 4.917\n",
      "Epoch   5 Batch  405/575   train_loss = 4.971\n",
      "Epoch   5 Batch  406/575   train_loss = 4.442\n",
      "Epoch   5 Batch  407/575   train_loss = 4.816\n",
      "Epoch   5 Batch  408/575   train_loss = 4.679\n",
      "Epoch   5 Batch  409/575   train_loss = 4.701\n",
      "Epoch   5 Batch  410/575   train_loss = 4.498\n",
      "Epoch   5 Batch  411/575   train_loss = 4.795\n",
      "Epoch   5 Batch  412/575   train_loss = 5.337\n",
      "Epoch   5 Batch  413/575   train_loss = 4.969\n",
      "Epoch   5 Batch  414/575   train_loss = 4.618\n",
      "Epoch   5 Batch  415/575   train_loss = 5.131\n",
      "Epoch   5 Batch  416/575   train_loss = 4.639\n",
      "Epoch   5 Batch  417/575   train_loss = 4.304\n",
      "Epoch   5 Batch  418/575   train_loss = 4.753\n",
      "Epoch   5 Batch  419/575   train_loss = 4.628\n",
      "Epoch   5 Batch  420/575   train_loss = 4.503\n",
      "Epoch   5 Batch  421/575   train_loss = 5.195\n",
      "Epoch   5 Batch  422/575   train_loss = 4.921\n",
      "Epoch   5 Batch  423/575   train_loss = 5.290\n",
      "Epoch   5 Batch  424/575   train_loss = 5.290\n",
      "Epoch   5 Batch  425/575   train_loss = 4.978\n",
      "Epoch   5 Batch  426/575   train_loss = 4.842\n",
      "Epoch   5 Batch  427/575   train_loss = 4.741\n",
      "Epoch   5 Batch  428/575   train_loss = 4.686\n",
      "Epoch   5 Batch  429/575   train_loss = 4.486\n",
      "Epoch   5 Batch  430/575   train_loss = 4.822\n",
      "Epoch   5 Batch  431/575   train_loss = 4.683\n",
      "Epoch   5 Batch  432/575   train_loss = 5.096\n",
      "Epoch   5 Batch  433/575   train_loss = 5.063\n",
      "Epoch   5 Batch  434/575   train_loss = 4.915\n",
      "Epoch   5 Batch  435/575   train_loss = 4.700\n",
      "Epoch   5 Batch  436/575   train_loss = 4.473\n",
      "Epoch   5 Batch  437/575   train_loss = 4.689\n",
      "Epoch   5 Batch  438/575   train_loss = 4.945\n",
      "Epoch   5 Batch  439/575   train_loss = 4.813\n",
      "Epoch   5 Batch  440/575   train_loss = 5.036\n",
      "Epoch   5 Batch  441/575   train_loss = 4.740\n",
      "Epoch   5 Batch  442/575   train_loss = 4.905\n",
      "Epoch   5 Batch  443/575   train_loss = 4.877\n",
      "Epoch   5 Batch  444/575   train_loss = 4.929\n",
      "Epoch   5 Batch  445/575   train_loss = 4.490\n",
      "Epoch   5 Batch  446/575   train_loss = 4.454\n",
      "Epoch   5 Batch  447/575   train_loss = 4.976\n",
      "Epoch   5 Batch  448/575   train_loss = 4.807\n",
      "Epoch   5 Batch  449/575   train_loss = 4.793\n",
      "Epoch   5 Batch  450/575   train_loss = 5.340\n",
      "Epoch   5 Batch  451/575   train_loss = 5.278\n",
      "Epoch   5 Batch  452/575   train_loss = 5.322\n",
      "Epoch   5 Batch  453/575   train_loss = 4.849\n",
      "Epoch   5 Batch  454/575   train_loss = 5.206\n",
      "Epoch   5 Batch  455/575   train_loss = 4.485\n",
      "Epoch   5 Batch  456/575   train_loss = 5.174\n",
      "Epoch   5 Batch  457/575   train_loss = 4.377\n",
      "Epoch   5 Batch  458/575   train_loss = 5.132\n",
      "Epoch   5 Batch  459/575   train_loss = 4.879\n",
      "Epoch   5 Batch  460/575   train_loss = 4.625\n",
      "Epoch   5 Batch  461/575   train_loss = 4.808\n",
      "Epoch   5 Batch  462/575   train_loss = 4.737\n",
      "Epoch   5 Batch  463/575   train_loss = 4.602\n",
      "Epoch   5 Batch  464/575   train_loss = 4.509\n",
      "Epoch   5 Batch  465/575   train_loss = 4.659\n",
      "Epoch   5 Batch  466/575   train_loss = 4.669\n",
      "Epoch   5 Batch  467/575   train_loss = 4.075\n",
      "Epoch   5 Batch  468/575   train_loss = 4.609\n",
      "Epoch   5 Batch  469/575   train_loss = 5.083\n",
      "Epoch   5 Batch  470/575   train_loss = 5.309\n",
      "Epoch   5 Batch  471/575   train_loss = 4.573\n",
      "Epoch   5 Batch  472/575   train_loss = 5.017\n",
      "Epoch   5 Batch  473/575   train_loss = 4.867\n",
      "Epoch   5 Batch  474/575   train_loss = 4.858\n",
      "Epoch   5 Batch  475/575   train_loss = 4.559\n",
      "Epoch   5 Batch  476/575   train_loss = 4.721\n",
      "Epoch   5 Batch  477/575   train_loss = 5.281\n",
      "Epoch   5 Batch  478/575   train_loss = 5.345\n",
      "Epoch   5 Batch  479/575   train_loss = 4.634\n",
      "Epoch   5 Batch  480/575   train_loss = 4.938\n",
      "Epoch   5 Batch  481/575   train_loss = 5.011\n",
      "Epoch   5 Batch  482/575   train_loss = 4.303\n",
      "Epoch   5 Batch  483/575   train_loss = 5.557\n",
      "Epoch   5 Batch  484/575   train_loss = 4.734\n",
      "Epoch   5 Batch  485/575   train_loss = 5.318\n",
      "Epoch   5 Batch  486/575   train_loss = 4.836\n",
      "Epoch   5 Batch  487/575   train_loss = 5.077\n",
      "Epoch   5 Batch  488/575   train_loss = 4.788\n",
      "Epoch   5 Batch  489/575   train_loss = 5.020\n",
      "Epoch   5 Batch  490/575   train_loss = 5.015\n",
      "Epoch   5 Batch  491/575   train_loss = 4.286\n",
      "Epoch   5 Batch  492/575   train_loss = 4.214\n",
      "Epoch   5 Batch  493/575   train_loss = 4.323\n",
      "Epoch   5 Batch  494/575   train_loss = 4.617\n",
      "Epoch   5 Batch  495/575   train_loss = 4.720\n",
      "Epoch   5 Batch  496/575   train_loss = 4.862\n",
      "Epoch   5 Batch  497/575   train_loss = 5.027\n",
      "Epoch   5 Batch  498/575   train_loss = 4.586\n",
      "Epoch   5 Batch  499/575   train_loss = 4.557\n",
      "Epoch   5 Batch  500/575   train_loss = 4.860\n",
      "Epoch   5 Batch  501/575   train_loss = 4.808\n",
      "Epoch   5 Batch  502/575   train_loss = 4.156\n",
      "Epoch   5 Batch  503/575   train_loss = 4.998\n",
      "Epoch   5 Batch  504/575   train_loss = 4.277\n",
      "Epoch   5 Batch  505/575   train_loss = 4.666\n",
      "Epoch   5 Batch  506/575   train_loss = 5.010\n",
      "Epoch   5 Batch  507/575   train_loss = 5.079\n",
      "Epoch   5 Batch  508/575   train_loss = 5.080\n",
      "Epoch   5 Batch  509/575   train_loss = 4.781\n",
      "Epoch   5 Batch  510/575   train_loss = 4.183\n",
      "Epoch   5 Batch  511/575   train_loss = 4.245\n",
      "Epoch   5 Batch  512/575   train_loss = 5.129\n",
      "Epoch   5 Batch  513/575   train_loss = 4.853\n",
      "Epoch   5 Batch  514/575   train_loss = 4.689\n",
      "Epoch   5 Batch  515/575   train_loss = 4.783\n",
      "Epoch   5 Batch  516/575   train_loss = 5.095\n",
      "Epoch   5 Batch  517/575   train_loss = 4.325\n",
      "Epoch   5 Batch  518/575   train_loss = 4.487\n",
      "Epoch   5 Batch  519/575   train_loss = 4.663\n",
      "Epoch   5 Batch  520/575   train_loss = 3.990\n",
      "Epoch   5 Batch  521/575   train_loss = 4.393\n",
      "Epoch   5 Batch  522/575   train_loss = 4.513\n",
      "Epoch   5 Batch  523/575   train_loss = 5.156\n",
      "Epoch   5 Batch  524/575   train_loss = 5.914\n",
      "Epoch   5 Batch  525/575   train_loss = 4.870\n",
      "Epoch   5 Batch  526/575   train_loss = 4.495\n",
      "Epoch   5 Batch  527/575   train_loss = 5.012\n",
      "Epoch   5 Batch  528/575   train_loss = 4.536\n",
      "Epoch   5 Batch  529/575   train_loss = 5.048\n",
      "Epoch   5 Batch  530/575   train_loss = 4.838\n",
      "Epoch   5 Batch  531/575   train_loss = 4.925\n",
      "Epoch   5 Batch  532/575   train_loss = 4.550\n",
      "Epoch   5 Batch  533/575   train_loss = 4.502\n",
      "Epoch   5 Batch  534/575   train_loss = 4.556\n",
      "Epoch   5 Batch  535/575   train_loss = 4.898\n",
      "Epoch   5 Batch  536/575   train_loss = 4.638\n",
      "Epoch   5 Batch  537/575   train_loss = 4.376\n",
      "Epoch   5 Batch  538/575   train_loss = 4.720\n",
      "Epoch   5 Batch  539/575   train_loss = 4.570\n",
      "Epoch   5 Batch  540/575   train_loss = 4.819\n",
      "Epoch   5 Batch  541/575   train_loss = 5.285\n",
      "Epoch   5 Batch  542/575   train_loss = 4.770\n",
      "Epoch   5 Batch  543/575   train_loss = 4.968\n",
      "Epoch   5 Batch  544/575   train_loss = 4.718\n",
      "Epoch   5 Batch  545/575   train_loss = 4.775\n",
      "Epoch   5 Batch  546/575   train_loss = 4.773\n",
      "Epoch   5 Batch  547/575   train_loss = 4.770\n",
      "Epoch   5 Batch  548/575   train_loss = 4.837\n",
      "Epoch   5 Batch  549/575   train_loss = 4.620\n",
      "Epoch   5 Batch  550/575   train_loss = 4.547\n",
      "Epoch   5 Batch  551/575   train_loss = 4.533\n",
      "Epoch   5 Batch  552/575   train_loss = 4.759\n",
      "Epoch   5 Batch  553/575   train_loss = 4.932\n",
      "Epoch   5 Batch  554/575   train_loss = 4.483\n",
      "Epoch   5 Batch  555/575   train_loss = 4.761\n",
      "Epoch   5 Batch  556/575   train_loss = 4.466\n",
      "Epoch   5 Batch  557/575   train_loss = 4.378\n",
      "Epoch   5 Batch  558/575   train_loss = 4.735\n",
      "Epoch   5 Batch  559/575   train_loss = 4.884\n",
      "Epoch   5 Batch  560/575   train_loss = 4.284\n",
      "Epoch   5 Batch  561/575   train_loss = 4.690\n",
      "Epoch   5 Batch  562/575   train_loss = 4.221\n",
      "Epoch   5 Batch  563/575   train_loss = 4.666\n",
      "Epoch   5 Batch  564/575   train_loss = 4.368\n",
      "Epoch   5 Batch  565/575   train_loss = 3.920\n",
      "Epoch   5 Batch  566/575   train_loss = 4.393\n",
      "Epoch   5 Batch  567/575   train_loss = 4.329\n",
      "Epoch   5 Batch  568/575   train_loss = 4.464\n",
      "Epoch   5 Batch  569/575   train_loss = 4.754\n",
      "Epoch   5 Batch  570/575   train_loss = 4.326\n",
      "Epoch   5 Batch  571/575   train_loss = 4.164\n",
      "Epoch   5 Batch  572/575   train_loss = 3.905\n",
      "Epoch   5 Batch  573/575   train_loss = 4.262\n",
      "Epoch   5 Batch  574/575   train_loss = 4.881\n",
      "Epoch   6 Batch    0/575   train_loss = 4.539\n",
      "Epoch   6 Batch    1/575   train_loss = 4.505\n",
      "Epoch   6 Batch    2/575   train_loss = 4.712\n",
      "Epoch   6 Batch    3/575   train_loss = 4.462\n",
      "Epoch   6 Batch    4/575   train_loss = 4.393\n",
      "Epoch   6 Batch    5/575   train_loss = 4.328\n",
      "Epoch   6 Batch    6/575   train_loss = 4.223\n",
      "Epoch   6 Batch    7/575   train_loss = 4.552\n",
      "Epoch   6 Batch    8/575   train_loss = 4.543\n",
      "Epoch   6 Batch    9/575   train_loss = 4.742\n",
      "Epoch   6 Batch   10/575   train_loss = 4.238\n",
      "Epoch   6 Batch   11/575   train_loss = 4.368\n",
      "Epoch   6 Batch   12/575   train_loss = 4.522\n",
      "Epoch   6 Batch   13/575   train_loss = 4.699\n",
      "Epoch   6 Batch   14/575   train_loss = 5.146\n",
      "Epoch   6 Batch   15/575   train_loss = 4.833\n",
      "Epoch   6 Batch   16/575   train_loss = 4.674\n",
      "Epoch   6 Batch   17/575   train_loss = 4.016\n",
      "Epoch   6 Batch   18/575   train_loss = 5.015\n",
      "Epoch   6 Batch   19/575   train_loss = 4.666\n",
      "Epoch   6 Batch   20/575   train_loss = 4.087\n",
      "Epoch   6 Batch   21/575   train_loss = 4.502\n",
      "Epoch   6 Batch   22/575   train_loss = 5.107\n",
      "Epoch   6 Batch   23/575   train_loss = 4.635\n",
      "Epoch   6 Batch   24/575   train_loss = 4.544\n",
      "Epoch   6 Batch   25/575   train_loss = 4.707\n",
      "Epoch   6 Batch   26/575   train_loss = 4.915\n",
      "Epoch   6 Batch   27/575   train_loss = 4.653\n",
      "Epoch   6 Batch   28/575   train_loss = 4.184\n",
      "Epoch   6 Batch   29/575   train_loss = 4.366\n",
      "Epoch   6 Batch   30/575   train_loss = 5.083\n",
      "Epoch   6 Batch   31/575   train_loss = 5.413\n",
      "Epoch   6 Batch   32/575   train_loss = 4.693\n",
      "Epoch   6 Batch   33/575   train_loss = 4.988\n",
      "Epoch   6 Batch   34/575   train_loss = 4.577\n",
      "Epoch   6 Batch   35/575   train_loss = 4.402\n",
      "Epoch   6 Batch   36/575   train_loss = 4.890\n",
      "Epoch   6 Batch   37/575   train_loss = 3.973\n",
      "Epoch   6 Batch   38/575   train_loss = 4.738\n",
      "Epoch   6 Batch   39/575   train_loss = 4.085\n",
      "Epoch   6 Batch   40/575   train_loss = 3.896\n",
      "Epoch   6 Batch   41/575   train_loss = 5.826\n",
      "Epoch   6 Batch   42/575   train_loss = 4.717\n",
      "Epoch   6 Batch   43/575   train_loss = 4.652\n",
      "Epoch   6 Batch   44/575   train_loss = 4.554\n",
      "Epoch   6 Batch   45/575   train_loss = 4.405\n",
      "Epoch   6 Batch   46/575   train_loss = 4.388\n",
      "Epoch   6 Batch   47/575   train_loss = 4.822\n",
      "Epoch   6 Batch   48/575   train_loss = 3.982\n",
      "Epoch   6 Batch   49/575   train_loss = 4.253\n",
      "Epoch   6 Batch   50/575   train_loss = 4.470\n",
      "Epoch   6 Batch   51/575   train_loss = 4.363\n",
      "Epoch   6 Batch   52/575   train_loss = 4.378\n",
      "Epoch   6 Batch   53/575   train_loss = 4.061\n",
      "Epoch   6 Batch   54/575   train_loss = 4.557\n",
      "Epoch   6 Batch   55/575   train_loss = 4.878\n",
      "Epoch   6 Batch   56/575   train_loss = 4.241\n",
      "Epoch   6 Batch   57/575   train_loss = 4.629\n",
      "Epoch   6 Batch   58/575   train_loss = 4.236\n",
      "Epoch   6 Batch   59/575   train_loss = 4.094\n",
      "Epoch   6 Batch   60/575   train_loss = 4.682\n",
      "Epoch   6 Batch   61/575   train_loss = 4.685\n",
      "Epoch   6 Batch   62/575   train_loss = 4.158\n",
      "Epoch   6 Batch   63/575   train_loss = 4.964\n",
      "Epoch   6 Batch   64/575   train_loss = 4.861\n",
      "Epoch   6 Batch   65/575   train_loss = 4.384\n",
      "Epoch   6 Batch   66/575   train_loss = 4.355\n",
      "Epoch   6 Batch   67/575   train_loss = 4.276\n",
      "Epoch   6 Batch   68/575   train_loss = 4.317\n",
      "Epoch   6 Batch   69/575   train_loss = 4.062\n",
      "Epoch   6 Batch   70/575   train_loss = 4.522\n",
      "Epoch   6 Batch   71/575   train_loss = 4.280\n",
      "Epoch   6 Batch   72/575   train_loss = 4.861\n",
      "Epoch   6 Batch   73/575   train_loss = 4.873\n",
      "Epoch   6 Batch   74/575   train_loss = 5.005\n",
      "Epoch   6 Batch   75/575   train_loss = 4.641\n",
      "Epoch   6 Batch   76/575   train_loss = 4.942\n",
      "Epoch   6 Batch   77/575   train_loss = 5.047\n",
      "Epoch   6 Batch   78/575   train_loss = 4.773\n",
      "Epoch   6 Batch   79/575   train_loss = 4.544\n",
      "Epoch   6 Batch   80/575   train_loss = 4.495\n",
      "Epoch   6 Batch   81/575   train_loss = 4.426\n",
      "Epoch   6 Batch   82/575   train_loss = 4.187\n",
      "Epoch   6 Batch   83/575   train_loss = 4.501\n",
      "Epoch   6 Batch   84/575   train_loss = 4.487\n",
      "Epoch   6 Batch   85/575   train_loss = 5.137\n",
      "Epoch   6 Batch   86/575   train_loss = 4.436\n",
      "Epoch   6 Batch   87/575   train_loss = 5.071\n",
      "Epoch   6 Batch   88/575   train_loss = 3.888\n",
      "Epoch   6 Batch   89/575   train_loss = 4.131\n",
      "Epoch   6 Batch   90/575   train_loss = 4.328\n",
      "Epoch   6 Batch   91/575   train_loss = 4.590\n",
      "Epoch   6 Batch   92/575   train_loss = 5.099\n",
      "Epoch   6 Batch   93/575   train_loss = 4.704\n",
      "Epoch   6 Batch   94/575   train_loss = 4.235\n",
      "Epoch   6 Batch   95/575   train_loss = 4.601\n",
      "Epoch   6 Batch   96/575   train_loss = 4.631\n",
      "Epoch   6 Batch   97/575   train_loss = 4.717\n",
      "Epoch   6 Batch   98/575   train_loss = 4.464\n",
      "Epoch   6 Batch   99/575   train_loss = 5.063\n",
      "Epoch   6 Batch  100/575   train_loss = 4.796\n",
      "Epoch   6 Batch  101/575   train_loss = 4.392\n",
      "Epoch   6 Batch  102/575   train_loss = 4.213\n",
      "Epoch   6 Batch  103/575   train_loss = 4.765\n",
      "Epoch   6 Batch  104/575   train_loss = 4.564\n",
      "Epoch   6 Batch  105/575   train_loss = 4.712\n",
      "Epoch   6 Batch  106/575   train_loss = 4.394\n",
      "Epoch   6 Batch  107/575   train_loss = 4.274\n",
      "Epoch   6 Batch  108/575   train_loss = 4.705\n",
      "Epoch   6 Batch  109/575   train_loss = 4.438\n",
      "Epoch   6 Batch  110/575   train_loss = 3.593\n",
      "Epoch   6 Batch  111/575   train_loss = 4.500\n",
      "Epoch   6 Batch  112/575   train_loss = 4.639\n",
      "Epoch   6 Batch  113/575   train_loss = 5.168\n",
      "Epoch   6 Batch  114/575   train_loss = 4.849\n",
      "Epoch   6 Batch  115/575   train_loss = 4.373\n",
      "Epoch   6 Batch  116/575   train_loss = 4.798\n",
      "Epoch   6 Batch  117/575   train_loss = 4.180\n",
      "Epoch   6 Batch  118/575   train_loss = 4.622\n",
      "Epoch   6 Batch  119/575   train_loss = 5.129\n",
      "Epoch   6 Batch  120/575   train_loss = 4.699\n",
      "Epoch   6 Batch  121/575   train_loss = 4.767\n",
      "Epoch   6 Batch  122/575   train_loss = 4.769\n",
      "Epoch   6 Batch  123/575   train_loss = 4.522\n",
      "Epoch   6 Batch  124/575   train_loss = 4.963\n",
      "Epoch   6 Batch  125/575   train_loss = 4.742\n",
      "Epoch   6 Batch  126/575   train_loss = 4.592\n",
      "Epoch   6 Batch  127/575   train_loss = 4.082\n",
      "Epoch   6 Batch  128/575   train_loss = 4.554\n",
      "Epoch   6 Batch  129/575   train_loss = 4.522\n",
      "Epoch   6 Batch  130/575   train_loss = 4.812\n",
      "Epoch   6 Batch  131/575   train_loss = 4.026\n",
      "Epoch   6 Batch  132/575   train_loss = 4.421\n",
      "Epoch   6 Batch  133/575   train_loss = 4.612\n",
      "Epoch   6 Batch  134/575   train_loss = 4.462\n",
      "Epoch   6 Batch  135/575   train_loss = 4.140\n",
      "Epoch   6 Batch  136/575   train_loss = 5.489\n",
      "Epoch   6 Batch  137/575   train_loss = 4.727\n",
      "Epoch   6 Batch  138/575   train_loss = 5.084\n",
      "Epoch   6 Batch  139/575   train_loss = 4.744\n",
      "Epoch   6 Batch  140/575   train_loss = 4.712\n",
      "Epoch   6 Batch  141/575   train_loss = 5.022\n",
      "Epoch   6 Batch  142/575   train_loss = 5.266\n",
      "Epoch   6 Batch  143/575   train_loss = 4.772\n",
      "Epoch   6 Batch  144/575   train_loss = 4.333\n",
      "Epoch   6 Batch  145/575   train_loss = 4.303\n",
      "Epoch   6 Batch  146/575   train_loss = 4.311\n",
      "Epoch   6 Batch  147/575   train_loss = 4.478\n",
      "Epoch   6 Batch  148/575   train_loss = 3.987\n",
      "Epoch   6 Batch  149/575   train_loss = 4.671\n",
      "Epoch   6 Batch  150/575   train_loss = 4.927\n",
      "Epoch   6 Batch  151/575   train_loss = 4.216\n",
      "Epoch   6 Batch  152/575   train_loss = 4.193\n",
      "Epoch   6 Batch  153/575   train_loss = 3.731\n",
      "Epoch   6 Batch  154/575   train_loss = 3.908\n",
      "Epoch   6 Batch  155/575   train_loss = 4.637\n",
      "Epoch   6 Batch  156/575   train_loss = 4.495\n",
      "Epoch   6 Batch  157/575   train_loss = 4.634\n",
      "Epoch   6 Batch  158/575   train_loss = 4.587\n",
      "Epoch   6 Batch  159/575   train_loss = 4.746\n",
      "Epoch   6 Batch  160/575   train_loss = 4.320\n",
      "Epoch   6 Batch  161/575   train_loss = 4.908\n",
      "Epoch   6 Batch  162/575   train_loss = 4.748\n",
      "Epoch   6 Batch  163/575   train_loss = 4.506\n",
      "Epoch   6 Batch  164/575   train_loss = 4.831\n",
      "Epoch   6 Batch  165/575   train_loss = 4.682\n",
      "Epoch   6 Batch  166/575   train_loss = 4.286\n",
      "Epoch   6 Batch  167/575   train_loss = 4.333\n",
      "Epoch   6 Batch  168/575   train_loss = 4.739\n",
      "Epoch   6 Batch  169/575   train_loss = 4.349\n",
      "Epoch   6 Batch  170/575   train_loss = 4.818\n",
      "Epoch   6 Batch  171/575   train_loss = 4.396\n",
      "Epoch   6 Batch  172/575   train_loss = 4.640\n",
      "Epoch   6 Batch  173/575   train_loss = 4.623\n",
      "Epoch   6 Batch  174/575   train_loss = 4.865\n",
      "Epoch   6 Batch  175/575   train_loss = 4.636\n",
      "Epoch   6 Batch  176/575   train_loss = 4.704\n",
      "Epoch   6 Batch  177/575   train_loss = 4.767\n",
      "Epoch   6 Batch  178/575   train_loss = 4.552\n",
      "Epoch   6 Batch  179/575   train_loss = 4.305\n",
      "Epoch   6 Batch  180/575   train_loss = 4.386\n",
      "Epoch   6 Batch  181/575   train_loss = 4.609\n",
      "Epoch   6 Batch  182/575   train_loss = 4.416\n",
      "Epoch   6 Batch  183/575   train_loss = 4.813\n",
      "Epoch   6 Batch  184/575   train_loss = 3.929\n",
      "Epoch   6 Batch  185/575   train_loss = 4.474\n",
      "Epoch   6 Batch  186/575   train_loss = 4.672\n",
      "Epoch   6 Batch  187/575   train_loss = 4.520\n",
      "Epoch   6 Batch  188/575   train_loss = 4.499\n",
      "Epoch   6 Batch  189/575   train_loss = 4.498\n",
      "Epoch   6 Batch  190/575   train_loss = 4.548\n",
      "Epoch   6 Batch  191/575   train_loss = 5.003\n",
      "Epoch   6 Batch  192/575   train_loss = 4.852\n",
      "Epoch   6 Batch  193/575   train_loss = 4.559\n",
      "Epoch   6 Batch  194/575   train_loss = 4.685\n",
      "Epoch   6 Batch  195/575   train_loss = 4.312\n",
      "Epoch   6 Batch  196/575   train_loss = 4.377\n",
      "Epoch   6 Batch  197/575   train_loss = 4.303\n",
      "Epoch   6 Batch  198/575   train_loss = 4.844\n",
      "Epoch   6 Batch  199/575   train_loss = 4.030\n",
      "Epoch   6 Batch  200/575   train_loss = 4.312\n",
      "Epoch   6 Batch  201/575   train_loss = 4.571\n",
      "Epoch   6 Batch  202/575   train_loss = 4.209\n",
      "Epoch   6 Batch  203/575   train_loss = 4.453\n",
      "Epoch   6 Batch  204/575   train_loss = 4.397\n",
      "Epoch   6 Batch  205/575   train_loss = 4.142\n",
      "Epoch   6 Batch  206/575   train_loss = 4.491\n",
      "Epoch   6 Batch  207/575   train_loss = 4.296\n",
      "Epoch   6 Batch  208/575   train_loss = 4.155\n",
      "Epoch   6 Batch  209/575   train_loss = 4.222\n",
      "Epoch   6 Batch  210/575   train_loss = 4.600\n",
      "Epoch   6 Batch  211/575   train_loss = 5.149\n",
      "Epoch   6 Batch  212/575   train_loss = 4.778\n",
      "Epoch   6 Batch  213/575   train_loss = 4.594\n",
      "Epoch   6 Batch  214/575   train_loss = 4.275\n",
      "Epoch   6 Batch  215/575   train_loss = 3.827\n",
      "Epoch   6 Batch  216/575   train_loss = 4.248\n",
      "Epoch   6 Batch  217/575   train_loss = 3.996\n",
      "Epoch   6 Batch  218/575   train_loss = 4.673\n",
      "Epoch   6 Batch  219/575   train_loss = 4.347\n",
      "Epoch   6 Batch  220/575   train_loss = 3.947\n",
      "Epoch   6 Batch  221/575   train_loss = 4.514\n",
      "Epoch   6 Batch  222/575   train_loss = 4.596\n",
      "Epoch   6 Batch  223/575   train_loss = 4.853\n",
      "Epoch   6 Batch  224/575   train_loss = 4.175\n",
      "Epoch   6 Batch  225/575   train_loss = 4.306\n",
      "Epoch   6 Batch  226/575   train_loss = 4.640\n",
      "Epoch   6 Batch  227/575   train_loss = 4.949\n",
      "Epoch   6 Batch  228/575   train_loss = 4.777\n",
      "Epoch   6 Batch  229/575   train_loss = 3.797\n",
      "Epoch   6 Batch  230/575   train_loss = 3.974\n",
      "Epoch   6 Batch  231/575   train_loss = 4.459\n",
      "Epoch   6 Batch  232/575   train_loss = 4.595\n",
      "Epoch   6 Batch  233/575   train_loss = 4.575\n",
      "Epoch   6 Batch  234/575   train_loss = 4.675\n",
      "Epoch   6 Batch  235/575   train_loss = 4.334\n",
      "Epoch   6 Batch  236/575   train_loss = 3.930\n",
      "Epoch   6 Batch  237/575   train_loss = 4.229\n",
      "Epoch   6 Batch  238/575   train_loss = 4.247\n",
      "Epoch   6 Batch  239/575   train_loss = 4.549\n",
      "Epoch   6 Batch  240/575   train_loss = 4.819\n",
      "Epoch   6 Batch  241/575   train_loss = 4.874\n",
      "Epoch   6 Batch  242/575   train_loss = 4.364\n",
      "Epoch   6 Batch  243/575   train_loss = 4.575\n",
      "Epoch   6 Batch  244/575   train_loss = 4.443\n",
      "Epoch   6 Batch  245/575   train_loss = 4.641\n",
      "Epoch   6 Batch  246/575   train_loss = 4.861\n",
      "Epoch   6 Batch  247/575   train_loss = 4.411\n",
      "Epoch   6 Batch  248/575   train_loss = 4.673\n",
      "Epoch   6 Batch  249/575   train_loss = 4.544\n",
      "Epoch   6 Batch  250/575   train_loss = 4.372\n",
      "Epoch   6 Batch  251/575   train_loss = 4.605\n",
      "Epoch   6 Batch  252/575   train_loss = 4.851\n",
      "Epoch   6 Batch  253/575   train_loss = 4.489\n",
      "Epoch   6 Batch  254/575   train_loss = 4.092\n",
      "Epoch   6 Batch  255/575   train_loss = 4.746\n",
      "Epoch   6 Batch  256/575   train_loss = 4.404\n",
      "Epoch   6 Batch  257/575   train_loss = 4.532\n",
      "Epoch   6 Batch  258/575   train_loss = 4.909\n",
      "Epoch   6 Batch  259/575   train_loss = 4.068\n",
      "Epoch   6 Batch  260/575   train_loss = 4.258\n",
      "Epoch   6 Batch  261/575   train_loss = 5.040\n",
      "Epoch   6 Batch  262/575   train_loss = 4.742\n",
      "Epoch   6 Batch  263/575   train_loss = 4.677\n",
      "Epoch   6 Batch  264/575   train_loss = 3.899\n",
      "Epoch   6 Batch  265/575   train_loss = 3.842\n",
      "Epoch   6 Batch  266/575   train_loss = 3.965\n",
      "Epoch   6 Batch  267/575   train_loss = 4.444\n",
      "Epoch   6 Batch  268/575   train_loss = 4.470\n",
      "Epoch   6 Batch  269/575   train_loss = 4.635\n",
      "Epoch   6 Batch  270/575   train_loss = 5.022\n",
      "Epoch   6 Batch  271/575   train_loss = 4.212\n",
      "Epoch   6 Batch  272/575   train_loss = 4.709\n",
      "Epoch   6 Batch  273/575   train_loss = 4.713\n",
      "Epoch   6 Batch  274/575   train_loss = 4.701\n",
      "Epoch   6 Batch  275/575   train_loss = 4.513\n",
      "Epoch   6 Batch  276/575   train_loss = 5.060\n",
      "Epoch   6 Batch  277/575   train_loss = 3.810\n",
      "Epoch   6 Batch  278/575   train_loss = 4.576\n",
      "Epoch   6 Batch  279/575   train_loss = 4.676\n",
      "Epoch   6 Batch  280/575   train_loss = 4.786\n",
      "Epoch   6 Batch  281/575   train_loss = 4.304\n",
      "Epoch   6 Batch  282/575   train_loss = 4.607\n",
      "Epoch   6 Batch  283/575   train_loss = 4.855\n",
      "Epoch   6 Batch  284/575   train_loss = 5.033\n",
      "Epoch   6 Batch  285/575   train_loss = 4.285\n",
      "Epoch   6 Batch  286/575   train_loss = 4.595\n",
      "Epoch   6 Batch  287/575   train_loss = 4.744\n",
      "Epoch   6 Batch  288/575   train_loss = 4.485\n",
      "Epoch   6 Batch  289/575   train_loss = 4.516\n",
      "Epoch   6 Batch  290/575   train_loss = 4.579\n",
      "Epoch   6 Batch  291/575   train_loss = 5.151\n",
      "Epoch   6 Batch  292/575   train_loss = 5.052\n",
      "Epoch   6 Batch  293/575   train_loss = 5.239\n",
      "Epoch   6 Batch  294/575   train_loss = 4.883\n",
      "Epoch   6 Batch  295/575   train_loss = 4.441\n",
      "Epoch   6 Batch  296/575   train_loss = 4.579\n",
      "Epoch   6 Batch  297/575   train_loss = 4.634\n",
      "Epoch   6 Batch  298/575   train_loss = 4.754\n",
      "Epoch   6 Batch  299/575   train_loss = 4.671\n",
      "Epoch   6 Batch  300/575   train_loss = 4.539\n",
      "Epoch   6 Batch  301/575   train_loss = 4.408\n",
      "Epoch   6 Batch  302/575   train_loss = 4.804\n",
      "Epoch   6 Batch  303/575   train_loss = 4.268\n",
      "Epoch   6 Batch  304/575   train_loss = 4.400\n",
      "Epoch   6 Batch  305/575   train_loss = 4.383\n",
      "Epoch   6 Batch  306/575   train_loss = 4.687\n",
      "Epoch   6 Batch  307/575   train_loss = 4.739\n",
      "Epoch   6 Batch  308/575   train_loss = 4.620\n",
      "Epoch   6 Batch  309/575   train_loss = 4.889\n",
      "Epoch   6 Batch  310/575   train_loss = 4.929\n",
      "Epoch   6 Batch  311/575   train_loss = 4.250\n",
      "Epoch   6 Batch  312/575   train_loss = 4.664\n",
      "Epoch   6 Batch  313/575   train_loss = 5.017\n",
      "Epoch   6 Batch  314/575   train_loss = 4.919\n",
      "Epoch   6 Batch  315/575   train_loss = 5.123\n",
      "Epoch   6 Batch  316/575   train_loss = 4.487\n",
      "Epoch   6 Batch  317/575   train_loss = 4.143\n",
      "Epoch   6 Batch  318/575   train_loss = 3.857\n",
      "Epoch   6 Batch  319/575   train_loss = 4.581\n",
      "Epoch   6 Batch  320/575   train_loss = 4.406\n",
      "Epoch   6 Batch  321/575   train_loss = 4.892\n",
      "Epoch   6 Batch  322/575   train_loss = 4.490\n",
      "Epoch   6 Batch  323/575   train_loss = 4.990\n",
      "Epoch   6 Batch  324/575   train_loss = 4.480\n",
      "Epoch   6 Batch  325/575   train_loss = 4.624\n",
      "Epoch   6 Batch  326/575   train_loss = 4.389\n",
      "Epoch   6 Batch  327/575   train_loss = 4.786\n",
      "Epoch   6 Batch  328/575   train_loss = 4.416\n",
      "Epoch   6 Batch  329/575   train_loss = 5.026\n",
      "Epoch   6 Batch  330/575   train_loss = 4.625\n",
      "Epoch   6 Batch  331/575   train_loss = 5.116\n",
      "Epoch   6 Batch  332/575   train_loss = 4.738\n",
      "Epoch   6 Batch  333/575   train_loss = 4.681\n",
      "Epoch   6 Batch  334/575   train_loss = 4.531\n",
      "Epoch   6 Batch  335/575   train_loss = 4.279\n",
      "Epoch   6 Batch  336/575   train_loss = 4.648\n",
      "Epoch   6 Batch  337/575   train_loss = 4.518\n",
      "Epoch   6 Batch  338/575   train_loss = 4.334\n",
      "Epoch   6 Batch  339/575   train_loss = 4.646\n",
      "Epoch   6 Batch  340/575   train_loss = 4.447\n",
      "Epoch   6 Batch  341/575   train_loss = 4.443\n",
      "Epoch   6 Batch  342/575   train_loss = 4.295\n",
      "Epoch   6 Batch  343/575   train_loss = 4.779\n",
      "Epoch   6 Batch  344/575   train_loss = 4.809\n",
      "Epoch   6 Batch  345/575   train_loss = 4.423\n",
      "Epoch   6 Batch  346/575   train_loss = 5.119\n",
      "Epoch   6 Batch  347/575   train_loss = 4.288\n",
      "Epoch   6 Batch  348/575   train_loss = 4.305\n",
      "Epoch   6 Batch  349/575   train_loss = 4.104\n",
      "Epoch   6 Batch  350/575   train_loss = 4.709\n",
      "Epoch   6 Batch  351/575   train_loss = 5.033\n",
      "Epoch   6 Batch  352/575   train_loss = 4.876\n",
      "Epoch   6 Batch  353/575   train_loss = 4.768\n",
      "Epoch   6 Batch  354/575   train_loss = 4.373\n",
      "Epoch   6 Batch  355/575   train_loss = 4.991\n",
      "Epoch   6 Batch  356/575   train_loss = 4.754\n",
      "Epoch   6 Batch  357/575   train_loss = 4.701\n",
      "Epoch   6 Batch  358/575   train_loss = 4.872\n",
      "Epoch   6 Batch  359/575   train_loss = 5.070\n",
      "Epoch   6 Batch  360/575   train_loss = 4.177\n",
      "Epoch   6 Batch  361/575   train_loss = 4.710\n",
      "Epoch   6 Batch  362/575   train_loss = 4.519\n",
      "Epoch   6 Batch  363/575   train_loss = 4.611\n",
      "Epoch   6 Batch  364/575   train_loss = 4.322\n",
      "Epoch   6 Batch  365/575   train_loss = 4.733\n",
      "Epoch   6 Batch  366/575   train_loss = 4.141\n",
      "Epoch   6 Batch  367/575   train_loss = 4.446\n",
      "Epoch   6 Batch  368/575   train_loss = 5.188\n",
      "Epoch   6 Batch  369/575   train_loss = 4.895\n",
      "Epoch   6 Batch  370/575   train_loss = 4.771\n",
      "Epoch   6 Batch  371/575   train_loss = 4.824\n",
      "Epoch   6 Batch  372/575   train_loss = 4.395\n",
      "Epoch   6 Batch  373/575   train_loss = 4.850\n",
      "Epoch   6 Batch  374/575   train_loss = 4.008\n",
      "Epoch   6 Batch  375/575   train_loss = 4.375\n",
      "Epoch   6 Batch  376/575   train_loss = 4.723\n",
      "Epoch   6 Batch  377/575   train_loss = 5.024\n",
      "Epoch   6 Batch  378/575   train_loss = 5.142\n",
      "Epoch   6 Batch  379/575   train_loss = 4.489\n",
      "Epoch   6 Batch  380/575   train_loss = 4.158\n",
      "Epoch   6 Batch  381/575   train_loss = 4.375\n",
      "Epoch   6 Batch  382/575   train_loss = 4.636\n",
      "Epoch   6 Batch  383/575   train_loss = 4.458\n",
      "Epoch   6 Batch  384/575   train_loss = 4.600\n",
      "Epoch   6 Batch  385/575   train_loss = 4.455\n",
      "Epoch   6 Batch  386/575   train_loss = 4.379\n",
      "Epoch   6 Batch  387/575   train_loss = 5.048\n",
      "Epoch   6 Batch  388/575   train_loss = 4.544\n",
      "Epoch   6 Batch  389/575   train_loss = 4.471\n",
      "Epoch   6 Batch  390/575   train_loss = 4.719\n",
      "Epoch   6 Batch  391/575   train_loss = 4.757\n",
      "Epoch   6 Batch  392/575   train_loss = 4.667\n",
      "Epoch   6 Batch  393/575   train_loss = 4.678\n",
      "Epoch   6 Batch  394/575   train_loss = 4.893\n",
      "Epoch   6 Batch  395/575   train_loss = 3.998\n",
      "Epoch   6 Batch  396/575   train_loss = 4.313\n",
      "Epoch   6 Batch  397/575   train_loss = 4.674\n",
      "Epoch   6 Batch  398/575   train_loss = 5.115\n",
      "Epoch   6 Batch  399/575   train_loss = 5.004\n",
      "Epoch   6 Batch  400/575   train_loss = 4.885\n",
      "Epoch   6 Batch  401/575   train_loss = 4.253\n",
      "Epoch   6 Batch  402/575   train_loss = 4.344\n",
      "Epoch   6 Batch  403/575   train_loss = 4.755\n",
      "Epoch   6 Batch  404/575   train_loss = 4.750\n",
      "Epoch   6 Batch  405/575   train_loss = 4.810\n",
      "Epoch   6 Batch  406/575   train_loss = 4.315\n",
      "Epoch   6 Batch  407/575   train_loss = 4.677\n",
      "Epoch   6 Batch  408/575   train_loss = 4.515\n",
      "Epoch   6 Batch  409/575   train_loss = 4.595\n",
      "Epoch   6 Batch  410/575   train_loss = 4.366\n",
      "Epoch   6 Batch  411/575   train_loss = 4.691\n",
      "Epoch   6 Batch  412/575   train_loss = 5.187\n",
      "Epoch   6 Batch  413/575   train_loss = 4.817\n",
      "Epoch   6 Batch  414/575   train_loss = 4.448\n",
      "Epoch   6 Batch  415/575   train_loss = 4.952\n",
      "Epoch   6 Batch  416/575   train_loss = 4.521\n",
      "Epoch   6 Batch  417/575   train_loss = 4.194\n",
      "Epoch   6 Batch  418/575   train_loss = 4.665\n",
      "Epoch   6 Batch  419/575   train_loss = 4.513\n",
      "Epoch   6 Batch  420/575   train_loss = 4.385\n",
      "Epoch   6 Batch  421/575   train_loss = 5.038\n",
      "Epoch   6 Batch  422/575   train_loss = 4.783\n",
      "Epoch   6 Batch  423/575   train_loss = 5.138\n",
      "Epoch   6 Batch  424/575   train_loss = 5.115\n",
      "Epoch   6 Batch  425/575   train_loss = 4.787\n",
      "Epoch   6 Batch  426/575   train_loss = 4.708\n",
      "Epoch   6 Batch  427/575   train_loss = 4.619\n",
      "Epoch   6 Batch  428/575   train_loss = 4.508\n",
      "Epoch   6 Batch  429/575   train_loss = 4.381\n",
      "Epoch   6 Batch  430/575   train_loss = 4.708\n",
      "Epoch   6 Batch  431/575   train_loss = 4.581\n",
      "Epoch   6 Batch  432/575   train_loss = 4.923\n",
      "Epoch   6 Batch  433/575   train_loss = 4.923\n",
      "Epoch   6 Batch  434/575   train_loss = 4.772\n",
      "Epoch   6 Batch  435/575   train_loss = 4.537\n",
      "Epoch   6 Batch  436/575   train_loss = 4.397\n",
      "Epoch   6 Batch  437/575   train_loss = 4.542\n",
      "Epoch   6 Batch  438/575   train_loss = 4.781\n",
      "Epoch   6 Batch  439/575   train_loss = 4.675\n",
      "Epoch   6 Batch  440/575   train_loss = 4.909\n",
      "Epoch   6 Batch  441/575   train_loss = 4.576\n",
      "Epoch   6 Batch  442/575   train_loss = 4.809\n",
      "Epoch   6 Batch  443/575   train_loss = 4.723\n",
      "Epoch   6 Batch  444/575   train_loss = 4.789\n",
      "Epoch   6 Batch  445/575   train_loss = 4.347\n",
      "Epoch   6 Batch  446/575   train_loss = 4.314\n",
      "Epoch   6 Batch  447/575   train_loss = 4.833\n",
      "Epoch   6 Batch  448/575   train_loss = 4.699\n",
      "Epoch   6 Batch  449/575   train_loss = 4.627\n",
      "Epoch   6 Batch  450/575   train_loss = 5.121\n",
      "Epoch   6 Batch  451/575   train_loss = 5.097\n",
      "Epoch   6 Batch  452/575   train_loss = 5.148\n",
      "Epoch   6 Batch  453/575   train_loss = 4.714\n",
      "Epoch   6 Batch  454/575   train_loss = 5.079\n",
      "Epoch   6 Batch  455/575   train_loss = 4.352\n",
      "Epoch   6 Batch  456/575   train_loss = 4.990\n",
      "Epoch   6 Batch  457/575   train_loss = 4.236\n",
      "Epoch   6 Batch  458/575   train_loss = 5.003\n",
      "Epoch   6 Batch  459/575   train_loss = 4.696\n",
      "Epoch   6 Batch  460/575   train_loss = 4.475\n",
      "Epoch   6 Batch  461/575   train_loss = 4.641\n",
      "Epoch   6 Batch  462/575   train_loss = 4.576\n",
      "Epoch   6 Batch  463/575   train_loss = 4.427\n",
      "Epoch   6 Batch  464/575   train_loss = 4.418\n",
      "Epoch   6 Batch  465/575   train_loss = 4.552\n",
      "Epoch   6 Batch  466/575   train_loss = 4.527\n",
      "Epoch   6 Batch  467/575   train_loss = 3.926\n",
      "Epoch   6 Batch  468/575   train_loss = 4.501\n",
      "Epoch   6 Batch  469/575   train_loss = 4.949\n",
      "Epoch   6 Batch  470/575   train_loss = 5.119\n",
      "Epoch   6 Batch  471/575   train_loss = 4.436\n",
      "Epoch   6 Batch  472/575   train_loss = 4.885\n",
      "Epoch   6 Batch  473/575   train_loss = 4.736\n",
      "Epoch   6 Batch  474/575   train_loss = 4.740\n",
      "Epoch   6 Batch  475/575   train_loss = 4.429\n",
      "Epoch   6 Batch  476/575   train_loss = 4.586\n",
      "Epoch   6 Batch  477/575   train_loss = 5.133\n",
      "Epoch   6 Batch  478/575   train_loss = 5.194\n",
      "Epoch   6 Batch  479/575   train_loss = 4.464\n",
      "Epoch   6 Batch  480/575   train_loss = 4.759\n",
      "Epoch   6 Batch  481/575   train_loss = 4.860\n",
      "Epoch   6 Batch  482/575   train_loss = 4.195\n",
      "Epoch   6 Batch  483/575   train_loss = 5.354\n",
      "Epoch   6 Batch  484/575   train_loss = 4.555\n",
      "Epoch   6 Batch  485/575   train_loss = 5.197\n",
      "Epoch   6 Batch  486/575   train_loss = 4.658\n",
      "Epoch   6 Batch  487/575   train_loss = 4.954\n",
      "Epoch   6 Batch  488/575   train_loss = 4.658\n",
      "Epoch   6 Batch  489/575   train_loss = 4.855\n",
      "Epoch   6 Batch  490/575   train_loss = 4.843\n",
      "Epoch   6 Batch  491/575   train_loss = 4.160\n",
      "Epoch   6 Batch  492/575   train_loss = 4.136\n",
      "Epoch   6 Batch  493/575   train_loss = 4.237\n",
      "Epoch   6 Batch  494/575   train_loss = 4.536\n",
      "Epoch   6 Batch  495/575   train_loss = 4.576\n",
      "Epoch   6 Batch  496/575   train_loss = 4.723\n",
      "Epoch   6 Batch  497/575   train_loss = 4.854\n",
      "Epoch   6 Batch  498/575   train_loss = 4.457\n",
      "Epoch   6 Batch  499/575   train_loss = 4.416\n",
      "Epoch   6 Batch  500/575   train_loss = 4.746\n",
      "Epoch   6 Batch  501/575   train_loss = 4.708\n",
      "Epoch   6 Batch  502/575   train_loss = 4.045\n",
      "Epoch   6 Batch  503/575   train_loss = 4.908\n",
      "Epoch   6 Batch  504/575   train_loss = 4.094\n",
      "Epoch   6 Batch  505/575   train_loss = 4.551\n",
      "Epoch   6 Batch  506/575   train_loss = 4.846\n",
      "Epoch   6 Batch  507/575   train_loss = 4.953\n",
      "Epoch   6 Batch  508/575   train_loss = 4.969\n",
      "Epoch   6 Batch  509/575   train_loss = 4.640\n",
      "Epoch   6 Batch  510/575   train_loss = 4.048\n",
      "Epoch   6 Batch  511/575   train_loss = 4.136\n",
      "Epoch   6 Batch  512/575   train_loss = 4.975\n",
      "Epoch   6 Batch  513/575   train_loss = 4.683\n",
      "Epoch   6 Batch  514/575   train_loss = 4.558\n",
      "Epoch   6 Batch  515/575   train_loss = 4.661\n",
      "Epoch   6 Batch  516/575   train_loss = 4.955\n",
      "Epoch   6 Batch  517/575   train_loss = 4.161\n",
      "Epoch   6 Batch  518/575   train_loss = 4.345\n",
      "Epoch   6 Batch  519/575   train_loss = 4.541\n",
      "Epoch   6 Batch  520/575   train_loss = 3.933\n",
      "Epoch   6 Batch  521/575   train_loss = 4.274\n",
      "Epoch   6 Batch  522/575   train_loss = 4.391\n",
      "Epoch   6 Batch  523/575   train_loss = 4.936\n",
      "Epoch   6 Batch  524/575   train_loss = 5.620\n",
      "Epoch   6 Batch  525/575   train_loss = 4.813\n",
      "Epoch   6 Batch  526/575   train_loss = 4.338\n",
      "Epoch   6 Batch  527/575   train_loss = 4.876\n",
      "Epoch   6 Batch  528/575   train_loss = 4.400\n",
      "Epoch   6 Batch  529/575   train_loss = 4.912\n",
      "Epoch   6 Batch  530/575   train_loss = 4.696\n",
      "Epoch   6 Batch  531/575   train_loss = 4.782\n",
      "Epoch   6 Batch  532/575   train_loss = 4.395\n",
      "Epoch   6 Batch  533/575   train_loss = 4.320\n",
      "Epoch   6 Batch  534/575   train_loss = 4.399\n",
      "Epoch   6 Batch  535/575   train_loss = 4.671\n",
      "Epoch   6 Batch  536/575   train_loss = 4.470\n",
      "Epoch   6 Batch  537/575   train_loss = 4.268\n",
      "Epoch   6 Batch  538/575   train_loss = 4.568\n",
      "Epoch   6 Batch  539/575   train_loss = 4.437\n",
      "Epoch   6 Batch  540/575   train_loss = 4.677\n",
      "Epoch   6 Batch  541/575   train_loss = 5.105\n",
      "Epoch   6 Batch  542/575   train_loss = 4.631\n",
      "Epoch   6 Batch  543/575   train_loss = 4.782\n",
      "Epoch   6 Batch  544/575   train_loss = 4.566\n",
      "Epoch   6 Batch  545/575   train_loss = 4.600\n",
      "Epoch   6 Batch  546/575   train_loss = 4.634\n",
      "Epoch   6 Batch  547/575   train_loss = 4.630\n",
      "Epoch   6 Batch  548/575   train_loss = 4.711\n",
      "Epoch   6 Batch  549/575   train_loss = 4.486\n",
      "Epoch   6 Batch  550/575   train_loss = 4.423\n",
      "Epoch   6 Batch  551/575   train_loss = 4.382\n",
      "Epoch   6 Batch  552/575   train_loss = 4.615\n",
      "Epoch   6 Batch  553/575   train_loss = 4.843\n",
      "Epoch   6 Batch  554/575   train_loss = 4.305\n",
      "Epoch   6 Batch  555/575   train_loss = 4.549\n",
      "Epoch   6 Batch  556/575   train_loss = 4.328\n",
      "Epoch   6 Batch  557/575   train_loss = 4.266\n",
      "Epoch   6 Batch  558/575   train_loss = 4.594\n",
      "Epoch   6 Batch  559/575   train_loss = 4.781\n",
      "Epoch   6 Batch  560/575   train_loss = 4.175\n",
      "Epoch   6 Batch  561/575   train_loss = 4.594\n",
      "Epoch   6 Batch  562/575   train_loss = 4.085\n",
      "Epoch   6 Batch  563/575   train_loss = 4.508\n",
      "Epoch   6 Batch  564/575   train_loss = 4.241\n",
      "Epoch   6 Batch  565/575   train_loss = 3.743\n",
      "Epoch   6 Batch  566/575   train_loss = 4.260\n",
      "Epoch   6 Batch  567/575   train_loss = 4.267\n",
      "Epoch   6 Batch  568/575   train_loss = 4.363\n",
      "Epoch   6 Batch  569/575   train_loss = 4.623\n",
      "Epoch   6 Batch  570/575   train_loss = 4.233\n",
      "Epoch   6 Batch  571/575   train_loss = 4.022\n",
      "Epoch   6 Batch  572/575   train_loss = 3.787\n",
      "Epoch   6 Batch  573/575   train_loss = 4.127\n",
      "Epoch   6 Batch  574/575   train_loss = 4.704\n",
      "Epoch   7 Batch    0/575   train_loss = 4.411\n",
      "Epoch   7 Batch    1/575   train_loss = 4.389\n",
      "Epoch   7 Batch    2/575   train_loss = 4.547\n",
      "Epoch   7 Batch    3/575   train_loss = 4.341\n",
      "Epoch   7 Batch    4/575   train_loss = 4.251\n",
      "Epoch   7 Batch    5/575   train_loss = 4.230\n",
      "Epoch   7 Batch    6/575   train_loss = 4.109\n",
      "Epoch   7 Batch    7/575   train_loss = 4.423\n",
      "Epoch   7 Batch    8/575   train_loss = 4.390\n",
      "Epoch   7 Batch    9/575   train_loss = 4.635\n",
      "Epoch   7 Batch   10/575   train_loss = 4.125\n",
      "Epoch   7 Batch   11/575   train_loss = 4.219\n",
      "Epoch   7 Batch   12/575   train_loss = 4.382\n",
      "Epoch   7 Batch   13/575   train_loss = 4.553\n",
      "Epoch   7 Batch   14/575   train_loss = 4.990\n",
      "Epoch   7 Batch   15/575   train_loss = 4.662\n",
      "Epoch   7 Batch   16/575   train_loss = 4.525\n",
      "Epoch   7 Batch   17/575   train_loss = 3.898\n",
      "Epoch   7 Batch   18/575   train_loss = 4.868\n",
      "Epoch   7 Batch   19/575   train_loss = 4.534\n",
      "Epoch   7 Batch   20/575   train_loss = 3.925\n",
      "Epoch   7 Batch   21/575   train_loss = 4.415\n",
      "Epoch   7 Batch   22/575   train_loss = 4.951\n",
      "Epoch   7 Batch   23/575   train_loss = 4.507\n",
      "Epoch   7 Batch   24/575   train_loss = 4.425\n",
      "Epoch   7 Batch   25/575   train_loss = 4.575\n",
      "Epoch   7 Batch   26/575   train_loss = 4.803\n",
      "Epoch   7 Batch   27/575   train_loss = 4.506\n",
      "Epoch   7 Batch   28/575   train_loss = 4.043\n",
      "Epoch   7 Batch   29/575   train_loss = 4.244\n",
      "Epoch   7 Batch   30/575   train_loss = 4.974\n",
      "Epoch   7 Batch   31/575   train_loss = 5.238\n",
      "Epoch   7 Batch   32/575   train_loss = 4.579\n",
      "Epoch   7 Batch   33/575   train_loss = 4.847\n",
      "Epoch   7 Batch   34/575   train_loss = 4.482\n",
      "Epoch   7 Batch   35/575   train_loss = 4.263\n",
      "Epoch   7 Batch   36/575   train_loss = 4.802\n",
      "Epoch   7 Batch   37/575   train_loss = 3.885\n",
      "Epoch   7 Batch   38/575   train_loss = 4.628\n",
      "Epoch   7 Batch   39/575   train_loss = 3.959\n",
      "Epoch   7 Batch   40/575   train_loss = 3.805\n",
      "Epoch   7 Batch   41/575   train_loss = 5.659\n",
      "Epoch   7 Batch   42/575   train_loss = 4.609\n",
      "Epoch   7 Batch   43/575   train_loss = 4.505\n",
      "Epoch   7 Batch   44/575   train_loss = 4.397\n",
      "Epoch   7 Batch   45/575   train_loss = 4.283\n",
      "Epoch   7 Batch   46/575   train_loss = 4.292\n",
      "Epoch   7 Batch   47/575   train_loss = 4.746\n",
      "Epoch   7 Batch   48/575   train_loss = 3.895\n",
      "Epoch   7 Batch   49/575   train_loss = 4.142\n",
      "Epoch   7 Batch   50/575   train_loss = 4.382\n",
      "Epoch   7 Batch   51/575   train_loss = 4.271\n",
      "Epoch   7 Batch   52/575   train_loss = 4.270\n",
      "Epoch   7 Batch   53/575   train_loss = 3.920\n",
      "Epoch   7 Batch   54/575   train_loss = 4.415\n",
      "Epoch   7 Batch   55/575   train_loss = 4.720\n",
      "Epoch   7 Batch   56/575   train_loss = 4.124\n",
      "Epoch   7 Batch   57/575   train_loss = 4.550\n",
      "Epoch   7 Batch   58/575   train_loss = 4.109\n",
      "Epoch   7 Batch   59/575   train_loss = 3.980\n",
      "Epoch   7 Batch   60/575   train_loss = 4.536\n",
      "Epoch   7 Batch   61/575   train_loss = 4.579\n",
      "Epoch   7 Batch   62/575   train_loss = 4.023\n",
      "Epoch   7 Batch   63/575   train_loss = 4.803\n",
      "Epoch   7 Batch   64/575   train_loss = 4.742\n",
      "Epoch   7 Batch   65/575   train_loss = 4.284\n",
      "Epoch   7 Batch   66/575   train_loss = 4.250\n",
      "Epoch   7 Batch   67/575   train_loss = 4.169\n",
      "Epoch   7 Batch   68/575   train_loss = 4.183\n",
      "Epoch   7 Batch   69/575   train_loss = 3.969\n",
      "Epoch   7 Batch   70/575   train_loss = 4.386\n",
      "Epoch   7 Batch   71/575   train_loss = 4.158\n",
      "Epoch   7 Batch   72/575   train_loss = 4.714\n",
      "Epoch   7 Batch   73/575   train_loss = 4.737\n",
      "Epoch   7 Batch   74/575   train_loss = 4.863\n",
      "Epoch   7 Batch   75/575   train_loss = 4.496\n",
      "Epoch   7 Batch   76/575   train_loss = 4.784\n",
      "Epoch   7 Batch   77/575   train_loss = 4.918\n",
      "Epoch   7 Batch   78/575   train_loss = 4.625\n",
      "Epoch   7 Batch   79/575   train_loss = 4.396\n",
      "Epoch   7 Batch   80/575   train_loss = 4.386\n",
      "Epoch   7 Batch   81/575   train_loss = 4.295\n",
      "Epoch   7 Batch   82/575   train_loss = 4.083\n",
      "Epoch   7 Batch   83/575   train_loss = 4.379\n",
      "Epoch   7 Batch   84/575   train_loss = 4.362\n",
      "Epoch   7 Batch   85/575   train_loss = 4.965\n",
      "Epoch   7 Batch   86/575   train_loss = 4.320\n",
      "Epoch   7 Batch   87/575   train_loss = 4.907\n",
      "Epoch   7 Batch   88/575   train_loss = 3.786\n",
      "Epoch   7 Batch   89/575   train_loss = 4.020\n",
      "Epoch   7 Batch   90/575   train_loss = 4.188\n",
      "Epoch   7 Batch   91/575   train_loss = 4.431\n",
      "Epoch   7 Batch   92/575   train_loss = 4.959\n",
      "Epoch   7 Batch   93/575   train_loss = 4.585\n",
      "Epoch   7 Batch   94/575   train_loss = 4.111\n",
      "Epoch   7 Batch   95/575   train_loss = 4.468\n",
      "Epoch   7 Batch   96/575   train_loss = 4.490\n",
      "Epoch   7 Batch   97/575   train_loss = 4.578\n",
      "Epoch   7 Batch   98/575   train_loss = 4.357\n",
      "Epoch   7 Batch   99/575   train_loss = 4.911\n",
      "Epoch   7 Batch  100/575   train_loss = 4.642\n",
      "Epoch   7 Batch  101/575   train_loss = 4.251\n",
      "Epoch   7 Batch  102/575   train_loss = 4.102\n",
      "Epoch   7 Batch  103/575   train_loss = 4.581\n",
      "Epoch   7 Batch  104/575   train_loss = 4.446\n",
      "Epoch   7 Batch  105/575   train_loss = 4.601\n",
      "Epoch   7 Batch  106/575   train_loss = 4.242\n",
      "Epoch   7 Batch  107/575   train_loss = 4.160\n",
      "Epoch   7 Batch  108/575   train_loss = 4.578\n",
      "Epoch   7 Batch  109/575   train_loss = 4.321\n",
      "Epoch   7 Batch  110/575   train_loss = 3.454\n",
      "Epoch   7 Batch  111/575   train_loss = 4.414\n",
      "Epoch   7 Batch  112/575   train_loss = 4.470\n",
      "Epoch   7 Batch  113/575   train_loss = 5.024\n",
      "Epoch   7 Batch  114/575   train_loss = 4.722\n",
      "Epoch   7 Batch  115/575   train_loss = 4.241\n",
      "Epoch   7 Batch  116/575   train_loss = 4.660\n",
      "Epoch   7 Batch  117/575   train_loss = 4.068\n",
      "Epoch   7 Batch  118/575   train_loss = 4.470\n",
      "Epoch   7 Batch  119/575   train_loss = 4.975\n",
      "Epoch   7 Batch  120/575   train_loss = 4.559\n",
      "Epoch   7 Batch  121/575   train_loss = 4.606\n",
      "Epoch   7 Batch  122/575   train_loss = 4.649\n",
      "Epoch   7 Batch  123/575   train_loss = 4.377\n",
      "Epoch   7 Batch  124/575   train_loss = 4.795\n",
      "Epoch   7 Batch  125/575   train_loss = 4.571\n",
      "Epoch   7 Batch  126/575   train_loss = 4.445\n",
      "Epoch   7 Batch  127/575   train_loss = 3.974\n",
      "Epoch   7 Batch  128/575   train_loss = 4.475\n",
      "Epoch   7 Batch  129/575   train_loss = 4.408\n",
      "Epoch   7 Batch  130/575   train_loss = 4.692\n",
      "Epoch   7 Batch  131/575   train_loss = 3.944\n",
      "Epoch   7 Batch  132/575   train_loss = 4.305\n",
      "Epoch   7 Batch  133/575   train_loss = 4.475\n",
      "Epoch   7 Batch  134/575   train_loss = 4.358\n",
      "Epoch   7 Batch  135/575   train_loss = 4.016\n",
      "Epoch   7 Batch  136/575   train_loss = 5.297\n",
      "Epoch   7 Batch  137/575   train_loss = 4.647\n",
      "Epoch   7 Batch  138/575   train_loss = 4.964\n",
      "Epoch   7 Batch  139/575   train_loss = 4.576\n",
      "Epoch   7 Batch  140/575   train_loss = 4.563\n",
      "Epoch   7 Batch  141/575   train_loss = 4.897\n",
      "Epoch   7 Batch  142/575   train_loss = 5.017\n",
      "Epoch   7 Batch  143/575   train_loss = 4.632\n",
      "Epoch   7 Batch  144/575   train_loss = 4.219\n",
      "Epoch   7 Batch  145/575   train_loss = 4.178\n",
      "Epoch   7 Batch  146/575   train_loss = 4.186\n",
      "Epoch   7 Batch  147/575   train_loss = 4.357\n",
      "Epoch   7 Batch  148/575   train_loss = 3.879\n",
      "Epoch   7 Batch  149/575   train_loss = 4.550\n",
      "Epoch   7 Batch  150/575   train_loss = 4.795\n",
      "Epoch   7 Batch  151/575   train_loss = 4.106\n",
      "Epoch   7 Batch  152/575   train_loss = 4.062\n",
      "Epoch   7 Batch  153/575   train_loss = 3.615\n",
      "Epoch   7 Batch  154/575   train_loss = 3.798\n",
      "Epoch   7 Batch  155/575   train_loss = 4.515\n",
      "Epoch   7 Batch  156/575   train_loss = 4.394\n",
      "Epoch   7 Batch  157/575   train_loss = 4.484\n",
      "Epoch   7 Batch  158/575   train_loss = 4.468\n",
      "Epoch   7 Batch  159/575   train_loss = 4.597\n",
      "Epoch   7 Batch  160/575   train_loss = 4.209\n",
      "Epoch   7 Batch  161/575   train_loss = 4.759\n",
      "Epoch   7 Batch  162/575   train_loss = 4.603\n",
      "Epoch   7 Batch  163/575   train_loss = 4.391\n",
      "Epoch   7 Batch  164/575   train_loss = 4.699\n",
      "Epoch   7 Batch  165/575   train_loss = 4.524\n",
      "Epoch   7 Batch  166/575   train_loss = 4.162\n",
      "Epoch   7 Batch  167/575   train_loss = 4.184\n",
      "Epoch   7 Batch  168/575   train_loss = 4.588\n",
      "Epoch   7 Batch  169/575   train_loss = 4.229\n",
      "Epoch   7 Batch  170/575   train_loss = 4.661\n",
      "Epoch   7 Batch  171/575   train_loss = 4.295\n",
      "Epoch   7 Batch  172/575   train_loss = 4.505\n",
      "Epoch   7 Batch  173/575   train_loss = 4.468\n",
      "Epoch   7 Batch  174/575   train_loss = 4.714\n",
      "Epoch   7 Batch  175/575   train_loss = 4.478\n",
      "Epoch   7 Batch  176/575   train_loss = 4.569\n",
      "Epoch   7 Batch  177/575   train_loss = 4.569\n",
      "Epoch   7 Batch  178/575   train_loss = 4.419\n",
      "Epoch   7 Batch  179/575   train_loss = 4.160\n",
      "Epoch   7 Batch  180/575   train_loss = 4.255\n",
      "Epoch   7 Batch  181/575   train_loss = 4.460\n",
      "Epoch   7 Batch  182/575   train_loss = 4.286\n",
      "Epoch   7 Batch  183/575   train_loss = 4.646\n",
      "Epoch   7 Batch  184/575   train_loss = 3.812\n",
      "Epoch   7 Batch  185/575   train_loss = 4.368\n",
      "Epoch   7 Batch  186/575   train_loss = 4.518\n",
      "Epoch   7 Batch  187/575   train_loss = 4.402\n",
      "Epoch   7 Batch  188/575   train_loss = 4.359\n",
      "Epoch   7 Batch  189/575   train_loss = 4.351\n",
      "Epoch   7 Batch  190/575   train_loss = 4.395\n",
      "Epoch   7 Batch  191/575   train_loss = 4.880\n",
      "Epoch   7 Batch  192/575   train_loss = 4.692\n",
      "Epoch   7 Batch  193/575   train_loss = 4.427\n",
      "Epoch   7 Batch  194/575   train_loss = 4.554\n",
      "Epoch   7 Batch  195/575   train_loss = 4.164\n",
      "Epoch   7 Batch  196/575   train_loss = 4.256\n",
      "Epoch   7 Batch  197/575   train_loss = 4.198\n",
      "Epoch   7 Batch  198/575   train_loss = 4.696\n",
      "Epoch   7 Batch  199/575   train_loss = 3.926\n",
      "Epoch   7 Batch  200/575   train_loss = 4.192\n",
      "Epoch   7 Batch  201/575   train_loss = 4.443\n",
      "Epoch   7 Batch  202/575   train_loss = 4.125\n",
      "Epoch   7 Batch  203/575   train_loss = 4.321\n",
      "Epoch   7 Batch  204/575   train_loss = 4.266\n",
      "Epoch   7 Batch  205/575   train_loss = 4.051\n",
      "Epoch   7 Batch  206/575   train_loss = 4.343\n",
      "Epoch   7 Batch  207/575   train_loss = 4.197\n",
      "Epoch   7 Batch  208/575   train_loss = 4.064\n",
      "Epoch   7 Batch  209/575   train_loss = 4.133\n",
      "Epoch   7 Batch  210/575   train_loss = 4.512\n",
      "Epoch   7 Batch  211/575   train_loss = 4.990\n",
      "Epoch   7 Batch  212/575   train_loss = 4.645\n",
      "Epoch   7 Batch  213/575   train_loss = 4.468\n",
      "Epoch   7 Batch  214/575   train_loss = 4.162\n",
      "Epoch   7 Batch  215/575   train_loss = 3.721\n",
      "Epoch   7 Batch  216/575   train_loss = 4.122\n",
      "Epoch   7 Batch  217/575   train_loss = 3.886\n",
      "Epoch   7 Batch  218/575   train_loss = 4.561\n",
      "Epoch   7 Batch  219/575   train_loss = 4.184\n",
      "Epoch   7 Batch  220/575   train_loss = 3.833\n",
      "Epoch   7 Batch  221/575   train_loss = 4.333\n",
      "Epoch   7 Batch  222/575   train_loss = 4.443\n",
      "Epoch   7 Batch  223/575   train_loss = 4.704\n",
      "Epoch   7 Batch  224/575   train_loss = 4.025\n",
      "Epoch   7 Batch  225/575   train_loss = 4.157\n",
      "Epoch   7 Batch  226/575   train_loss = 4.522\n",
      "Epoch   7 Batch  227/575   train_loss = 4.813\n",
      "Epoch   7 Batch  228/575   train_loss = 4.625\n",
      "Epoch   7 Batch  229/575   train_loss = 3.722\n",
      "Epoch   7 Batch  230/575   train_loss = 3.888\n",
      "Epoch   7 Batch  231/575   train_loss = 4.327\n",
      "Epoch   7 Batch  232/575   train_loss = 4.491\n",
      "Epoch   7 Batch  233/575   train_loss = 4.424\n",
      "Epoch   7 Batch  234/575   train_loss = 4.554\n",
      "Epoch   7 Batch  235/575   train_loss = 4.246\n",
      "Epoch   7 Batch  236/575   train_loss = 3.808\n",
      "Epoch   7 Batch  237/575   train_loss = 4.108\n",
      "Epoch   7 Batch  238/575   train_loss = 4.121\n",
      "Epoch   7 Batch  239/575   train_loss = 4.432\n",
      "Epoch   7 Batch  240/575   train_loss = 4.649\n",
      "Epoch   7 Batch  241/575   train_loss = 4.722\n",
      "Epoch   7 Batch  242/575   train_loss = 4.264\n",
      "Epoch   7 Batch  243/575   train_loss = 4.403\n",
      "Epoch   7 Batch  244/575   train_loss = 4.295\n",
      "Epoch   7 Batch  245/575   train_loss = 4.517\n",
      "Epoch   7 Batch  246/575   train_loss = 4.715\n",
      "Epoch   7 Batch  247/575   train_loss = 4.330\n",
      "Epoch   7 Batch  248/575   train_loss = 4.522\n",
      "Epoch   7 Batch  249/575   train_loss = 4.392\n",
      "Epoch   7 Batch  250/575   train_loss = 4.284\n",
      "Epoch   7 Batch  251/575   train_loss = 4.434\n",
      "Epoch   7 Batch  252/575   train_loss = 4.747\n",
      "Epoch   7 Batch  253/575   train_loss = 4.377\n",
      "Epoch   7 Batch  254/575   train_loss = 4.001\n",
      "Epoch   7 Batch  255/575   train_loss = 4.614\n",
      "Epoch   7 Batch  256/575   train_loss = 4.273\n",
      "Epoch   7 Batch  257/575   train_loss = 4.420\n",
      "Epoch   7 Batch  258/575   train_loss = 4.788\n",
      "Epoch   7 Batch  259/575   train_loss = 3.952\n",
      "Epoch   7 Batch  260/575   train_loss = 4.176\n",
      "Epoch   7 Batch  261/575   train_loss = 4.875\n",
      "Epoch   7 Batch  262/575   train_loss = 4.610\n",
      "Epoch   7 Batch  263/575   train_loss = 4.567\n",
      "Epoch   7 Batch  264/575   train_loss = 3.772\n",
      "Epoch   7 Batch  265/575   train_loss = 3.752\n",
      "Epoch   7 Batch  266/575   train_loss = 3.876\n",
      "Epoch   7 Batch  267/575   train_loss = 4.343\n",
      "Epoch   7 Batch  268/575   train_loss = 4.346\n",
      "Epoch   7 Batch  269/575   train_loss = 4.530\n",
      "Epoch   7 Batch  270/575   train_loss = 4.918\n",
      "Epoch   7 Batch  271/575   train_loss = 4.082\n",
      "Epoch   7 Batch  272/575   train_loss = 4.580\n",
      "Epoch   7 Batch  273/575   train_loss = 4.593\n",
      "Epoch   7 Batch  274/575   train_loss = 4.565\n",
      "Epoch   7 Batch  275/575   train_loss = 4.388\n",
      "Epoch   7 Batch  276/575   train_loss = 4.948\n",
      "Epoch   7 Batch  277/575   train_loss = 3.713\n",
      "Epoch   7 Batch  278/575   train_loss = 4.484\n",
      "Epoch   7 Batch  279/575   train_loss = 4.630\n",
      "Epoch   7 Batch  280/575   train_loss = 4.651\n",
      "Epoch   7 Batch  281/575   train_loss = 4.205\n",
      "Epoch   7 Batch  282/575   train_loss = 4.515\n",
      "Epoch   7 Batch  283/575   train_loss = 4.730\n",
      "Epoch   7 Batch  284/575   train_loss = 4.888\n",
      "Epoch   7 Batch  285/575   train_loss = 4.153\n",
      "Epoch   7 Batch  286/575   train_loss = 4.455\n",
      "Epoch   7 Batch  287/575   train_loss = 4.590\n",
      "Epoch   7 Batch  288/575   train_loss = 4.347\n",
      "Epoch   7 Batch  289/575   train_loss = 4.395\n",
      "Epoch   7 Batch  290/575   train_loss = 4.440\n",
      "Epoch   7 Batch  291/575   train_loss = 5.020\n",
      "Epoch   7 Batch  292/575   train_loss = 4.933\n",
      "Epoch   7 Batch  293/575   train_loss = 5.116\n",
      "Epoch   7 Batch  294/575   train_loss = 4.742\n",
      "Epoch   7 Batch  295/575   train_loss = 4.347\n",
      "Epoch   7 Batch  296/575   train_loss = 4.521\n",
      "Epoch   7 Batch  297/575   train_loss = 4.492\n",
      "Epoch   7 Batch  298/575   train_loss = 4.644\n",
      "Epoch   7 Batch  299/575   train_loss = 4.557\n",
      "Epoch   7 Batch  300/575   train_loss = 4.455\n",
      "Epoch   7 Batch  301/575   train_loss = 4.298\n",
      "Epoch   7 Batch  302/575   train_loss = 4.682\n",
      "Epoch   7 Batch  303/575   train_loss = 4.195\n",
      "Epoch   7 Batch  304/575   train_loss = 4.298\n",
      "Epoch   7 Batch  305/575   train_loss = 4.288\n",
      "Epoch   7 Batch  306/575   train_loss = 4.543\n",
      "Epoch   7 Batch  307/575   train_loss = 4.601\n",
      "Epoch   7 Batch  308/575   train_loss = 4.476\n",
      "Epoch   7 Batch  309/575   train_loss = 4.750\n",
      "Epoch   7 Batch  310/575   train_loss = 4.807\n",
      "Epoch   7 Batch  311/575   train_loss = 4.161\n",
      "Epoch   7 Batch  312/575   train_loss = 4.508\n",
      "Epoch   7 Batch  313/575   train_loss = 4.829\n",
      "Epoch   7 Batch  314/575   train_loss = 4.802\n",
      "Epoch   7 Batch  315/575   train_loss = 4.957\n",
      "Epoch   7 Batch  316/575   train_loss = 4.367\n",
      "Epoch   7 Batch  317/575   train_loss = 4.022\n",
      "Epoch   7 Batch  318/575   train_loss = 3.749\n",
      "Epoch   7 Batch  319/575   train_loss = 4.465\n",
      "Epoch   7 Batch  320/575   train_loss = 4.315\n",
      "Epoch   7 Batch  321/575   train_loss = 4.733\n",
      "Epoch   7 Batch  322/575   train_loss = 4.379\n",
      "Epoch   7 Batch  323/575   train_loss = 4.905\n",
      "Epoch   7 Batch  324/575   train_loss = 4.353\n",
      "Epoch   7 Batch  325/575   train_loss = 4.515\n",
      "Epoch   7 Batch  326/575   train_loss = 4.272\n",
      "Epoch   7 Batch  327/575   train_loss = 4.652\n",
      "Epoch   7 Batch  328/575   train_loss = 4.255\n",
      "Epoch   7 Batch  329/575   train_loss = 4.900\n",
      "Epoch   7 Batch  330/575   train_loss = 4.509\n",
      "Epoch   7 Batch  331/575   train_loss = 4.970\n",
      "Epoch   7 Batch  332/575   train_loss = 4.568\n",
      "Epoch   7 Batch  333/575   train_loss = 4.536\n",
      "Epoch   7 Batch  334/575   train_loss = 4.433\n",
      "Epoch   7 Batch  335/575   train_loss = 4.162\n",
      "Epoch   7 Batch  336/575   train_loss = 4.547\n",
      "Epoch   7 Batch  337/575   train_loss = 4.372\n",
      "Epoch   7 Batch  338/575   train_loss = 4.200\n",
      "Epoch   7 Batch  339/575   train_loss = 4.519\n",
      "Epoch   7 Batch  340/575   train_loss = 4.325\n",
      "Epoch   7 Batch  341/575   train_loss = 4.339\n",
      "Epoch   7 Batch  342/575   train_loss = 4.187\n",
      "Epoch   7 Batch  343/575   train_loss = 4.671\n",
      "Epoch   7 Batch  344/575   train_loss = 4.708\n",
      "Epoch   7 Batch  345/575   train_loss = 4.301\n",
      "Epoch   7 Batch  346/575   train_loss = 4.969\n",
      "Epoch   7 Batch  347/575   train_loss = 4.170\n",
      "Epoch   7 Batch  348/575   train_loss = 4.199\n",
      "Epoch   7 Batch  349/575   train_loss = 3.984\n",
      "Epoch   7 Batch  350/575   train_loss = 4.552\n",
      "Epoch   7 Batch  351/575   train_loss = 4.903\n",
      "Epoch   7 Batch  352/575   train_loss = 4.761\n",
      "Epoch   7 Batch  353/575   train_loss = 4.608\n",
      "Epoch   7 Batch  354/575   train_loss = 4.218\n",
      "Epoch   7 Batch  355/575   train_loss = 4.840\n",
      "Epoch   7 Batch  356/575   train_loss = 4.639\n",
      "Epoch   7 Batch  357/575   train_loss = 4.641\n",
      "Epoch   7 Batch  358/575   train_loss = 4.746\n",
      "Epoch   7 Batch  359/575   train_loss = 4.927\n",
      "Epoch   7 Batch  360/575   train_loss = 4.059\n",
      "Epoch   7 Batch  361/575   train_loss = 4.603\n",
      "Epoch   7 Batch  362/575   train_loss = 4.394\n",
      "Epoch   7 Batch  363/575   train_loss = 4.507\n",
      "Epoch   7 Batch  364/575   train_loss = 4.237\n",
      "Epoch   7 Batch  365/575   train_loss = 4.581\n",
      "Epoch   7 Batch  366/575   train_loss = 4.019\n",
      "Epoch   7 Batch  367/575   train_loss = 4.290\n",
      "Epoch   7 Batch  368/575   train_loss = 5.068\n",
      "Epoch   7 Batch  369/575   train_loss = 4.772\n",
      "Epoch   7 Batch  370/575   train_loss = 4.625\n",
      "Epoch   7 Batch  371/575   train_loss = 4.639\n",
      "Epoch   7 Batch  372/575   train_loss = 4.276\n",
      "Epoch   7 Batch  373/575   train_loss = 4.720\n",
      "Epoch   7 Batch  374/575   train_loss = 3.880\n",
      "Epoch   7 Batch  375/575   train_loss = 4.265\n",
      "Epoch   7 Batch  376/575   train_loss = 4.581\n",
      "Epoch   7 Batch  377/575   train_loss = 4.882\n",
      "Epoch   7 Batch  378/575   train_loss = 4.996\n",
      "Epoch   7 Batch  379/575   train_loss = 4.385\n",
      "Epoch   7 Batch  380/575   train_loss = 4.060\n",
      "Epoch   7 Batch  381/575   train_loss = 4.246\n",
      "Epoch   7 Batch  382/575   train_loss = 4.519\n",
      "Epoch   7 Batch  383/575   train_loss = 4.324\n",
      "Epoch   7 Batch  384/575   train_loss = 4.488\n",
      "Epoch   7 Batch  385/575   train_loss = 4.367\n",
      "Epoch   7 Batch  386/575   train_loss = 4.273\n",
      "Epoch   7 Batch  387/575   train_loss = 4.889\n",
      "Epoch   7 Batch  388/575   train_loss = 4.476\n",
      "Epoch   7 Batch  389/575   train_loss = 4.381\n",
      "Epoch   7 Batch  390/575   train_loss = 4.609\n",
      "Epoch   7 Batch  391/575   train_loss = 4.626\n",
      "Epoch   7 Batch  392/575   train_loss = 4.578\n",
      "Epoch   7 Batch  393/575   train_loss = 4.572\n",
      "Epoch   7 Batch  394/575   train_loss = 4.759\n",
      "Epoch   7 Batch  395/575   train_loss = 3.898\n",
      "Epoch   7 Batch  396/575   train_loss = 4.141\n",
      "Epoch   7 Batch  397/575   train_loss = 4.542\n",
      "Epoch   7 Batch  398/575   train_loss = 4.995\n",
      "Epoch   7 Batch  399/575   train_loss = 4.893\n",
      "Epoch   7 Batch  400/575   train_loss = 4.718\n",
      "Epoch   7 Batch  401/575   train_loss = 4.150\n",
      "Epoch   7 Batch  402/575   train_loss = 4.231\n",
      "Epoch   7 Batch  403/575   train_loss = 4.649\n",
      "Epoch   7 Batch  404/575   train_loss = 4.579\n",
      "Epoch   7 Batch  405/575   train_loss = 4.662\n",
      "Epoch   7 Batch  406/575   train_loss = 4.194\n",
      "Epoch   7 Batch  407/575   train_loss = 4.567\n",
      "Epoch   7 Batch  408/575   train_loss = 4.377\n",
      "Epoch   7 Batch  409/575   train_loss = 4.475\n",
      "Epoch   7 Batch  410/575   train_loss = 4.216\n",
      "Epoch   7 Batch  411/575   train_loss = 4.610\n",
      "Epoch   7 Batch  412/575   train_loss = 5.055\n",
      "Epoch   7 Batch  413/575   train_loss = 4.647\n",
      "Epoch   7 Batch  414/575   train_loss = 4.305\n",
      "Epoch   7 Batch  415/575   train_loss = 4.778\n",
      "Epoch   7 Batch  416/575   train_loss = 4.372\n",
      "Epoch   7 Batch  417/575   train_loss = 4.108\n",
      "Epoch   7 Batch  418/575   train_loss = 4.542\n",
      "Epoch   7 Batch  419/575   train_loss = 4.386\n",
      "Epoch   7 Batch  420/575   train_loss = 4.260\n",
      "Epoch   7 Batch  421/575   train_loss = 4.891\n",
      "Epoch   7 Batch  422/575   train_loss = 4.663\n",
      "Epoch   7 Batch  423/575   train_loss = 4.957\n",
      "Epoch   7 Batch  424/575   train_loss = 4.956\n",
      "Epoch   7 Batch  425/575   train_loss = 4.596\n",
      "Epoch   7 Batch  426/575   train_loss = 4.586\n",
      "Epoch   7 Batch  427/575   train_loss = 4.515\n",
      "Epoch   7 Batch  428/575   train_loss = 4.327\n",
      "Epoch   7 Batch  429/575   train_loss = 4.280\n",
      "Epoch   7 Batch  430/575   train_loss = 4.568\n",
      "Epoch   7 Batch  431/575   train_loss = 4.436\n",
      "Epoch   7 Batch  432/575   train_loss = 4.801\n",
      "Epoch   7 Batch  433/575   train_loss = 4.757\n",
      "Epoch   7 Batch  434/575   train_loss = 4.644\n",
      "Epoch   7 Batch  435/575   train_loss = 4.402\n",
      "Epoch   7 Batch  436/575   train_loss = 4.295\n",
      "Epoch   7 Batch  437/575   train_loss = 4.406\n",
      "Epoch   7 Batch  438/575   train_loss = 4.652\n",
      "Epoch   7 Batch  439/575   train_loss = 4.583\n",
      "Epoch   7 Batch  440/575   train_loss = 4.808\n",
      "Epoch   7 Batch  441/575   train_loss = 4.407\n",
      "Epoch   7 Batch  442/575   train_loss = 4.720\n",
      "Epoch   7 Batch  443/575   train_loss = 4.575\n",
      "Epoch   7 Batch  444/575   train_loss = 4.650\n",
      "Epoch   7 Batch  445/575   train_loss = 4.180\n",
      "Epoch   7 Batch  446/575   train_loss = 4.190\n",
      "Epoch   7 Batch  447/575   train_loss = 4.675\n",
      "Epoch   7 Batch  448/575   train_loss = 4.603\n",
      "Epoch   7 Batch  449/575   train_loss = 4.487\n",
      "Epoch   7 Batch  450/575   train_loss = 4.945\n",
      "Epoch   7 Batch  451/575   train_loss = 4.952\n",
      "Epoch   7 Batch  452/575   train_loss = 4.978\n",
      "Epoch   7 Batch  453/575   train_loss = 4.575\n",
      "Epoch   7 Batch  454/575   train_loss = 4.951\n",
      "Epoch   7 Batch  455/575   train_loss = 4.239\n",
      "Epoch   7 Batch  456/575   train_loss = 4.803\n",
      "Epoch   7 Batch  457/575   train_loss = 4.114\n",
      "Epoch   7 Batch  458/575   train_loss = 4.860\n",
      "Epoch   7 Batch  459/575   train_loss = 4.540\n",
      "Epoch   7 Batch  460/575   train_loss = 4.303\n",
      "Epoch   7 Batch  461/575   train_loss = 4.480\n",
      "Epoch   7 Batch  462/575   train_loss = 4.444\n",
      "Epoch   7 Batch  463/575   train_loss = 4.304\n",
      "Epoch   7 Batch  464/575   train_loss = 4.285\n",
      "Epoch   7 Batch  465/575   train_loss = 4.433\n",
      "Epoch   7 Batch  466/575   train_loss = 4.390\n",
      "Epoch   7 Batch  467/575   train_loss = 3.813\n",
      "Epoch   7 Batch  468/575   train_loss = 4.387\n",
      "Epoch   7 Batch  469/575   train_loss = 4.825\n",
      "Epoch   7 Batch  470/575   train_loss = 4.964\n",
      "Epoch   7 Batch  471/575   train_loss = 4.297\n",
      "Epoch   7 Batch  472/575   train_loss = 4.758\n",
      "Epoch   7 Batch  473/575   train_loss = 4.600\n",
      "Epoch   7 Batch  474/575   train_loss = 4.625\n",
      "Epoch   7 Batch  475/575   train_loss = 4.292\n",
      "Epoch   7 Batch  476/575   train_loss = 4.435\n",
      "Epoch   7 Batch  477/575   train_loss = 5.014\n",
      "Epoch   7 Batch  478/575   train_loss = 5.044\n",
      "Epoch   7 Batch  479/575   train_loss = 4.335\n",
      "Epoch   7 Batch  480/575   train_loss = 4.586\n",
      "Epoch   7 Batch  481/575   train_loss = 4.714\n",
      "Epoch   7 Batch  482/575   train_loss = 4.114\n",
      "Epoch   7 Batch  483/575   train_loss = 5.202\n",
      "Epoch   7 Batch  484/575   train_loss = 4.407\n",
      "Epoch   7 Batch  485/575   train_loss = 5.057\n",
      "Epoch   7 Batch  486/575   train_loss = 4.527\n",
      "Epoch   7 Batch  487/575   train_loss = 4.835\n",
      "Epoch   7 Batch  488/575   train_loss = 4.544\n",
      "Epoch   7 Batch  489/575   train_loss = 4.701\n",
      "Epoch   7 Batch  490/575   train_loss = 4.693\n",
      "Epoch   7 Batch  491/575   train_loss = 4.065\n",
      "Epoch   7 Batch  492/575   train_loss = 4.061\n",
      "Epoch   7 Batch  493/575   train_loss = 4.115\n",
      "Epoch   7 Batch  494/575   train_loss = 4.438\n",
      "Epoch   7 Batch  495/575   train_loss = 4.440\n",
      "Epoch   7 Batch  496/575   train_loss = 4.580\n",
      "Epoch   7 Batch  497/575   train_loss = 4.696\n",
      "Epoch   7 Batch  498/575   train_loss = 4.325\n",
      "Epoch   7 Batch  499/575   train_loss = 4.283\n",
      "Epoch   7 Batch  500/575   train_loss = 4.635\n",
      "Epoch   7 Batch  501/575   train_loss = 4.607\n",
      "Epoch   7 Batch  502/575   train_loss = 3.941\n",
      "Epoch   7 Batch  503/575   train_loss = 4.849\n",
      "Epoch   7 Batch  504/575   train_loss = 3.967\n",
      "Epoch   7 Batch  505/575   train_loss = 4.449\n",
      "Epoch   7 Batch  506/575   train_loss = 4.683\n",
      "Epoch   7 Batch  507/575   train_loss = 4.811\n",
      "Epoch   7 Batch  508/575   train_loss = 4.831\n",
      "Epoch   7 Batch  509/575   train_loss = 4.543\n",
      "Epoch   7 Batch  510/575   train_loss = 3.929\n",
      "Epoch   7 Batch  511/575   train_loss = 4.045\n",
      "Epoch   7 Batch  512/575   train_loss = 4.841\n",
      "Epoch   7 Batch  513/575   train_loss = 4.548\n",
      "Epoch   7 Batch  514/575   train_loss = 4.457\n",
      "Epoch   7 Batch  515/575   train_loss = 4.497\n",
      "Epoch   7 Batch  516/575   train_loss = 4.821\n",
      "Epoch   7 Batch  517/575   train_loss = 4.031\n",
      "Epoch   7 Batch  518/575   train_loss = 4.226\n",
      "Epoch   7 Batch  519/575   train_loss = 4.417\n",
      "Epoch   7 Batch  520/575   train_loss = 3.860\n",
      "Epoch   7 Batch  521/575   train_loss = 4.175\n",
      "Epoch   7 Batch  522/575   train_loss = 4.300\n",
      "Epoch   7 Batch  523/575   train_loss = 4.764\n",
      "Epoch   7 Batch  524/575   train_loss = 5.359\n",
      "Epoch   7 Batch  525/575   train_loss = 4.716\n",
      "Epoch   7 Batch  526/575   train_loss = 4.201\n",
      "Epoch   7 Batch  527/575   train_loss = 4.768\n",
      "Epoch   7 Batch  528/575   train_loss = 4.279\n",
      "Epoch   7 Batch  529/575   train_loss = 4.802\n",
      "Epoch   7 Batch  530/575   train_loss = 4.575\n",
      "Epoch   7 Batch  531/575   train_loss = 4.682\n",
      "Epoch   7 Batch  532/575   train_loss = 4.275\n",
      "Epoch   7 Batch  533/575   train_loss = 4.196\n",
      "Epoch   7 Batch  534/575   train_loss = 4.254\n",
      "Epoch   7 Batch  535/575   train_loss = 4.526\n",
      "Epoch   7 Batch  536/575   train_loss = 4.326\n",
      "Epoch   7 Batch  537/575   train_loss = 4.181\n",
      "Epoch   7 Batch  538/575   train_loss = 4.422\n",
      "Epoch   7 Batch  539/575   train_loss = 4.299\n",
      "Epoch   7 Batch  540/575   train_loss = 4.564\n",
      "Epoch   7 Batch  541/575   train_loss = 4.975\n",
      "Epoch   7 Batch  542/575   train_loss = 4.499\n",
      "Epoch   7 Batch  543/575   train_loss = 4.642\n",
      "Epoch   7 Batch  544/575   train_loss = 4.436\n",
      "Epoch   7 Batch  545/575   train_loss = 4.454\n",
      "Epoch   7 Batch  546/575   train_loss = 4.477\n",
      "Epoch   7 Batch  547/575   train_loss = 4.496\n",
      "Epoch   7 Batch  548/575   train_loss = 4.604\n",
      "Epoch   7 Batch  549/575   train_loss = 4.399\n",
      "Epoch   7 Batch  550/575   train_loss = 4.332\n",
      "Epoch   7 Batch  551/575   train_loss = 4.229\n",
      "Epoch   7 Batch  552/575   train_loss = 4.482\n",
      "Epoch   7 Batch  553/575   train_loss = 4.754\n",
      "Epoch   7 Batch  554/575   train_loss = 4.153\n",
      "Epoch   7 Batch  555/575   train_loss = 4.389\n",
      "Epoch   7 Batch  556/575   train_loss = 4.200\n",
      "Epoch   7 Batch  557/575   train_loss = 4.182\n",
      "Epoch   7 Batch  558/575   train_loss = 4.474\n",
      "Epoch   7 Batch  559/575   train_loss = 4.642\n",
      "Epoch   7 Batch  560/575   train_loss = 4.072\n",
      "Epoch   7 Batch  561/575   train_loss = 4.437\n",
      "Epoch   7 Batch  562/575   train_loss = 3.982\n",
      "Epoch   7 Batch  563/575   train_loss = 4.375\n",
      "Epoch   7 Batch  564/575   train_loss = 4.106\n",
      "Epoch   7 Batch  565/575   train_loss = 3.640\n",
      "Epoch   7 Batch  566/575   train_loss = 4.124\n",
      "Epoch   7 Batch  567/575   train_loss = 4.219\n",
      "Epoch   7 Batch  568/575   train_loss = 4.283\n",
      "Epoch   7 Batch  569/575   train_loss = 4.516\n",
      "Epoch   7 Batch  570/575   train_loss = 4.097\n",
      "Epoch   7 Batch  571/575   train_loss = 3.898\n",
      "Epoch   7 Batch  572/575   train_loss = 3.673\n",
      "Epoch   7 Batch  573/575   train_loss = 4.045\n",
      "Epoch   7 Batch  574/575   train_loss = 4.585\n",
      "Epoch   8 Batch    0/575   train_loss = 4.297\n",
      "Epoch   8 Batch    1/575   train_loss = 4.296\n",
      "Epoch   8 Batch    2/575   train_loss = 4.447\n",
      "Epoch   8 Batch    3/575   train_loss = 4.240\n",
      "Epoch   8 Batch    4/575   train_loss = 4.181\n",
      "Epoch   8 Batch    5/575   train_loss = 4.168\n",
      "Epoch   8 Batch    6/575   train_loss = 4.010\n",
      "Epoch   8 Batch    7/575   train_loss = 4.328\n",
      "Epoch   8 Batch    8/575   train_loss = 4.250\n",
      "Epoch   8 Batch    9/575   train_loss = 4.534\n",
      "Epoch   8 Batch   10/575   train_loss = 4.040\n",
      "Epoch   8 Batch   11/575   train_loss = 4.099\n",
      "Epoch   8 Batch   12/575   train_loss = 4.305\n",
      "Epoch   8 Batch   13/575   train_loss = 4.435\n",
      "Epoch   8 Batch   14/575   train_loss = 4.819\n",
      "Epoch   8 Batch   15/575   train_loss = 4.521\n",
      "Epoch   8 Batch   16/575   train_loss = 4.389\n",
      "Epoch   8 Batch   17/575   train_loss = 3.788\n",
      "Epoch   8 Batch   18/575   train_loss = 4.730\n",
      "Epoch   8 Batch   19/575   train_loss = 4.430\n",
      "Epoch   8 Batch   20/575   train_loss = 3.800\n",
      "Epoch   8 Batch   21/575   train_loss = 4.319\n",
      "Epoch   8 Batch   22/575   train_loss = 4.818\n",
      "Epoch   8 Batch   23/575   train_loss = 4.384\n",
      "Epoch   8 Batch   24/575   train_loss = 4.302\n",
      "Epoch   8 Batch   25/575   train_loss = 4.442\n",
      "Epoch   8 Batch   26/575   train_loss = 4.683\n",
      "Epoch   8 Batch   27/575   train_loss = 4.368\n",
      "Epoch   8 Batch   28/575   train_loss = 3.927\n",
      "Epoch   8 Batch   29/575   train_loss = 4.121\n",
      "Epoch   8 Batch   30/575   train_loss = 4.808\n",
      "Epoch   8 Batch   31/575   train_loss = 5.094\n",
      "Epoch   8 Batch   32/575   train_loss = 4.441\n",
      "Epoch   8 Batch   33/575   train_loss = 4.724\n",
      "Epoch   8 Batch   34/575   train_loss = 4.393\n",
      "Epoch   8 Batch   35/575   train_loss = 4.109\n",
      "Epoch   8 Batch   36/575   train_loss = 4.679\n",
      "Epoch   8 Batch   37/575   train_loss = 3.796\n",
      "Epoch   8 Batch   38/575   train_loss = 4.518\n",
      "Epoch   8 Batch   39/575   train_loss = 3.855\n",
      "Epoch   8 Batch   40/575   train_loss = 3.742\n",
      "Epoch   8 Batch   41/575   train_loss = 5.447\n",
      "Epoch   8 Batch   42/575   train_loss = 4.499\n",
      "Epoch   8 Batch   43/575   train_loss = 4.390\n",
      "Epoch   8 Batch   44/575   train_loss = 4.259\n",
      "Epoch   8 Batch   45/575   train_loss = 4.175\n",
      "Epoch   8 Batch   46/575   train_loss = 4.156\n",
      "Epoch   8 Batch   47/575   train_loss = 4.670\n",
      "Epoch   8 Batch   48/575   train_loss = 3.806\n",
      "Epoch   8 Batch   49/575   train_loss = 4.057\n",
      "Epoch   8 Batch   50/575   train_loss = 4.285\n",
      "Epoch   8 Batch   51/575   train_loss = 4.174\n",
      "Epoch   8 Batch   52/575   train_loss = 4.168\n",
      "Epoch   8 Batch   53/575   train_loss = 3.818\n",
      "Epoch   8 Batch   54/575   train_loss = 4.283\n",
      "Epoch   8 Batch   55/575   train_loss = 4.609\n",
      "Epoch   8 Batch   56/575   train_loss = 4.001\n",
      "Epoch   8 Batch   57/575   train_loss = 4.468\n",
      "Epoch   8 Batch   58/575   train_loss = 3.983\n",
      "Epoch   8 Batch   59/575   train_loss = 3.880\n",
      "Epoch   8 Batch   60/575   train_loss = 4.380\n",
      "Epoch   8 Batch   61/575   train_loss = 4.477\n",
      "Epoch   8 Batch   62/575   train_loss = 3.896\n",
      "Epoch   8 Batch   63/575   train_loss = 4.661\n",
      "Epoch   8 Batch   64/575   train_loss = 4.615\n",
      "Epoch   8 Batch   65/575   train_loss = 4.194\n",
      "Epoch   8 Batch   66/575   train_loss = 4.149\n",
      "Epoch   8 Batch   67/575   train_loss = 4.085\n",
      "Epoch   8 Batch   68/575   train_loss = 4.081\n",
      "Epoch   8 Batch   69/575   train_loss = 3.871\n",
      "Epoch   8 Batch   70/575   train_loss = 4.270\n",
      "Epoch   8 Batch   71/575   train_loss = 4.046\n",
      "Epoch   8 Batch   72/575   train_loss = 4.589\n",
      "Epoch   8 Batch   73/575   train_loss = 4.631\n",
      "Epoch   8 Batch   74/575   train_loss = 4.713\n",
      "Epoch   8 Batch   75/575   train_loss = 4.411\n",
      "Epoch   8 Batch   76/575   train_loss = 4.629\n",
      "Epoch   8 Batch   77/575   train_loss = 4.813\n",
      "Epoch   8 Batch   78/575   train_loss = 4.505\n",
      "Epoch   8 Batch   79/575   train_loss = 4.295\n",
      "Epoch   8 Batch   80/575   train_loss = 4.290\n",
      "Epoch   8 Batch   81/575   train_loss = 4.158\n",
      "Epoch   8 Batch   82/575   train_loss = 3.986\n",
      "Epoch   8 Batch   83/575   train_loss = 4.261\n",
      "Epoch   8 Batch   84/575   train_loss = 4.250\n",
      "Epoch   8 Batch   85/575   train_loss = 4.826\n",
      "Epoch   8 Batch   86/575   train_loss = 4.187\n",
      "Epoch   8 Batch   87/575   train_loss = 4.772\n",
      "Epoch   8 Batch   88/575   train_loss = 3.700\n",
      "Epoch   8 Batch   89/575   train_loss = 3.907\n",
      "Epoch   8 Batch   90/575   train_loss = 4.065\n",
      "Epoch   8 Batch   91/575   train_loss = 4.307\n",
      "Epoch   8 Batch   92/575   train_loss = 4.801\n",
      "Epoch   8 Batch   93/575   train_loss = 4.488\n",
      "Epoch   8 Batch   94/575   train_loss = 3.993\n",
      "Epoch   8 Batch   95/575   train_loss = 4.336\n",
      "Epoch   8 Batch   96/575   train_loss = 4.355\n",
      "Epoch   8 Batch   97/575   train_loss = 4.432\n",
      "Epoch   8 Batch   98/575   train_loss = 4.244\n",
      "Epoch   8 Batch   99/575   train_loss = 4.765\n",
      "Epoch   8 Batch  100/575   train_loss = 4.517\n",
      "Epoch   8 Batch  101/575   train_loss = 4.130\n",
      "Epoch   8 Batch  102/575   train_loss = 3.999\n",
      "Epoch   8 Batch  103/575   train_loss = 4.419\n",
      "Epoch   8 Batch  104/575   train_loss = 4.331\n",
      "Epoch   8 Batch  105/575   train_loss = 4.471\n",
      "Epoch   8 Batch  106/575   train_loss = 4.112\n",
      "Epoch   8 Batch  107/575   train_loss = 4.059\n",
      "Epoch   8 Batch  108/575   train_loss = 4.445\n",
      "Epoch   8 Batch  109/575   train_loss = 4.185\n",
      "Epoch   8 Batch  110/575   train_loss = 3.354\n",
      "Epoch   8 Batch  111/575   train_loss = 4.321\n",
      "Epoch   8 Batch  112/575   train_loss = 4.339\n",
      "Epoch   8 Batch  113/575   train_loss = 4.883\n",
      "Epoch   8 Batch  114/575   train_loss = 4.565\n",
      "Epoch   8 Batch  115/575   train_loss = 4.125\n",
      "Epoch   8 Batch  116/575   train_loss = 4.543\n",
      "Epoch   8 Batch  117/575   train_loss = 3.952\n",
      "Epoch   8 Batch  118/575   train_loss = 4.319\n",
      "Epoch   8 Batch  119/575   train_loss = 4.831\n",
      "Epoch   8 Batch  120/575   train_loss = 4.432\n",
      "Epoch   8 Batch  121/575   train_loss = 4.444\n",
      "Epoch   8 Batch  122/575   train_loss = 4.553\n",
      "Epoch   8 Batch  123/575   train_loss = 4.254\n",
      "Epoch   8 Batch  124/575   train_loss = 4.629\n",
      "Epoch   8 Batch  125/575   train_loss = 4.409\n",
      "Epoch   8 Batch  126/575   train_loss = 4.285\n",
      "Epoch   8 Batch  127/575   train_loss = 3.859\n",
      "Epoch   8 Batch  128/575   train_loss = 4.369\n",
      "Epoch   8 Batch  129/575   train_loss = 4.290\n",
      "Epoch   8 Batch  130/575   train_loss = 4.582\n",
      "Epoch   8 Batch  131/575   train_loss = 3.797\n",
      "Epoch   8 Batch  132/575   train_loss = 4.200\n",
      "Epoch   8 Batch  133/575   train_loss = 4.365\n",
      "Epoch   8 Batch  134/575   train_loss = 4.262\n",
      "Epoch   8 Batch  135/575   train_loss = 3.884\n",
      "Epoch   8 Batch  136/575   train_loss = 5.112\n",
      "Epoch   8 Batch  137/575   train_loss = 4.559\n",
      "Epoch   8 Batch  138/575   train_loss = 4.844\n",
      "Epoch   8 Batch  139/575   train_loss = 4.429\n",
      "Epoch   8 Batch  140/575   train_loss = 4.421\n",
      "Epoch   8 Batch  141/575   train_loss = 4.737\n",
      "Epoch   8 Batch  142/575   train_loss = 4.813\n",
      "Epoch   8 Batch  143/575   train_loss = 4.508\n",
      "Epoch   8 Batch  144/575   train_loss = 4.104\n",
      "Epoch   8 Batch  145/575   train_loss = 4.044\n",
      "Epoch   8 Batch  146/575   train_loss = 4.085\n",
      "Epoch   8 Batch  147/575   train_loss = 4.239\n",
      "Epoch   8 Batch  148/575   train_loss = 3.783\n",
      "Epoch   8 Batch  149/575   train_loss = 4.442\n",
      "Epoch   8 Batch  150/575   train_loss = 4.660\n",
      "Epoch   8 Batch  151/575   train_loss = 4.021\n",
      "Epoch   8 Batch  152/575   train_loss = 3.957\n",
      "Epoch   8 Batch  153/575   train_loss = 3.523\n",
      "Epoch   8 Batch  154/575   train_loss = 3.704\n",
      "Epoch   8 Batch  155/575   train_loss = 4.390\n",
      "Epoch   8 Batch  156/575   train_loss = 4.314\n",
      "Epoch   8 Batch  157/575   train_loss = 4.346\n",
      "Epoch   8 Batch  158/575   train_loss = 4.376\n",
      "Epoch   8 Batch  159/575   train_loss = 4.468\n",
      "Epoch   8 Batch  160/575   train_loss = 4.124\n",
      "Epoch   8 Batch  161/575   train_loss = 4.605\n",
      "Epoch   8 Batch  162/575   train_loss = 4.462\n",
      "Epoch   8 Batch  163/575   train_loss = 4.293\n",
      "Epoch   8 Batch  164/575   train_loss = 4.596\n",
      "Epoch   8 Batch  165/575   train_loss = 4.406\n",
      "Epoch   8 Batch  166/575   train_loss = 4.080\n",
      "Epoch   8 Batch  167/575   train_loss = 4.066\n",
      "Epoch   8 Batch  168/575   train_loss = 4.439\n",
      "Epoch   8 Batch  169/575   train_loss = 4.137\n",
      "Epoch   8 Batch  170/575   train_loss = 4.492\n",
      "Epoch   8 Batch  171/575   train_loss = 4.197\n",
      "Epoch   8 Batch  172/575   train_loss = 4.382\n",
      "Epoch   8 Batch  173/575   train_loss = 4.339\n",
      "Epoch   8 Batch  174/575   train_loss = 4.568\n",
      "Epoch   8 Batch  175/575   train_loss = 4.350\n",
      "Epoch   8 Batch  176/575   train_loss = 4.430\n",
      "Epoch   8 Batch  177/575   train_loss = 4.397\n",
      "Epoch   8 Batch  178/575   train_loss = 4.302\n",
      "Epoch   8 Batch  179/575   train_loss = 4.026\n",
      "Epoch   8 Batch  180/575   train_loss = 4.129\n",
      "Epoch   8 Batch  181/575   train_loss = 4.335\n",
      "Epoch   8 Batch  182/575   train_loss = 4.188\n",
      "Epoch   8 Batch  183/575   train_loss = 4.489\n",
      "Epoch   8 Batch  184/575   train_loss = 3.677\n",
      "Epoch   8 Batch  185/575   train_loss = 4.253\n",
      "Epoch   8 Batch  186/575   train_loss = 4.359\n",
      "Epoch   8 Batch  187/575   train_loss = 4.286\n",
      "Epoch   8 Batch  188/575   train_loss = 4.209\n",
      "Epoch   8 Batch  189/575   train_loss = 4.260\n",
      "Epoch   8 Batch  190/575   train_loss = 4.209\n",
      "Epoch   8 Batch  191/575   train_loss = 4.745\n",
      "Epoch   8 Batch  192/575   train_loss = 4.526\n",
      "Epoch   8 Batch  193/575   train_loss = 4.321\n",
      "Epoch   8 Batch  194/575   train_loss = 4.434\n",
      "Epoch   8 Batch  195/575   train_loss = 4.035\n",
      "Epoch   8 Batch  196/575   train_loss = 4.154\n",
      "Epoch   8 Batch  197/575   train_loss = 4.076\n",
      "Epoch   8 Batch  198/575   train_loss = 4.555\n",
      "Epoch   8 Batch  199/575   train_loss = 3.818\n",
      "Epoch   8 Batch  200/575   train_loss = 4.107\n",
      "Epoch   8 Batch  201/575   train_loss = 4.324\n",
      "Epoch   8 Batch  202/575   train_loss = 4.037\n",
      "Epoch   8 Batch  203/575   train_loss = 4.236\n",
      "Epoch   8 Batch  204/575   train_loss = 4.144\n",
      "Epoch   8 Batch  205/575   train_loss = 3.939\n",
      "Epoch   8 Batch  206/575   train_loss = 4.199\n",
      "Epoch   8 Batch  207/575   train_loss = 4.085\n",
      "Epoch   8 Batch  208/575   train_loss = 3.975\n",
      "Epoch   8 Batch  209/575   train_loss = 4.054\n",
      "Epoch   8 Batch  210/575   train_loss = 4.410\n",
      "Epoch   8 Batch  211/575   train_loss = 4.828\n",
      "Epoch   8 Batch  212/575   train_loss = 4.506\n",
      "Epoch   8 Batch  213/575   train_loss = 4.345\n",
      "Epoch   8 Batch  214/575   train_loss = 4.051\n",
      "Epoch   8 Batch  215/575   train_loss = 3.634\n",
      "Epoch   8 Batch  216/575   train_loss = 3.979\n",
      "Epoch   8 Batch  217/575   train_loss = 3.750\n",
      "Epoch   8 Batch  218/575   train_loss = 4.423\n",
      "Epoch   8 Batch  219/575   train_loss = 4.027\n",
      "Epoch   8 Batch  220/575   train_loss = 3.717\n",
      "Epoch   8 Batch  221/575   train_loss = 4.202\n",
      "Epoch   8 Batch  222/575   train_loss = 4.299\n",
      "Epoch   8 Batch  223/575   train_loss = 4.568\n",
      "Epoch   8 Batch  224/575   train_loss = 3.902\n",
      "Epoch   8 Batch  225/575   train_loss = 4.014\n",
      "Epoch   8 Batch  226/575   train_loss = 4.400\n",
      "Epoch   8 Batch  227/575   train_loss = 4.683\n",
      "Epoch   8 Batch  228/575   train_loss = 4.493\n",
      "Epoch   8 Batch  229/575   train_loss = 3.654\n",
      "Epoch   8 Batch  230/575   train_loss = 3.782\n",
      "Epoch   8 Batch  231/575   train_loss = 4.244\n",
      "Epoch   8 Batch  232/575   train_loss = 4.412\n",
      "Epoch   8 Batch  233/575   train_loss = 4.301\n",
      "Epoch   8 Batch  234/575   train_loss = 4.422\n",
      "Epoch   8 Batch  235/575   train_loss = 4.176\n",
      "Epoch   8 Batch  236/575   train_loss = 3.708\n",
      "Epoch   8 Batch  237/575   train_loss = 3.969\n",
      "Epoch   8 Batch  238/575   train_loss = 4.000\n",
      "Epoch   8 Batch  239/575   train_loss = 4.304\n",
      "Epoch   8 Batch  240/575   train_loss = 4.515\n",
      "Epoch   8 Batch  241/575   train_loss = 4.579\n",
      "Epoch   8 Batch  242/575   train_loss = 4.169\n",
      "Epoch   8 Batch  243/575   train_loss = 4.268\n",
      "Epoch   8 Batch  244/575   train_loss = 4.170\n",
      "Epoch   8 Batch  245/575   train_loss = 4.374\n",
      "Epoch   8 Batch  246/575   train_loss = 4.580\n",
      "Epoch   8 Batch  247/575   train_loss = 4.259\n",
      "Epoch   8 Batch  248/575   train_loss = 4.380\n",
      "Epoch   8 Batch  249/575   train_loss = 4.248\n",
      "Epoch   8 Batch  250/575   train_loss = 4.171\n",
      "Epoch   8 Batch  251/575   train_loss = 4.277\n",
      "Epoch   8 Batch  252/575   train_loss = 4.641\n",
      "Epoch   8 Batch  253/575   train_loss = 4.270\n",
      "Epoch   8 Batch  254/575   train_loss = 3.914\n",
      "Epoch   8 Batch  255/575   train_loss = 4.486\n",
      "Epoch   8 Batch  256/575   train_loss = 4.142\n",
      "Epoch   8 Batch  257/575   train_loss = 4.290\n",
      "Epoch   8 Batch  258/575   train_loss = 4.654\n",
      "Epoch   8 Batch  259/575   train_loss = 3.811\n",
      "Epoch   8 Batch  260/575   train_loss = 4.072\n",
      "Epoch   8 Batch  261/575   train_loss = 4.706\n",
      "Epoch   8 Batch  262/575   train_loss = 4.475\n",
      "Epoch   8 Batch  263/575   train_loss = 4.440\n",
      "Epoch   8 Batch  264/575   train_loss = 3.655\n",
      "Epoch   8 Batch  265/575   train_loss = 3.647\n",
      "Epoch   8 Batch  266/575   train_loss = 3.774\n",
      "Epoch   8 Batch  267/575   train_loss = 4.249\n",
      "Epoch   8 Batch  268/575   train_loss = 4.220\n",
      "Epoch   8 Batch  269/575   train_loss = 4.416\n",
      "Epoch   8 Batch  270/575   train_loss = 4.787\n",
      "Epoch   8 Batch  271/575   train_loss = 3.955\n",
      "Epoch   8 Batch  272/575   train_loss = 4.484\n",
      "Epoch   8 Batch  273/575   train_loss = 4.489\n",
      "Epoch   8 Batch  274/575   train_loss = 4.454\n",
      "Epoch   8 Batch  275/575   train_loss = 4.255\n",
      "Epoch   8 Batch  276/575   train_loss = 4.809\n",
      "Epoch   8 Batch  277/575   train_loss = 3.607\n",
      "Epoch   8 Batch  278/575   train_loss = 4.382\n",
      "Epoch   8 Batch  279/575   train_loss = 4.523\n",
      "Epoch   8 Batch  280/575   train_loss = 4.513\n",
      "Epoch   8 Batch  281/575   train_loss = 4.124\n",
      "Epoch   8 Batch  282/575   train_loss = 4.417\n",
      "Epoch   8 Batch  283/575   train_loss = 4.631\n",
      "Epoch   8 Batch  284/575   train_loss = 4.764\n",
      "Epoch   8 Batch  285/575   train_loss = 4.011\n",
      "Epoch   8 Batch  286/575   train_loss = 4.333\n",
      "Epoch   8 Batch  287/575   train_loss = 4.449\n",
      "Epoch   8 Batch  288/575   train_loss = 4.239\n",
      "Epoch   8 Batch  289/575   train_loss = 4.303\n",
      "Epoch   8 Batch  290/575   train_loss = 4.312\n",
      "Epoch   8 Batch  291/575   train_loss = 4.862\n",
      "Epoch   8 Batch  292/575   train_loss = 4.826\n",
      "Epoch   8 Batch  293/575   train_loss = 4.972\n",
      "Epoch   8 Batch  294/575   train_loss = 4.590\n",
      "Epoch   8 Batch  295/575   train_loss = 4.251\n",
      "Epoch   8 Batch  296/575   train_loss = 4.435\n",
      "Epoch   8 Batch  297/575   train_loss = 4.336\n",
      "Epoch   8 Batch  298/575   train_loss = 4.544\n",
      "Epoch   8 Batch  299/575   train_loss = 4.457\n",
      "Epoch   8 Batch  300/575   train_loss = 4.365\n",
      "Epoch   8 Batch  301/575   train_loss = 4.175\n",
      "Epoch   8 Batch  302/575   train_loss = 4.555\n",
      "Epoch   8 Batch  303/575   train_loss = 4.128\n",
      "Epoch   8 Batch  304/575   train_loss = 4.187\n",
      "Epoch   8 Batch  305/575   train_loss = 4.175\n",
      "Epoch   8 Batch  306/575   train_loss = 4.425\n",
      "Epoch   8 Batch  307/575   train_loss = 4.459\n",
      "Epoch   8 Batch  308/575   train_loss = 4.330\n",
      "Epoch   8 Batch  309/575   train_loss = 4.614\n",
      "Epoch   8 Batch  310/575   train_loss = 4.691\n",
      "Epoch   8 Batch  311/575   train_loss = 4.068\n",
      "Epoch   8 Batch  312/575   train_loss = 4.374\n",
      "Epoch   8 Batch  313/575   train_loss = 4.689\n",
      "Epoch   8 Batch  314/575   train_loss = 4.654\n",
      "Epoch   8 Batch  315/575   train_loss = 4.807\n",
      "Epoch   8 Batch  316/575   train_loss = 4.236\n",
      "Epoch   8 Batch  317/575   train_loss = 3.934\n",
      "Epoch   8 Batch  318/575   train_loss = 3.675\n",
      "Epoch   8 Batch  319/575   train_loss = 4.362\n",
      "Epoch   8 Batch  320/575   train_loss = 4.222\n",
      "Epoch   8 Batch  321/575   train_loss = 4.575\n",
      "Epoch   8 Batch  322/575   train_loss = 4.268\n",
      "Epoch   8 Batch  323/575   train_loss = 4.809\n",
      "Epoch   8 Batch  324/575   train_loss = 4.257\n",
      "Epoch   8 Batch  325/575   train_loss = 4.409\n",
      "Epoch   8 Batch  326/575   train_loss = 4.179\n",
      "Epoch   8 Batch  327/575   train_loss = 4.524\n",
      "Epoch   8 Batch  328/575   train_loss = 4.103\n",
      "Epoch   8 Batch  329/575   train_loss = 4.790\n",
      "Epoch   8 Batch  330/575   train_loss = 4.400\n",
      "Epoch   8 Batch  331/575   train_loss = 4.811\n",
      "Epoch   8 Batch  332/575   train_loss = 4.422\n",
      "Epoch   8 Batch  333/575   train_loss = 4.394\n",
      "Epoch   8 Batch  334/575   train_loss = 4.341\n",
      "Epoch   8 Batch  335/575   train_loss = 4.065\n",
      "Epoch   8 Batch  336/575   train_loss = 4.427\n",
      "Epoch   8 Batch  337/575   train_loss = 4.245\n",
      "Epoch   8 Batch  338/575   train_loss = 4.068\n",
      "Epoch   8 Batch  339/575   train_loss = 4.415\n",
      "Epoch   8 Batch  340/575   train_loss = 4.223\n",
      "Epoch   8 Batch  341/575   train_loss = 4.239\n",
      "Epoch   8 Batch  342/575   train_loss = 4.080\n",
      "Epoch   8 Batch  343/575   train_loss = 4.569\n",
      "Epoch   8 Batch  344/575   train_loss = 4.605\n",
      "Epoch   8 Batch  345/575   train_loss = 4.193\n",
      "Epoch   8 Batch  346/575   train_loss = 4.822\n",
      "Epoch   8 Batch  347/575   train_loss = 4.059\n",
      "Epoch   8 Batch  348/575   train_loss = 4.098\n",
      "Epoch   8 Batch  349/575   train_loss = 3.887\n",
      "Epoch   8 Batch  350/575   train_loss = 4.421\n",
      "Epoch   8 Batch  351/575   train_loss = 4.742\n",
      "Epoch   8 Batch  352/575   train_loss = 4.645\n",
      "Epoch   8 Batch  353/575   train_loss = 4.467\n",
      "Epoch   8 Batch  354/575   train_loss = 4.041\n",
      "Epoch   8 Batch  355/575   train_loss = 4.716\n",
      "Epoch   8 Batch  356/575   train_loss = 4.523\n",
      "Epoch   8 Batch  357/575   train_loss = 4.571\n",
      "Epoch   8 Batch  358/575   train_loss = 4.642\n",
      "Epoch   8 Batch  359/575   train_loss = 4.765\n",
      "Epoch   8 Batch  360/575   train_loss = 3.981\n",
      "Epoch   8 Batch  361/575   train_loss = 4.481\n",
      "Epoch   8 Batch  362/575   train_loss = 4.284\n",
      "Epoch   8 Batch  363/575   train_loss = 4.396\n",
      "Epoch   8 Batch  364/575   train_loss = 4.151\n",
      "Epoch   8 Batch  365/575   train_loss = 4.447\n",
      "Epoch   8 Batch  366/575   train_loss = 3.895\n",
      "Epoch   8 Batch  367/575   train_loss = 4.163\n",
      "Epoch   8 Batch  368/575   train_loss = 4.938\n",
      "Epoch   8 Batch  369/575   train_loss = 4.656\n",
      "Epoch   8 Batch  370/575   train_loss = 4.468\n",
      "Epoch   8 Batch  371/575   train_loss = 4.461\n",
      "Epoch   8 Batch  372/575   train_loss = 4.149\n",
      "Epoch   8 Batch  373/575   train_loss = 4.591\n",
      "Epoch   8 Batch  374/575   train_loss = 3.761\n",
      "Epoch   8 Batch  375/575   train_loss = 4.151\n",
      "Epoch   8 Batch  376/575   train_loss = 4.469\n",
      "Epoch   8 Batch  377/575   train_loss = 4.744\n",
      "Epoch   8 Batch  378/575   train_loss = 4.852\n",
      "Epoch   8 Batch  379/575   train_loss = 4.299\n",
      "Epoch   8 Batch  380/575   train_loss = 3.979\n",
      "Epoch   8 Batch  381/575   train_loss = 4.122\n",
      "Epoch   8 Batch  382/575   train_loss = 4.435\n",
      "Epoch   8 Batch  383/575   train_loss = 4.210\n",
      "Epoch   8 Batch  384/575   train_loss = 4.337\n",
      "Epoch   8 Batch  385/575   train_loss = 4.246\n",
      "Epoch   8 Batch  386/575   train_loss = 4.166\n",
      "Epoch   8 Batch  387/575   train_loss = 4.734\n",
      "Epoch   8 Batch  388/575   train_loss = 4.413\n",
      "Epoch   8 Batch  389/575   train_loss = 4.299\n",
      "Epoch   8 Batch  390/575   train_loss = 4.479\n",
      "Epoch   8 Batch  391/575   train_loss = 4.500\n",
      "Epoch   8 Batch  392/575   train_loss = 4.489\n",
      "Epoch   8 Batch  393/575   train_loss = 4.479\n",
      "Epoch   8 Batch  394/575   train_loss = 4.609\n",
      "Epoch   8 Batch  395/575   train_loss = 3.808\n",
      "Epoch   8 Batch  396/575   train_loss = 4.016\n",
      "Epoch   8 Batch  397/575   train_loss = 4.431\n",
      "Epoch   8 Batch  398/575   train_loss = 4.871\n",
      "Epoch   8 Batch  399/575   train_loss = 4.762\n",
      "Epoch   8 Batch  400/575   train_loss = 4.603\n",
      "Epoch   8 Batch  401/575   train_loss = 4.065\n",
      "Epoch   8 Batch  402/575   train_loss = 4.130\n",
      "Epoch   8 Batch  403/575   train_loss = 4.511\n",
      "Epoch   8 Batch  404/575   train_loss = 4.426\n",
      "Epoch   8 Batch  405/575   train_loss = 4.526\n",
      "Epoch   8 Batch  406/575   train_loss = 4.074\n",
      "Epoch   8 Batch  407/575   train_loss = 4.453\n",
      "Epoch   8 Batch  408/575   train_loss = 4.229\n",
      "Epoch   8 Batch  409/575   train_loss = 4.374\n",
      "Epoch   8 Batch  410/575   train_loss = 4.090\n",
      "Epoch   8 Batch  411/575   train_loss = 4.545\n",
      "Epoch   8 Batch  412/575   train_loss = 4.931\n",
      "Epoch   8 Batch  413/575   train_loss = 4.483\n",
      "Epoch   8 Batch  414/575   train_loss = 4.181\n",
      "Epoch   8 Batch  415/575   train_loss = 4.624\n",
      "Epoch   8 Batch  416/575   train_loss = 4.243\n",
      "Epoch   8 Batch  417/575   train_loss = 4.024\n",
      "Epoch   8 Batch  418/575   train_loss = 4.425\n",
      "Epoch   8 Batch  419/575   train_loss = 4.279\n",
      "Epoch   8 Batch  420/575   train_loss = 4.146\n",
      "Epoch   8 Batch  421/575   train_loss = 4.760\n",
      "Epoch   8 Batch  422/575   train_loss = 4.539\n",
      "Epoch   8 Batch  423/575   train_loss = 4.802\n",
      "Epoch   8 Batch  424/575   train_loss = 4.775\n",
      "Epoch   8 Batch  425/575   train_loss = 4.430\n",
      "Epoch   8 Batch  426/575   train_loss = 4.491\n",
      "Epoch   8 Batch  427/575   train_loss = 4.400\n",
      "Epoch   8 Batch  428/575   train_loss = 4.147\n",
      "Epoch   8 Batch  429/575   train_loss = 4.194\n",
      "Epoch   8 Batch  430/575   train_loss = 4.457\n",
      "Epoch   8 Batch  431/575   train_loss = 4.328\n",
      "Epoch   8 Batch  432/575   train_loss = 4.692\n",
      "Epoch   8 Batch  433/575   train_loss = 4.640\n",
      "Epoch   8 Batch  434/575   train_loss = 4.520\n",
      "Epoch   8 Batch  435/575   train_loss = 4.246\n",
      "Epoch   8 Batch  436/575   train_loss = 4.202\n",
      "Epoch   8 Batch  437/575   train_loss = 4.283\n",
      "Epoch   8 Batch  438/575   train_loss = 4.508\n",
      "Epoch   8 Batch  439/575   train_loss = 4.491\n",
      "Epoch   8 Batch  440/575   train_loss = 4.696\n",
      "Epoch   8 Batch  441/575   train_loss = 4.251\n",
      "Epoch   8 Batch  442/575   train_loss = 4.621\n",
      "Epoch   8 Batch  443/575   train_loss = 4.458\n",
      "Epoch   8 Batch  444/575   train_loss = 4.505\n",
      "Epoch   8 Batch  445/575   train_loss = 4.020\n",
      "Epoch   8 Batch  446/575   train_loss = 4.076\n",
      "Epoch   8 Batch  447/575   train_loss = 4.548\n",
      "Epoch   8 Batch  448/575   train_loss = 4.521\n",
      "Epoch   8 Batch  449/575   train_loss = 4.378\n",
      "Epoch   8 Batch  450/575   train_loss = 4.748\n",
      "Epoch   8 Batch  451/575   train_loss = 4.808\n",
      "Epoch   8 Batch  452/575   train_loss = 4.825\n",
      "Epoch   8 Batch  453/575   train_loss = 4.465\n",
      "Epoch   8 Batch  454/575   train_loss = 4.835\n",
      "Epoch   8 Batch  455/575   train_loss = 4.111\n",
      "Epoch   8 Batch  456/575   train_loss = 4.653\n",
      "Epoch   8 Batch  457/575   train_loss = 3.987\n",
      "Epoch   8 Batch  458/575   train_loss = 4.730\n",
      "Epoch   8 Batch  459/575   train_loss = 4.419\n",
      "Epoch   8 Batch  460/575   train_loss = 4.182\n",
      "Epoch   8 Batch  461/575   train_loss = 4.316\n",
      "Epoch   8 Batch  462/575   train_loss = 4.312\n",
      "Epoch   8 Batch  463/575   train_loss = 4.153\n",
      "Epoch   8 Batch  464/575   train_loss = 4.189\n",
      "Epoch   8 Batch  465/575   train_loss = 4.319\n",
      "Epoch   8 Batch  466/575   train_loss = 4.261\n",
      "Epoch   8 Batch  467/575   train_loss = 3.718\n",
      "Epoch   8 Batch  468/575   train_loss = 4.304\n",
      "Epoch   8 Batch  469/575   train_loss = 4.702\n",
      "Epoch   8 Batch  470/575   train_loss = 4.808\n",
      "Epoch   8 Batch  471/575   train_loss = 4.192\n",
      "Epoch   8 Batch  472/575   train_loss = 4.660\n",
      "Epoch   8 Batch  473/575   train_loss = 4.508\n",
      "Epoch   8 Batch  474/575   train_loss = 4.516\n",
      "Epoch   8 Batch  475/575   train_loss = 4.165\n",
      "Epoch   8 Batch  476/575   train_loss = 4.305\n",
      "Epoch   8 Batch  477/575   train_loss = 4.853\n",
      "Epoch   8 Batch  478/575   train_loss = 4.901\n",
      "Epoch   8 Batch  479/575   train_loss = 4.208\n",
      "Epoch   8 Batch  480/575   train_loss = 4.433\n",
      "Epoch   8 Batch  481/575   train_loss = 4.601\n",
      "Epoch   8 Batch  482/575   train_loss = 4.025\n",
      "Epoch   8 Batch  483/575   train_loss = 5.033\n",
      "Epoch   8 Batch  484/575   train_loss = 4.269\n",
      "Epoch   8 Batch  485/575   train_loss = 4.924\n",
      "Epoch   8 Batch  486/575   train_loss = 4.425\n",
      "Epoch   8 Batch  487/575   train_loss = 4.699\n",
      "Epoch   8 Batch  488/575   train_loss = 4.439\n",
      "Epoch   8 Batch  489/575   train_loss = 4.610\n",
      "Epoch   8 Batch  490/575   train_loss = 4.564\n",
      "Epoch   8 Batch  491/575   train_loss = 3.961\n",
      "Epoch   8 Batch  492/575   train_loss = 3.970\n",
      "Epoch   8 Batch  493/575   train_loss = 3.993\n",
      "Epoch   8 Batch  494/575   train_loss = 4.346\n",
      "Epoch   8 Batch  495/575   train_loss = 4.296\n",
      "Epoch   8 Batch  496/575   train_loss = 4.483\n",
      "Epoch   8 Batch  497/575   train_loss = 4.558\n",
      "Epoch   8 Batch  498/575   train_loss = 4.212\n",
      "Epoch   8 Batch  499/575   train_loss = 4.215\n",
      "Epoch   8 Batch  500/575   train_loss = 4.526\n",
      "Epoch   8 Batch  501/575   train_loss = 4.492\n",
      "Epoch   8 Batch  502/575   train_loss = 3.842\n",
      "Epoch   8 Batch  503/575   train_loss = 4.731\n",
      "Epoch   8 Batch  504/575   train_loss = 3.824\n",
      "Epoch   8 Batch  505/575   train_loss = 4.338\n",
      "Epoch   8 Batch  506/575   train_loss = 4.536\n",
      "Epoch   8 Batch  507/575   train_loss = 4.672\n",
      "Epoch   8 Batch  508/575   train_loss = 4.732\n",
      "Epoch   8 Batch  509/575   train_loss = 4.452\n",
      "Epoch   8 Batch  510/575   train_loss = 3.820\n",
      "Epoch   8 Batch  511/575   train_loss = 3.953\n",
      "Epoch   8 Batch  512/575   train_loss = 4.709\n",
      "Epoch   8 Batch  513/575   train_loss = 4.418\n",
      "Epoch   8 Batch  514/575   train_loss = 4.343\n",
      "Epoch   8 Batch  515/575   train_loss = 4.374\n",
      "Epoch   8 Batch  516/575   train_loss = 4.698\n",
      "Epoch   8 Batch  517/575   train_loss = 3.905\n",
      "Epoch   8 Batch  518/575   train_loss = 4.134\n",
      "Epoch   8 Batch  519/575   train_loss = 4.303\n",
      "Epoch   8 Batch  520/575   train_loss = 3.794\n",
      "Epoch   8 Batch  521/575   train_loss = 4.074\n",
      "Epoch   8 Batch  522/575   train_loss = 4.234\n",
      "Epoch   8 Batch  523/575   train_loss = 4.560\n",
      "Epoch   8 Batch  524/575   train_loss = 5.087\n",
      "Epoch   8 Batch  525/575   train_loss = 4.633\n",
      "Epoch   8 Batch  526/575   train_loss = 4.074\n",
      "Epoch   8 Batch  527/575   train_loss = 4.658\n",
      "Epoch   8 Batch  528/575   train_loss = 4.180\n",
      "Epoch   8 Batch  529/575   train_loss = 4.692\n",
      "Epoch   8 Batch  530/575   train_loss = 4.482\n",
      "Epoch   8 Batch  531/575   train_loss = 4.588\n",
      "Epoch   8 Batch  532/575   train_loss = 4.180\n",
      "Epoch   8 Batch  533/575   train_loss = 4.067\n",
      "Epoch   8 Batch  534/575   train_loss = 4.119\n",
      "Epoch   8 Batch  535/575   train_loss = 4.376\n",
      "Epoch   8 Batch  536/575   train_loss = 4.214\n",
      "Epoch   8 Batch  537/575   train_loss = 4.064\n",
      "Epoch   8 Batch  538/575   train_loss = 4.286\n",
      "Epoch   8 Batch  539/575   train_loss = 4.172\n",
      "Epoch   8 Batch  540/575   train_loss = 4.440\n",
      "Epoch   8 Batch  541/575   train_loss = 4.836\n",
      "Epoch   8 Batch  542/575   train_loss = 4.387\n",
      "Epoch   8 Batch  543/575   train_loss = 4.455\n",
      "Epoch   8 Batch  544/575   train_loss = 4.315\n",
      "Epoch   8 Batch  545/575   train_loss = 4.332\n",
      "Epoch   8 Batch  546/575   train_loss = 4.357\n",
      "Epoch   8 Batch  547/575   train_loss = 4.385\n",
      "Epoch   8 Batch  548/575   train_loss = 4.522\n",
      "Epoch   8 Batch  549/575   train_loss = 4.309\n",
      "Epoch   8 Batch  550/575   train_loss = 4.203\n",
      "Epoch   8 Batch  551/575   train_loss = 4.092\n",
      "Epoch   8 Batch  552/575   train_loss = 4.382\n",
      "Epoch   8 Batch  553/575   train_loss = 4.654\n",
      "Epoch   8 Batch  554/575   train_loss = 4.002\n",
      "Epoch   8 Batch  555/575   train_loss = 4.246\n",
      "Epoch   8 Batch  556/575   train_loss = 4.062\n",
      "Epoch   8 Batch  557/575   train_loss = 4.108\n",
      "Epoch   8 Batch  558/575   train_loss = 4.377\n",
      "Epoch   8 Batch  559/575   train_loss = 4.517\n",
      "Epoch   8 Batch  560/575   train_loss = 3.977\n",
      "Epoch   8 Batch  561/575   train_loss = 4.331\n",
      "Epoch   8 Batch  562/575   train_loss = 3.880\n",
      "Epoch   8 Batch  563/575   train_loss = 4.231\n",
      "Epoch   8 Batch  564/575   train_loss = 4.003\n",
      "Epoch   8 Batch  565/575   train_loss = 3.517\n",
      "Epoch   8 Batch  566/575   train_loss = 3.995\n",
      "Epoch   8 Batch  567/575   train_loss = 4.159\n",
      "Epoch   8 Batch  568/575   train_loss = 4.197\n",
      "Epoch   8 Batch  569/575   train_loss = 4.421\n",
      "Epoch   8 Batch  570/575   train_loss = 4.006\n",
      "Epoch   8 Batch  571/575   train_loss = 3.792\n",
      "Epoch   8 Batch  572/575   train_loss = 3.590\n",
      "Epoch   8 Batch  573/575   train_loss = 3.942\n",
      "Epoch   8 Batch  574/575   train_loss = 4.453\n",
      "Epoch   9 Batch    0/575   train_loss = 4.189\n",
      "Epoch   9 Batch    1/575   train_loss = 4.202\n",
      "Epoch   9 Batch    2/575   train_loss = 4.337\n",
      "Epoch   9 Batch    3/575   train_loss = 4.152\n",
      "Epoch   9 Batch    4/575   train_loss = 4.066\n",
      "Epoch   9 Batch    5/575   train_loss = 4.072\n",
      "Epoch   9 Batch    6/575   train_loss = 3.911\n",
      "Epoch   9 Batch    7/575   train_loss = 4.233\n",
      "Epoch   9 Batch    8/575   train_loss = 4.112\n",
      "Epoch   9 Batch    9/575   train_loss = 4.426\n",
      "Epoch   9 Batch   10/575   train_loss = 3.940\n",
      "Epoch   9 Batch   11/575   train_loss = 3.983\n",
      "Epoch   9 Batch   12/575   train_loss = 4.173\n",
      "Epoch   9 Batch   13/575   train_loss = 4.331\n",
      "Epoch   9 Batch   14/575   train_loss = 4.675\n",
      "Epoch   9 Batch   15/575   train_loss = 4.379\n",
      "Epoch   9 Batch   16/575   train_loss = 4.272\n",
      "Epoch   9 Batch   17/575   train_loss = 3.677\n",
      "Epoch   9 Batch   18/575   train_loss = 4.596\n",
      "Epoch   9 Batch   19/575   train_loss = 4.328\n",
      "Epoch   9 Batch   20/575   train_loss = 3.691\n",
      "Epoch   9 Batch   21/575   train_loss = 4.219\n",
      "Epoch   9 Batch   22/575   train_loss = 4.680\n",
      "Epoch   9 Batch   23/575   train_loss = 4.264\n",
      "Epoch   9 Batch   24/575   train_loss = 4.176\n",
      "Epoch   9 Batch   25/575   train_loss = 4.271\n",
      "Epoch   9 Batch   26/575   train_loss = 4.524\n",
      "Epoch   9 Batch   27/575   train_loss = 4.208\n",
      "Epoch   9 Batch   28/575   train_loss = 3.814\n",
      "Epoch   9 Batch   29/575   train_loss = 4.004\n",
      "Epoch   9 Batch   30/575   train_loss = 4.683\n",
      "Epoch   9 Batch   31/575   train_loss = 4.922\n",
      "Epoch   9 Batch   32/575   train_loss = 4.348\n",
      "Epoch   9 Batch   33/575   train_loss = 4.560\n",
      "Epoch   9 Batch   34/575   train_loss = 4.294\n",
      "Epoch   9 Batch   35/575   train_loss = 3.957\n",
      "Epoch   9 Batch   36/575   train_loss = 4.561\n",
      "Epoch   9 Batch   37/575   train_loss = 3.741\n",
      "Epoch   9 Batch   38/575   train_loss = 4.411\n",
      "Epoch   9 Batch   39/575   train_loss = 3.741\n",
      "Epoch   9 Batch   40/575   train_loss = 3.642\n",
      "Epoch   9 Batch   41/575   train_loss = 5.311\n",
      "Epoch   9 Batch   42/575   train_loss = 4.398\n",
      "Epoch   9 Batch   43/575   train_loss = 4.256\n",
      "Epoch   9 Batch   44/575   train_loss = 4.118\n",
      "Epoch   9 Batch   45/575   train_loss = 4.067\n",
      "Epoch   9 Batch   46/575   train_loss = 4.034\n",
      "Epoch   9 Batch   47/575   train_loss = 4.570\n",
      "Epoch   9 Batch   48/575   train_loss = 3.753\n",
      "Epoch   9 Batch   49/575   train_loss = 3.978\n",
      "Epoch   9 Batch   50/575   train_loss = 4.178\n",
      "Epoch   9 Batch   51/575   train_loss = 4.106\n",
      "Epoch   9 Batch   52/575   train_loss = 4.104\n",
      "Epoch   9 Batch   53/575   train_loss = 3.713\n",
      "Epoch   9 Batch   54/575   train_loss = 4.170\n",
      "Epoch   9 Batch   55/575   train_loss = 4.456\n",
      "Epoch   9 Batch   56/575   train_loss = 3.895\n",
      "Epoch   9 Batch   57/575   train_loss = 4.354\n",
      "Epoch   9 Batch   58/575   train_loss = 3.903\n",
      "Epoch   9 Batch   59/575   train_loss = 3.760\n",
      "Epoch   9 Batch   60/575   train_loss = 4.232\n",
      "Epoch   9 Batch   61/575   train_loss = 4.384\n",
      "Epoch   9 Batch   62/575   train_loss = 3.792\n",
      "Epoch   9 Batch   63/575   train_loss = 4.543\n",
      "Epoch   9 Batch   64/575   train_loss = 4.487\n",
      "Epoch   9 Batch   65/575   train_loss = 4.080\n",
      "Epoch   9 Batch   66/575   train_loss = 4.053\n",
      "Epoch   9 Batch   67/575   train_loss = 3.977\n",
      "Epoch   9 Batch   68/575   train_loss = 3.965\n",
      "Epoch   9 Batch   69/575   train_loss = 3.778\n",
      "Epoch   9 Batch   70/575   train_loss = 4.153\n",
      "Epoch   9 Batch   71/575   train_loss = 3.957\n",
      "Epoch   9 Batch   72/575   train_loss = 4.466\n",
      "Epoch   9 Batch   73/575   train_loss = 4.495\n",
      "Epoch   9 Batch   74/575   train_loss = 4.572\n",
      "Epoch   9 Batch   75/575   train_loss = 4.288\n",
      "Epoch   9 Batch   76/575   train_loss = 4.480\n",
      "Epoch   9 Batch   77/575   train_loss = 4.709\n",
      "Epoch   9 Batch   78/575   train_loss = 4.373\n",
      "Epoch   9 Batch   79/575   train_loss = 4.195\n",
      "Epoch   9 Batch   80/575   train_loss = 4.194\n",
      "Epoch   9 Batch   81/575   train_loss = 4.044\n",
      "Epoch   9 Batch   82/575   train_loss = 3.921\n",
      "Epoch   9 Batch   83/575   train_loss = 4.162\n",
      "Epoch   9 Batch   84/575   train_loss = 4.136\n",
      "Epoch   9 Batch   85/575   train_loss = 4.686\n",
      "Epoch   9 Batch   86/575   train_loss = 4.078\n",
      "Epoch   9 Batch   87/575   train_loss = 4.652\n",
      "Epoch   9 Batch   88/575   train_loss = 3.632\n",
      "Epoch   9 Batch   89/575   train_loss = 3.800\n",
      "Epoch   9 Batch   90/575   train_loss = 3.916\n",
      "Epoch   9 Batch   91/575   train_loss = 4.198\n",
      "Epoch   9 Batch   92/575   train_loss = 4.678\n",
      "Epoch   9 Batch   93/575   train_loss = 4.365\n",
      "Epoch   9 Batch   94/575   train_loss = 3.860\n",
      "Epoch   9 Batch   95/575   train_loss = 4.218\n",
      "Epoch   9 Batch   96/575   train_loss = 4.234\n",
      "Epoch   9 Batch   97/575   train_loss = 4.314\n",
      "Epoch   9 Batch   98/575   train_loss = 4.133\n",
      "Epoch   9 Batch   99/575   train_loss = 4.615\n",
      "Epoch   9 Batch  100/575   train_loss = 4.393\n",
      "Epoch   9 Batch  101/575   train_loss = 4.020\n",
      "Epoch   9 Batch  102/575   train_loss = 3.914\n",
      "Epoch   9 Batch  103/575   train_loss = 4.262\n",
      "Epoch   9 Batch  104/575   train_loss = 4.219\n",
      "Epoch   9 Batch  105/575   train_loss = 4.327\n",
      "Epoch   9 Batch  106/575   train_loss = 4.006\n",
      "Epoch   9 Batch  107/575   train_loss = 3.977\n",
      "Epoch   9 Batch  108/575   train_loss = 4.350\n",
      "Epoch   9 Batch  109/575   train_loss = 4.081\n",
      "Epoch   9 Batch  110/575   train_loss = 3.273\n",
      "Epoch   9 Batch  111/575   train_loss = 4.218\n",
      "Epoch   9 Batch  112/575   train_loss = 4.216\n",
      "Epoch   9 Batch  113/575   train_loss = 4.754\n",
      "Epoch   9 Batch  114/575   train_loss = 4.464\n",
      "Epoch   9 Batch  115/575   train_loss = 4.015\n",
      "Epoch   9 Batch  116/575   train_loss = 4.407\n",
      "Epoch   9 Batch  117/575   train_loss = 3.876\n",
      "Epoch   9 Batch  118/575   train_loss = 4.179\n",
      "Epoch   9 Batch  119/575   train_loss = 4.688\n",
      "Epoch   9 Batch  120/575   train_loss = 4.317\n",
      "Epoch   9 Batch  121/575   train_loss = 4.292\n",
      "Epoch   9 Batch  122/575   train_loss = 4.453\n",
      "Epoch   9 Batch  123/575   train_loss = 4.149\n",
      "Epoch   9 Batch  124/575   train_loss = 4.493\n",
      "Epoch   9 Batch  125/575   train_loss = 4.288\n",
      "Epoch   9 Batch  126/575   train_loss = 4.129\n",
      "Epoch   9 Batch  127/575   train_loss = 3.771\n",
      "Epoch   9 Batch  128/575   train_loss = 4.253\n",
      "Epoch   9 Batch  129/575   train_loss = 4.157\n",
      "Epoch   9 Batch  130/575   train_loss = 4.494\n",
      "Epoch   9 Batch  131/575   train_loss = 3.675\n",
      "Epoch   9 Batch  132/575   train_loss = 4.153\n",
      "Epoch   9 Batch  133/575   train_loss = 4.244\n",
      "Epoch   9 Batch  134/575   train_loss = 4.180\n",
      "Epoch   9 Batch  135/575   train_loss = 3.776\n",
      "Epoch   9 Batch  136/575   train_loss = 4.957\n",
      "Epoch   9 Batch  137/575   train_loss = 4.494\n",
      "Epoch   9 Batch  138/575   train_loss = 4.717\n",
      "Epoch   9 Batch  139/575   train_loss = 4.259\n",
      "Epoch   9 Batch  140/575   train_loss = 4.283\n",
      "Epoch   9 Batch  141/575   train_loss = 4.598\n",
      "Epoch   9 Batch  142/575   train_loss = 4.607\n",
      "Epoch   9 Batch  143/575   train_loss = 4.403\n",
      "Epoch   9 Batch  144/575   train_loss = 3.983\n",
      "Epoch   9 Batch  145/575   train_loss = 3.942\n",
      "Epoch   9 Batch  146/575   train_loss = 3.972\n",
      "Epoch   9 Batch  147/575   train_loss = 4.128\n",
      "Epoch   9 Batch  148/575   train_loss = 3.700\n",
      "Epoch   9 Batch  149/575   train_loss = 4.352\n",
      "Epoch   9 Batch  150/575   train_loss = 4.536\n",
      "Epoch   9 Batch  151/575   train_loss = 3.943\n",
      "Epoch   9 Batch  152/575   train_loss = 3.865\n",
      "Epoch   9 Batch  153/575   train_loss = 3.437\n",
      "Epoch   9 Batch  154/575   train_loss = 3.617\n",
      "Epoch   9 Batch  155/575   train_loss = 4.259\n",
      "Epoch   9 Batch  156/575   train_loss = 4.197\n",
      "Epoch   9 Batch  157/575   train_loss = 4.220\n",
      "Epoch   9 Batch  158/575   train_loss = 4.256\n",
      "Epoch   9 Batch  159/575   train_loss = 4.342\n",
      "Epoch   9 Batch  160/575   train_loss = 4.033\n",
      "Epoch   9 Batch  161/575   train_loss = 4.445\n",
      "Epoch   9 Batch  162/575   train_loss = 4.346\n",
      "Epoch   9 Batch  163/575   train_loss = 4.177\n",
      "Epoch   9 Batch  164/575   train_loss = 4.465\n",
      "Epoch   9 Batch  165/575   train_loss = 4.297\n",
      "Epoch   9 Batch  166/575   train_loss = 4.005\n",
      "Epoch   9 Batch  167/575   train_loss = 3.940\n",
      "Epoch   9 Batch  168/575   train_loss = 4.333\n",
      "Epoch   9 Batch  169/575   train_loss = 4.055\n",
      "Epoch   9 Batch  170/575   train_loss = 4.338\n",
      "Epoch   9 Batch  171/575   train_loss = 4.093\n",
      "Epoch   9 Batch  172/575   train_loss = 4.271\n",
      "Epoch   9 Batch  173/575   train_loss = 4.230\n",
      "Epoch   9 Batch  174/575   train_loss = 4.468\n",
      "Epoch   9 Batch  175/575   train_loss = 4.210\n",
      "Epoch   9 Batch  176/575   train_loss = 4.275\n",
      "Epoch   9 Batch  177/575   train_loss = 4.202\n",
      "Epoch   9 Batch  178/575   train_loss = 4.185\n",
      "Epoch   9 Batch  179/575   train_loss = 3.935\n",
      "Epoch   9 Batch  180/575   train_loss = 4.014\n",
      "Epoch   9 Batch  181/575   train_loss = 4.236\n",
      "Epoch   9 Batch  182/575   train_loss = 4.099\n",
      "Epoch   9 Batch  183/575   train_loss = 4.342\n",
      "Epoch   9 Batch  184/575   train_loss = 3.538\n",
      "Epoch   9 Batch  185/575   train_loss = 4.154\n",
      "Epoch   9 Batch  186/575   train_loss = 4.234\n",
      "Epoch   9 Batch  187/575   train_loss = 4.178\n",
      "Epoch   9 Batch  188/575   train_loss = 4.084\n",
      "Epoch   9 Batch  189/575   train_loss = 4.150\n",
      "Epoch   9 Batch  190/575   train_loss = 4.069\n",
      "Epoch   9 Batch  191/575   train_loss = 4.627\n",
      "Epoch   9 Batch  192/575   train_loss = 4.381\n",
      "Epoch   9 Batch  193/575   train_loss = 4.212\n",
      "Epoch   9 Batch  194/575   train_loss = 4.318\n",
      "Epoch   9 Batch  195/575   train_loss = 3.910\n",
      "Epoch   9 Batch  196/575   train_loss = 4.039\n",
      "Epoch   9 Batch  197/575   train_loss = 3.962\n",
      "Epoch   9 Batch  198/575   train_loss = 4.413\n",
      "Epoch   9 Batch  199/575   train_loss = 3.730\n",
      "Epoch   9 Batch  200/575   train_loss = 3.993\n",
      "Epoch   9 Batch  201/575   train_loss = 4.213\n",
      "Epoch   9 Batch  202/575   train_loss = 3.949\n",
      "Epoch   9 Batch  203/575   train_loss = 4.126\n",
      "Epoch   9 Batch  204/575   train_loss = 4.040\n",
      "Epoch   9 Batch  205/575   train_loss = 3.843\n",
      "Epoch   9 Batch  206/575   train_loss = 4.079\n",
      "Epoch   9 Batch  207/575   train_loss = 3.999\n",
      "Epoch   9 Batch  208/575   train_loss = 3.901\n",
      "Epoch   9 Batch  209/575   train_loss = 3.981\n",
      "Epoch   9 Batch  210/575   train_loss = 4.314\n",
      "Epoch   9 Batch  211/575   train_loss = 4.685\n",
      "Epoch   9 Batch  212/575   train_loss = 4.379\n",
      "Epoch   9 Batch  213/575   train_loss = 4.230\n",
      "Epoch   9 Batch  214/575   train_loss = 3.958\n",
      "Epoch   9 Batch  215/575   train_loss = 3.557\n",
      "Epoch   9 Batch  216/575   train_loss = 3.862\n",
      "Epoch   9 Batch  217/575   train_loss = 3.639\n",
      "Epoch   9 Batch  218/575   train_loss = 4.325\n",
      "Epoch   9 Batch  219/575   train_loss = 3.880\n",
      "Epoch   9 Batch  220/575   train_loss = 3.621\n",
      "Epoch   9 Batch  221/575   train_loss = 4.088\n",
      "Epoch   9 Batch  222/575   train_loss = 4.174\n",
      "Epoch   9 Batch  223/575   train_loss = 4.436\n",
      "Epoch   9 Batch  224/575   train_loss = 3.793\n",
      "Epoch   9 Batch  225/575   train_loss = 3.874\n",
      "Epoch   9 Batch  226/575   train_loss = 4.294\n",
      "Epoch   9 Batch  227/575   train_loss = 4.558\n",
      "Epoch   9 Batch  228/575   train_loss = 4.366\n",
      "Epoch   9 Batch  229/575   train_loss = 3.582\n",
      "Epoch   9 Batch  230/575   train_loss = 3.704\n",
      "Epoch   9 Batch  231/575   train_loss = 4.146\n",
      "Epoch   9 Batch  232/575   train_loss = 4.322\n",
      "Epoch   9 Batch  233/575   train_loss = 4.176\n",
      "Epoch   9 Batch  234/575   train_loss = 4.294\n",
      "Epoch   9 Batch  235/575   train_loss = 4.096\n",
      "Epoch   9 Batch  236/575   train_loss = 3.626\n",
      "Epoch   9 Batch  237/575   train_loss = 3.874\n",
      "Epoch   9 Batch  238/575   train_loss = 3.905\n",
      "Epoch   9 Batch  239/575   train_loss = 4.181\n",
      "Epoch   9 Batch  240/575   train_loss = 4.404\n",
      "Epoch   9 Batch  241/575   train_loss = 4.448\n",
      "Epoch   9 Batch  242/575   train_loss = 4.081\n",
      "Epoch   9 Batch  243/575   train_loss = 4.140\n",
      "Epoch   9 Batch  244/575   train_loss = 4.061\n",
      "Epoch   9 Batch  245/575   train_loss = 4.244\n",
      "Epoch   9 Batch  246/575   train_loss = 4.457\n",
      "Epoch   9 Batch  247/575   train_loss = 4.191\n",
      "Epoch   9 Batch  248/575   train_loss = 4.247\n",
      "Epoch   9 Batch  249/575   train_loss = 4.120\n",
      "Epoch   9 Batch  250/575   train_loss = 4.091\n",
      "Epoch   9 Batch  251/575   train_loss = 4.130\n",
      "Epoch   9 Batch  252/575   train_loss = 4.518\n",
      "Epoch   9 Batch  253/575   train_loss = 4.168\n",
      "Epoch   9 Batch  254/575   train_loss = 3.831\n",
      "Epoch   9 Batch  255/575   train_loss = 4.366\n",
      "Epoch   9 Batch  256/575   train_loss = 4.028\n",
      "Epoch   9 Batch  257/575   train_loss = 4.169\n",
      "Epoch   9 Batch  258/575   train_loss = 4.535\n",
      "Epoch   9 Batch  259/575   train_loss = 3.671\n",
      "Epoch   9 Batch  260/575   train_loss = 3.978\n",
      "Epoch   9 Batch  261/575   train_loss = 4.583\n",
      "Epoch   9 Batch  262/575   train_loss = 4.362\n",
      "Epoch   9 Batch  263/575   train_loss = 4.318\n",
      "Epoch   9 Batch  264/575   train_loss = 3.562\n",
      "Epoch   9 Batch  265/575   train_loss = 3.554\n",
      "Epoch   9 Batch  266/575   train_loss = 3.675\n",
      "Epoch   9 Batch  267/575   train_loss = 4.169\n",
      "Epoch   9 Batch  268/575   train_loss = 4.101\n",
      "Epoch   9 Batch  269/575   train_loss = 4.299\n",
      "Epoch   9 Batch  270/575   train_loss = 4.651\n",
      "Epoch   9 Batch  271/575   train_loss = 3.846\n",
      "Epoch   9 Batch  272/575   train_loss = 4.379\n",
      "Epoch   9 Batch  273/575   train_loss = 4.364\n",
      "Epoch   9 Batch  274/575   train_loss = 4.299\n",
      "Epoch   9 Batch  275/575   train_loss = 4.100\n",
      "Epoch   9 Batch  276/575   train_loss = 4.663\n",
      "Epoch   9 Batch  277/575   train_loss = 3.521\n",
      "Epoch   9 Batch  278/575   train_loss = 4.274\n",
      "Epoch   9 Batch  279/575   train_loss = 4.422\n",
      "Epoch   9 Batch  280/575   train_loss = 4.351\n",
      "Epoch   9 Batch  281/575   train_loss = 4.027\n",
      "Epoch   9 Batch  282/575   train_loss = 4.337\n",
      "Epoch   9 Batch  283/575   train_loss = 4.488\n",
      "Epoch   9 Batch  284/575   train_loss = 4.591\n",
      "Epoch   9 Batch  285/575   train_loss = 3.891\n",
      "Epoch   9 Batch  286/575   train_loss = 4.216\n",
      "Epoch   9 Batch  287/575   train_loss = 4.296\n",
      "Epoch   9 Batch  288/575   train_loss = 4.137\n",
      "Epoch   9 Batch  289/575   train_loss = 4.175\n",
      "Epoch   9 Batch  290/575   train_loss = 4.193\n",
      "Epoch   9 Batch  291/575   train_loss = 4.687\n",
      "Epoch   9 Batch  292/575   train_loss = 4.729\n",
      "Epoch   9 Batch  293/575   train_loss = 4.796\n",
      "Epoch   9 Batch  294/575   train_loss = 4.453\n",
      "Epoch   9 Batch  295/575   train_loss = 4.116\n",
      "Epoch   9 Batch  296/575   train_loss = 4.353\n",
      "Epoch   9 Batch  297/575   train_loss = 4.226\n",
      "Epoch   9 Batch  298/575   train_loss = 4.418\n",
      "Epoch   9 Batch  299/575   train_loss = 4.333\n",
      "Epoch   9 Batch  300/575   train_loss = 4.282\n",
      "Epoch   9 Batch  301/575   train_loss = 4.070\n",
      "Epoch   9 Batch  302/575   train_loss = 4.434\n",
      "Epoch   9 Batch  303/575   train_loss = 4.056\n",
      "Epoch   9 Batch  304/575   train_loss = 4.057\n",
      "Epoch   9 Batch  305/575   train_loss = 4.056\n",
      "Epoch   9 Batch  306/575   train_loss = 4.301\n",
      "Epoch   9 Batch  307/575   train_loss = 4.310\n",
      "Epoch   9 Batch  308/575   train_loss = 4.216\n",
      "Epoch   9 Batch  309/575   train_loss = 4.460\n",
      "Epoch   9 Batch  310/575   train_loss = 4.565\n",
      "Epoch   9 Batch  311/575   train_loss = 3.980\n",
      "Epoch   9 Batch  312/575   train_loss = 4.263\n",
      "Epoch   9 Batch  313/575   train_loss = 4.547\n",
      "Epoch   9 Batch  314/575   train_loss = 4.512\n",
      "Epoch   9 Batch  315/575   train_loss = 4.651\n",
      "Epoch   9 Batch  316/575   train_loss = 4.118\n",
      "Epoch   9 Batch  317/575   train_loss = 3.879\n",
      "Epoch   9 Batch  318/575   train_loss = 3.602\n",
      "Epoch   9 Batch  319/575   train_loss = 4.242\n",
      "Epoch   9 Batch  320/575   train_loss = 4.126\n",
      "Epoch   9 Batch  321/575   train_loss = 4.432\n",
      "Epoch   9 Batch  322/575   train_loss = 4.158\n",
      "Epoch   9 Batch  323/575   train_loss = 4.710\n",
      "Epoch   9 Batch  324/575   train_loss = 4.158\n",
      "Epoch   9 Batch  325/575   train_loss = 4.291\n",
      "Epoch   9 Batch  326/575   train_loss = 4.049\n",
      "Epoch   9 Batch  327/575   train_loss = 4.392\n",
      "Epoch   9 Batch  328/575   train_loss = 3.971\n",
      "Epoch   9 Batch  329/575   train_loss = 4.662\n",
      "Epoch   9 Batch  330/575   train_loss = 4.301\n",
      "Epoch   9 Batch  331/575   train_loss = 4.656\n",
      "Epoch   9 Batch  332/575   train_loss = 4.283\n",
      "Epoch   9 Batch  333/575   train_loss = 4.268\n",
      "Epoch   9 Batch  334/575   train_loss = 4.214\n",
      "Epoch   9 Batch  335/575   train_loss = 3.947\n",
      "Epoch   9 Batch  336/575   train_loss = 4.304\n",
      "Epoch   9 Batch  337/575   train_loss = 4.101\n",
      "Epoch   9 Batch  338/575   train_loss = 3.948\n",
      "Epoch   9 Batch  339/575   train_loss = 4.298\n",
      "Epoch   9 Batch  340/575   train_loss = 4.099\n",
      "Epoch   9 Batch  341/575   train_loss = 4.115\n",
      "Epoch   9 Batch  342/575   train_loss = 3.987\n",
      "Epoch   9 Batch  343/575   train_loss = 4.444\n",
      "Epoch   9 Batch  344/575   train_loss = 4.505\n",
      "Epoch   9 Batch  345/575   train_loss = 4.105\n",
      "Epoch   9 Batch  346/575   train_loss = 4.678\n",
      "Epoch   9 Batch  347/575   train_loss = 3.978\n",
      "Epoch   9 Batch  348/575   train_loss = 3.973\n",
      "Epoch   9 Batch  349/575   train_loss = 3.809\n",
      "Epoch   9 Batch  350/575   train_loss = 4.278\n",
      "Epoch   9 Batch  351/575   train_loss = 4.590\n",
      "Epoch   9 Batch  352/575   train_loss = 4.506\n",
      "Epoch   9 Batch  353/575   train_loss = 4.339\n",
      "Epoch   9 Batch  354/575   train_loss = 3.896\n",
      "Epoch   9 Batch  355/575   train_loss = 4.586\n",
      "Epoch   9 Batch  356/575   train_loss = 4.397\n",
      "Epoch   9 Batch  357/575   train_loss = 4.516\n",
      "Epoch   9 Batch  358/575   train_loss = 4.507\n",
      "Epoch   9 Batch  359/575   train_loss = 4.622\n",
      "Epoch   9 Batch  360/575   train_loss = 3.893\n",
      "Epoch   9 Batch  361/575   train_loss = 4.357\n",
      "Epoch   9 Batch  362/575   train_loss = 4.182\n",
      "Epoch   9 Batch  363/575   train_loss = 4.287\n",
      "Epoch   9 Batch  364/575   train_loss = 4.060\n",
      "Epoch   9 Batch  365/575   train_loss = 4.333\n",
      "Epoch   9 Batch  366/575   train_loss = 3.796\n",
      "Epoch   9 Batch  367/575   train_loss = 4.074\n",
      "Epoch   9 Batch  368/575   train_loss = 4.802\n",
      "Epoch   9 Batch  369/575   train_loss = 4.534\n",
      "Epoch   9 Batch  370/575   train_loss = 4.342\n",
      "Epoch   9 Batch  371/575   train_loss = 4.341\n",
      "Epoch   9 Batch  372/575   train_loss = 4.033\n",
      "Epoch   9 Batch  373/575   train_loss = 4.450\n",
      "Epoch   9 Batch  374/575   train_loss = 3.661\n",
      "Epoch   9 Batch  375/575   train_loss = 4.055\n",
      "Epoch   9 Batch  376/575   train_loss = 4.330\n",
      "Epoch   9 Batch  377/575   train_loss = 4.621\n",
      "Epoch   9 Batch  378/575   train_loss = 4.739\n",
      "Epoch   9 Batch  379/575   train_loss = 4.238\n",
      "Epoch   9 Batch  380/575   train_loss = 3.902\n",
      "Epoch   9 Batch  381/575   train_loss = 4.017\n",
      "Epoch   9 Batch  382/575   train_loss = 4.313\n",
      "Epoch   9 Batch  383/575   train_loss = 4.087\n",
      "Epoch   9 Batch  384/575   train_loss = 4.199\n",
      "Epoch   9 Batch  385/575   train_loss = 4.140\n",
      "Epoch   9 Batch  386/575   train_loss = 4.058\n",
      "Epoch   9 Batch  387/575   train_loss = 4.602\n",
      "Epoch   9 Batch  388/575   train_loss = 4.301\n",
      "Epoch   9 Batch  389/575   train_loss = 4.185\n",
      "Epoch   9 Batch  390/575   train_loss = 4.341\n",
      "Epoch   9 Batch  391/575   train_loss = 4.372\n",
      "Epoch   9 Batch  392/575   train_loss = 4.400\n",
      "Epoch   9 Batch  393/575   train_loss = 4.410\n",
      "Epoch   9 Batch  394/575   train_loss = 4.473\n",
      "Epoch   9 Batch  395/575   train_loss = 3.728\n",
      "Epoch   9 Batch  396/575   train_loss = 3.875\n",
      "Epoch   9 Batch  397/575   train_loss = 4.296\n",
      "Epoch   9 Batch  398/575   train_loss = 4.738\n",
      "Epoch   9 Batch  399/575   train_loss = 4.622\n",
      "Epoch   9 Batch  400/575   train_loss = 4.472\n",
      "Epoch   9 Batch  401/575   train_loss = 3.988\n",
      "Epoch   9 Batch  402/575   train_loss = 4.038\n",
      "Epoch   9 Batch  403/575   train_loss = 4.384\n",
      "Epoch   9 Batch  404/575   train_loss = 4.312\n",
      "Epoch   9 Batch  405/575   train_loss = 4.429\n",
      "Epoch   9 Batch  406/575   train_loss = 3.974\n",
      "Epoch   9 Batch  407/575   train_loss = 4.355\n",
      "Epoch   9 Batch  408/575   train_loss = 4.103\n",
      "Epoch   9 Batch  409/575   train_loss = 4.274\n",
      "Epoch   9 Batch  410/575   train_loss = 3.981\n",
      "Epoch   9 Batch  411/575   train_loss = 4.475\n",
      "Epoch   9 Batch  412/575   train_loss = 4.799\n",
      "Epoch   9 Batch  413/575   train_loss = 4.319\n",
      "Epoch   9 Batch  414/575   train_loss = 4.048\n",
      "Epoch   9 Batch  415/575   train_loss = 4.460\n",
      "Epoch   9 Batch  416/575   train_loss = 4.120\n",
      "Epoch   9 Batch  417/575   train_loss = 3.922\n",
      "Epoch   9 Batch  418/575   train_loss = 4.292\n",
      "Epoch   9 Batch  419/575   train_loss = 4.175\n",
      "Epoch   9 Batch  420/575   train_loss = 4.041\n",
      "Epoch   9 Batch  421/575   train_loss = 4.618\n",
      "Epoch   9 Batch  422/575   train_loss = 4.433\n",
      "Epoch   9 Batch  423/575   train_loss = 4.682\n",
      "Epoch   9 Batch  424/575   train_loss = 4.608\n",
      "Epoch   9 Batch  425/575   train_loss = 4.221\n",
      "Epoch   9 Batch  426/575   train_loss = 4.405\n",
      "Epoch   9 Batch  427/575   train_loss = 4.292\n",
      "Epoch   9 Batch  428/575   train_loss = 3.991\n",
      "Epoch   9 Batch  429/575   train_loss = 4.135\n",
      "Epoch   9 Batch  430/575   train_loss = 4.370\n",
      "Epoch   9 Batch  431/575   train_loss = 4.214\n",
      "Epoch   9 Batch  432/575   train_loss = 4.565\n",
      "Epoch   9 Batch  433/575   train_loss = 4.538\n",
      "Epoch   9 Batch  434/575   train_loss = 4.400\n",
      "Epoch   9 Batch  435/575   train_loss = 4.111\n",
      "Epoch   9 Batch  436/575   train_loss = 4.127\n",
      "Epoch   9 Batch  437/575   train_loss = 4.174\n",
      "Epoch   9 Batch  438/575   train_loss = 4.382\n",
      "Epoch   9 Batch  439/575   train_loss = 4.410\n",
      "Epoch   9 Batch  440/575   train_loss = 4.560\n",
      "Epoch   9 Batch  441/575   train_loss = 4.132\n",
      "Epoch   9 Batch  442/575   train_loss = 4.490\n",
      "Epoch   9 Batch  443/575   train_loss = 4.333\n",
      "Epoch   9 Batch  444/575   train_loss = 4.368\n",
      "Epoch   9 Batch  445/575   train_loss = 3.857\n",
      "Epoch   9 Batch  446/575   train_loss = 3.977\n",
      "Epoch   9 Batch  447/575   train_loss = 4.436\n",
      "Epoch   9 Batch  448/575   train_loss = 4.437\n",
      "Epoch   9 Batch  449/575   train_loss = 4.252\n",
      "Epoch   9 Batch  450/575   train_loss = 4.590\n",
      "Epoch   9 Batch  451/575   train_loss = 4.666\n",
      "Epoch   9 Batch  452/575   train_loss = 4.618\n",
      "Epoch   9 Batch  453/575   train_loss = 4.320\n",
      "Epoch   9 Batch  454/575   train_loss = 4.733\n",
      "Epoch   9 Batch  455/575   train_loss = 4.011\n",
      "Epoch   9 Batch  456/575   train_loss = 4.497\n",
      "Epoch   9 Batch  457/575   train_loss = 3.850\n",
      "Epoch   9 Batch  458/575   train_loss = 4.607\n",
      "Epoch   9 Batch  459/575   train_loss = 4.294\n",
      "Epoch   9 Batch  460/575   train_loss = 4.045\n",
      "Epoch   9 Batch  461/575   train_loss = 4.163\n",
      "Epoch   9 Batch  462/575   train_loss = 4.184\n",
      "Epoch   9 Batch  463/575   train_loss = 4.007\n",
      "Epoch   9 Batch  464/575   train_loss = 4.090\n",
      "Epoch   9 Batch  465/575   train_loss = 4.218\n",
      "Epoch   9 Batch  466/575   train_loss = 4.133\n",
      "Epoch   9 Batch  467/575   train_loss = 3.608\n",
      "Epoch   9 Batch  468/575   train_loss = 4.242\n",
      "Epoch   9 Batch  469/575   train_loss = 4.585\n",
      "Epoch   9 Batch  470/575   train_loss = 4.669\n",
      "Epoch   9 Batch  471/575   train_loss = 4.085\n",
      "Epoch   9 Batch  472/575   train_loss = 4.538\n",
      "Epoch   9 Batch  473/575   train_loss = 4.391\n",
      "Epoch   9 Batch  474/575   train_loss = 4.404\n",
      "Epoch   9 Batch  475/575   train_loss = 4.051\n",
      "Epoch   9 Batch  476/575   train_loss = 4.205\n",
      "Epoch   9 Batch  477/575   train_loss = 4.732\n",
      "Epoch   9 Batch  478/575   train_loss = 4.754\n",
      "Epoch   9 Batch  479/575   train_loss = 4.081\n",
      "Epoch   9 Batch  480/575   train_loss = 4.300\n",
      "Epoch   9 Batch  481/575   train_loss = 4.472\n",
      "Epoch   9 Batch  482/575   train_loss = 3.928\n",
      "Epoch   9 Batch  483/575   train_loss = 4.861\n",
      "Epoch   9 Batch  484/575   train_loss = 4.134\n",
      "Epoch   9 Batch  485/575   train_loss = 4.791\n",
      "Epoch   9 Batch  486/575   train_loss = 4.351\n",
      "Epoch   9 Batch  487/575   train_loss = 4.574\n",
      "Epoch   9 Batch  488/575   train_loss = 4.351\n",
      "Epoch   9 Batch  489/575   train_loss = 4.512\n",
      "Epoch   9 Batch  490/575   train_loss = 4.440\n",
      "Epoch   9 Batch  491/575   train_loss = 3.883\n",
      "Epoch   9 Batch  492/575   train_loss = 3.892\n",
      "Epoch   9 Batch  493/575   train_loss = 3.898\n",
      "Epoch   9 Batch  494/575   train_loss = 4.262\n",
      "Epoch   9 Batch  495/575   train_loss = 4.195\n",
      "Epoch   9 Batch  496/575   train_loss = 4.370\n",
      "Epoch   9 Batch  497/575   train_loss = 4.417\n",
      "Epoch   9 Batch  498/575   train_loss = 4.106\n",
      "Epoch   9 Batch  499/575   train_loss = 4.077\n",
      "Epoch   9 Batch  500/575   train_loss = 4.434\n",
      "Epoch   9 Batch  501/575   train_loss = 4.382\n",
      "Epoch   9 Batch  502/575   train_loss = 3.748\n",
      "Epoch   9 Batch  503/575   train_loss = 4.659\n",
      "Epoch   9 Batch  504/575   train_loss = 3.726\n",
      "Epoch   9 Batch  505/575   train_loss = 4.244\n",
      "Epoch   9 Batch  506/575   train_loss = 4.404\n",
      "Epoch   9 Batch  507/575   train_loss = 4.540\n",
      "Epoch   9 Batch  508/575   train_loss = 4.589\n",
      "Epoch   9 Batch  509/575   train_loss = 4.382\n",
      "Epoch   9 Batch  510/575   train_loss = 3.742\n",
      "Epoch   9 Batch  511/575   train_loss = 3.883\n",
      "Epoch   9 Batch  512/575   train_loss = 4.564\n",
      "Epoch   9 Batch  513/575   train_loss = 4.277\n",
      "Epoch   9 Batch  514/575   train_loss = 4.249\n",
      "Epoch   9 Batch  515/575   train_loss = 4.258\n",
      "Epoch   9 Batch  516/575   train_loss = 4.547\n",
      "Epoch   9 Batch  517/575   train_loss = 3.789\n",
      "Epoch   9 Batch  518/575   train_loss = 4.046\n",
      "Epoch   9 Batch  519/575   train_loss = 4.168\n",
      "Epoch   9 Batch  520/575   train_loss = 3.718\n",
      "Epoch   9 Batch  521/575   train_loss = 3.951\n",
      "Epoch   9 Batch  522/575   train_loss = 4.146\n",
      "Epoch   9 Batch  523/575   train_loss = 4.384\n",
      "Epoch   9 Batch  524/575   train_loss = 4.868\n",
      "Epoch   9 Batch  525/575   train_loss = 4.529\n",
      "Epoch   9 Batch  526/575   train_loss = 3.973\n",
      "Epoch   9 Batch  527/575   train_loss = 4.555\n",
      "Epoch   9 Batch  528/575   train_loss = 4.049\n",
      "Epoch   9 Batch  529/575   train_loss = 4.618\n",
      "Epoch   9 Batch  530/575   train_loss = 4.363\n",
      "Epoch   9 Batch  531/575   train_loss = 4.457\n",
      "Epoch   9 Batch  532/575   train_loss = 4.063\n",
      "Epoch   9 Batch  533/575   train_loss = 3.921\n",
      "Epoch   9 Batch  534/575   train_loss = 3.995\n",
      "Epoch   9 Batch  535/575   train_loss = 4.250\n",
      "Epoch   9 Batch  536/575   train_loss = 4.034\n",
      "Epoch   9 Batch  537/575   train_loss = 3.968\n",
      "Epoch   9 Batch  538/575   train_loss = 4.172\n",
      "Epoch   9 Batch  539/575   train_loss = 4.080\n",
      "Epoch   9 Batch  540/575   train_loss = 4.315\n",
      "Epoch   9 Batch  541/575   train_loss = 4.691\n",
      "Epoch   9 Batch  542/575   train_loss = 4.272\n",
      "Epoch   9 Batch  543/575   train_loss = 4.328\n",
      "Epoch   9 Batch  544/575   train_loss = 4.181\n",
      "Epoch   9 Batch  545/575   train_loss = 4.204\n",
      "Epoch   9 Batch  546/575   train_loss = 4.254\n",
      "Epoch   9 Batch  547/575   train_loss = 4.275\n",
      "Epoch   9 Batch  548/575   train_loss = 4.428\n",
      "Epoch   9 Batch  549/575   train_loss = 4.220\n",
      "Epoch   9 Batch  550/575   train_loss = 4.081\n",
      "Epoch   9 Batch  551/575   train_loss = 3.994\n",
      "Epoch   9 Batch  552/575   train_loss = 4.258\n",
      "Epoch   9 Batch  553/575   train_loss = 4.564\n",
      "Epoch   9 Batch  554/575   train_loss = 3.875\n",
      "Epoch   9 Batch  555/575   train_loss = 4.075\n",
      "Epoch   9 Batch  556/575   train_loss = 3.954\n",
      "Epoch   9 Batch  557/575   train_loss = 4.001\n",
      "Epoch   9 Batch  558/575   train_loss = 4.287\n",
      "Epoch   9 Batch  559/575   train_loss = 4.392\n",
      "Epoch   9 Batch  560/575   train_loss = 3.882\n",
      "Epoch   9 Batch  561/575   train_loss = 4.225\n",
      "Epoch   9 Batch  562/575   train_loss = 3.804\n",
      "Epoch   9 Batch  563/575   train_loss = 4.123\n",
      "Epoch   9 Batch  564/575   train_loss = 3.921\n",
      "Epoch   9 Batch  565/575   train_loss = 3.440\n",
      "Epoch   9 Batch  566/575   train_loss = 3.866\n",
      "Epoch   9 Batch  567/575   train_loss = 4.086\n",
      "Epoch   9 Batch  568/575   train_loss = 4.120\n",
      "Epoch   9 Batch  569/575   train_loss = 4.353\n",
      "Epoch   9 Batch  570/575   train_loss = 3.896\n",
      "Epoch   9 Batch  571/575   train_loss = 3.684\n",
      "Epoch   9 Batch  572/575   train_loss = 3.501\n",
      "Epoch   9 Batch  573/575   train_loss = 3.832\n",
      "Epoch   9 Batch  574/575   train_loss = 4.339\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inp = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    return inp, initial_state, final_state, probs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # print(probabilities)\n",
    "    # print(int_to_vocab)\n",
    "    \n",
    "    return int_to_vocab[np.argmax(probabilities)]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate TV Script\n",
    "This will generate the TV script for you.  Set `gen_length` to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader stored\n",
      "gen_sentences ['moe_szyslak:']\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh, i got a bar.\n",
      "moe_szyslak:(to phone) oh\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "    print('loader stored')\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    print('gen_sentences', gen_sentences)\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        # print('dyn_input', dyn_input)\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # print('run prediction')\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        # print('prediction done')\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "        # print(pred_word)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # print('sentences generated', gen_sentences)\n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The TV Script is Nonsensical\n",
    "It's ok if the TV script doesn't make any sense.  We trained on less than a megabyte of text.  In order to get good results, you'll have to use a smaller vocabulary or get more data.  Luckly there's more data!  As we mentioned in the begging of this project, this is a subset of [another dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data).  We didn't have you train on all the data, because that would take too long.  However, you are free to train your neural network on all the data.  After you complete the project, of course.\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
